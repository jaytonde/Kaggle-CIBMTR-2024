{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b0e33a",
   "metadata": {
    "_cell_guid": "2971c992-4673-42e1-940b-a51f340798f7",
    "_uuid": "8728c17a-ca9b-4936-a2a3-16ef9d697979",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007907,
     "end_time": "2025-01-12T15:14:49.095537",
     "exception": false,
     "start_time": "2025-01-12T15:14:49.087630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae3e6d0",
   "metadata": {
    "_cell_guid": "80d5b93f-de27-4388-9119-7a66db28cda6",
    "_uuid": "94ef81be-2b03-4e3b-9b24-6620bd3876c2",
    "execution": {
     "iopub.execute_input": "2025-01-12T15:14:49.112018Z",
     "iopub.status.busy": "2025-01-12T15:14:49.111529Z",
     "iopub.status.idle": "2025-01-12T15:17:00.303711Z",
     "shell.execute_reply": "2025-01-12T15:17:00.301659Z"
    },
    "papermill": {
     "duration": 131.204861,
     "end_time": "2025-01-12T15:17:00.307756",
     "exception": false,
     "start_time": "2025-01-12T15:14:49.102895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd==1.7.0) (1.26.4)\r\n",
      "autograd is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Processing /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: autograd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.7.0)\r\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.13.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd>=1.2.0->autograd-gamma==0.5.0) (1.26.4)\r\n",
      "Building wheels for collected packages: autograd-gamma\r\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=b987de17eb8a274edaa9207770f5b27e0c723cc1b3f6823a2321bca1ffcec1a8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/b5/e0/4c79e15c0b5f2c15ecf613c720bb20daab20a666eb67135155\r\n",
      "Successfully built autograd-gamma\r\n",
      "Installing collected packages: autograd-gamma\r\n",
      "Successfully installed autograd-gamma-0.5.0\r\n",
      "Processing /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\r\n",
      "Installing collected packages: interface-meta\r\n",
      "Successfully installed interface-meta-1.3.0\r\n",
      "Processing /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (2.1.4)\r\n",
      "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->formulaic==1.0.2) (1.16.0)\r\n",
      "Installing collected packages: formulaic\r\n",
      "Successfully installed formulaic-1.0.2\r\n",
      "Processing /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.13.1)\r\n",
      "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (2.1.4)\r\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (3.7.1)\r\n",
      "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.7.0)\r\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (0.5.0)\r\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.0.2)\r\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.16.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (4.53.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (24.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (3.1.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines==0.30.0) (1.16.0)\r\n",
      "Installing collected packages: lifelines\r\n",
      "Successfully installed lifelines-0.30.0\r\n",
      "Looking in links: /kaggle/input/tensorflow-2-15/tensorflow\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from tensorflow==2.15.0)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from tensorflow==2.15.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/tensorboard-2.15.1-py3-none-any.whl (from tensorflow==2.15.0)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/tensorflow_estimator-2.15.0-py2.py3-none-any.whl (from tensorflow==2.15.0)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/keras-2.15.0-py3-none-any.whl (from tensorflow==2.15.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.4)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\r\n",
      "Installing collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\r\n",
      "  Attempting uninstall: wrapt\r\n",
      "    Found existing installation: wrapt 1.16.0\r\n",
      "    Uninstalling wrapt-1.16.0:\r\n",
      "      Successfully uninstalled wrapt-1.16.0\r\n",
      "  Attempting uninstall: ml-dtypes\r\n",
      "    Found existing installation: ml-dtypes 0.4.1\r\n",
      "    Uninstalling ml-dtypes-0.4.1:\r\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 3.4.1\r\n",
      "    Uninstalling keras-3.4.1:\r\n",
      "      Successfully uninstalled keras-3.4.1\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.17.0\r\n",
      "    Uninstalling tensorboard-2.17.0:\r\n",
      "      Successfully uninstalled tensorboard-2.17.0\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.17.0\r\n",
      "    Uninstalling tensorflow-2.17.0:\r\n",
      "      Successfully uninstalled tensorflow-2.17.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.15.0 which is incompatible.\r\n",
      "tensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.15.0 which is incompatible.\r\n",
      "tensorstore 0.1.65 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\r\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\r\n",
      "Looking in links: /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/deeptables-0.2.5-py3-none-any.whl\r\n",
      "\u001b[33mWARNING: Package 'deeptables' has an invalid Requires-Python: Invalid specifier: '>=3.6.*'\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (24.1)\r\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (1.13.1)\r\n",
      "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (2.1.4)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (1.2.2)\r\n",
      "Requirement already satisfied: lightgbm>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (4.5.0)\r\n",
      "Requirement already satisfied: category-encoders>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (2.6.4)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/hypernets-0.3.1-py3-none-any.whl (from deeptables==0.2.5)\r\n",
      "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (3.11.0)\r\n",
      "Requirement already satisfied: eli5 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (0.13.0)\r\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders>=2.1.0->deeptables==0.2.5) (0.14.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category-encoders>=2.1.0->deeptables==0.2.5) (0.5.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (2024.6.1)\r\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (7.34.0)\r\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (5.7.1)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/XlsxWriter-3.1.9-py3-none-any.whl (from hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (5.9.5)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (6.0.2)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/paramiko-3.4.0-py3-none-any.whl (from hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (2.32.3)\r\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (6.3.3)\r\n",
      "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (3.11.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (4.66.5)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (1.4.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->deeptables==0.2.5) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->deeptables==0.2.5) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->deeptables==0.2.5) (2024.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->deeptables==0.2.5) (3.5.0)\r\n",
      "Requirement already satisfied: attrs>17.1.0 in /usr/local/lib/python3.10/dist-packages (from eli5->deeptables==0.2.5) (24.2.0)\r\n",
      "Requirement already satisfied: jinja2>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from eli5->deeptables==0.2.5) (3.1.4)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from eli5->deeptables==0.2.5) (1.16.0)\r\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from eli5->deeptables==0.2.5) (0.20.3)\r\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from eli5->deeptables==0.2.5) (0.9.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.0.0->eli5->deeptables==0.2.5) (2.1.5)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (71.0.4)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.19.2)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (4.4.2)\r\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (3.0.47)\r\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (2.18.0)\r\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.2.0)\r\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.1.7)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (4.9.0)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (43.0.1)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->hypernets>=0.2.5.1->deeptables==0.2.5) (0.2.13)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (2024.8.30)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (1.17.1)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.8.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.7.0)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (2.22)\r\n",
      "Installing collected packages: XlsxWriter, bcrypt, pynacl, paramiko, hypernets, deeptables\r\n",
      "Successfully installed XlsxWriter-3.1.9 bcrypt-4.1.2 deeptables-0.2.5 hypernets-0.3.1 paramiko-3.4.0 pynacl-1.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
    "!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\n",
    "!pip install --no-index -U --find-links=/kaggle/input/tensorflow-2-15/tensorflow tensorflow==2.15.0\n",
    "!pip install --no-index -U --find-links=/kaggle/input/deeptables-v0-2-5/deeptables-0.2.5 deeptables==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8e698f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:00.334678Z",
     "iopub.status.busy": "2025-01-12T15:17:00.334167Z",
     "iopub.status.idle": "2025-01-12T15:17:00.341813Z",
     "shell.execute_reply": "2025-01-12T15:17:00.340304Z"
    },
    "papermill": {
     "duration": 0.023735,
     "end_time": "2025-01-12T15:17:00.344125",
     "exception": false,
     "start_time": "2025-01-12T15:17:00.320390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install deeptables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b28932",
   "metadata": {
    "_cell_guid": "13bd92b6-c130-451f-89f5-13538b54ff91",
    "_uuid": "b5920c89-c860-498c-8335-057d15d2a5ce",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011085,
     "end_time": "2025-01-12T15:17:00.367369",
     "exception": false,
     "start_time": "2025-01-12T15:17:00.356284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "900af3bb",
   "metadata": {
    "_cell_guid": "149452d5-3116-45fe-9dc8-f0574edca449",
    "_uuid": "15f750c5-85f5-4eb5-9059-d4cf826746b1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:00.394669Z",
     "iopub.status.busy": "2025-01-12T15:17:00.394132Z",
     "iopub.status.idle": "2025-01-12T15:17:15.389903Z",
     "shell.execute_reply": "2025-01-12T15:17:15.388241Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 15.011909,
     "end_time": "2025-01-12T15:17:15.392647",
     "exception": false,
     "start_time": "2025-01-12T15:17:00.380738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.15.0\n",
      "DeepTables version: 0.2.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import deeptables as dt\n",
    "from deeptables.models import deeptable, deepnets\n",
    "from deeptables.datasets import dsutils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print('Tensorflow version:',tf.__version__)\n",
    "print('DeepTables version:',dt.__version__)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234d667",
   "metadata": {
    "_cell_guid": "d1996c37-ed51-40d0-80bf-683972841d04",
    "_uuid": "28c2f5c0-6627-44b8-9a87-f5ac3380cdc5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01067,
     "end_time": "2025-01-12T15:17:15.414603",
     "exception": false,
     "start_time": "2025-01-12T15:17:15.403933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3e40c6",
   "metadata": {
    "_cell_guid": "2c1f4cb8-2e2b-4555-9412-81aa90dabb84",
    "_uuid": "0cc2587e-3d0a-4177-bcbe-c980ffe47773",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:15.439017Z",
     "iopub.status.busy": "2025-01-12T15:17:15.437801Z",
     "iopub.status.idle": "2025-01-12T15:17:53.415550Z",
     "shell.execute_reply": "2025-01-12T15:17:53.413644Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 38.005311,
     "end_time": "2025-01-12T15:17:53.430892",
     "exception": false,
     "start_time": "2025-01-12T15:17:15.425581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (3, 58)\n",
      "Train shape: (28800, 61)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>dri_score</th>\n",
       "      <th>psych_disturb</th>\n",
       "      <th>cyto_score</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>hla_match_c_high</th>\n",
       "      <th>hla_high_res_8</th>\n",
       "      <th>tbi_status</th>\n",
       "      <th>arrhythmia</th>\n",
       "      <th>hla_low_res_6</th>\n",
       "      <th>graft_type</th>\n",
       "      <th>vent_hist</th>\n",
       "      <th>renal_issue</th>\n",
       "      <th>pulm_severe</th>\n",
       "      <th>prim_disease_hct</th>\n",
       "      <th>hla_high_res_6</th>\n",
       "      <th>cmv_status</th>\n",
       "      <th>hla_high_res_10</th>\n",
       "      <th>hla_match_dqb1_high</th>\n",
       "      <th>tce_imm_match</th>\n",
       "      <th>hla_nmdp_6</th>\n",
       "      <th>hla_match_c_low</th>\n",
       "      <th>rituximab</th>\n",
       "      <th>hla_match_drb1_low</th>\n",
       "      <th>hla_match_dqb1_low</th>\n",
       "      <th>prod_type</th>\n",
       "      <th>cyto_score_detail</th>\n",
       "      <th>conditioning_intensity</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>year_hct</th>\n",
       "      <th>obesity</th>\n",
       "      <th>mrd_hct</th>\n",
       "      <th>in_vivo_tcd</th>\n",
       "      <th>tce_match</th>\n",
       "      <th>hla_match_a_high</th>\n",
       "      <th>hepatic_severe</th>\n",
       "      <th>donor_age</th>\n",
       "      <th>prior_tumor</th>\n",
       "      <th>hla_match_b_low</th>\n",
       "      <th>peptic_ulcer</th>\n",
       "      <th>age_at_hct</th>\n",
       "      <th>hla_match_a_low</th>\n",
       "      <th>gvhd_proph</th>\n",
       "      <th>rheum_issue</th>\n",
       "      <th>sex_match</th>\n",
       "      <th>hla_match_b_high</th>\n",
       "      <th>race_group</th>\n",
       "      <th>comorbidity_score</th>\n",
       "      <th>karnofsky_score</th>\n",
       "      <th>hepatic_mild</th>\n",
       "      <th>tce_div_match</th>\n",
       "      <th>donor_related</th>\n",
       "      <th>melphalan_dose</th>\n",
       "      <th>hla_low_res_8</th>\n",
       "      <th>cardiac</th>\n",
       "      <th>hla_match_drb1_high</th>\n",
       "      <th>pulm_moderate</th>\n",
       "      <th>hla_low_res_10</th>\n",
       "      <th>efs</th>\n",
       "      <th>efs_time</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>N/A - non-malignant indication</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Bone marrow</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>IEA</td>\n",
       "      <td>6.0</td>\n",
       "      <td>+/+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>2016</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>9.942</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FKalone</td>\n",
       "      <td>No</td>\n",
       "      <td>M-F</td>\n",
       "      <td>2.0</td>\n",
       "      <td>More than one race</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unrelated</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.356</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>No</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>TBI +- Other, &gt;cGy</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Peripheral blood</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>AML</td>\n",
       "      <td>6.0</td>\n",
       "      <td>+/+</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>P/P</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PB</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>MAC</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>2008</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>No</td>\n",
       "      <td>Permissive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>72.29</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>43.705</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Other GVHD Prophylaxis</td>\n",
       "      <td>No</td>\n",
       "      <td>F-F</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Asian</td>\n",
       "      <td>3.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Related</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.672</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>N/A - non-malignant indication</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Bone marrow</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>HIS</td>\n",
       "      <td>6.0</td>\n",
       "      <td>+/+</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>P/P</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>2019</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>33.997</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Cyclophosphamide alone</td>\n",
       "      <td>No</td>\n",
       "      <td>F-M</td>\n",
       "      <td>2.0</td>\n",
       "      <td>More than one race</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Related</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.793</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Bone marrow</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>ALL</td>\n",
       "      <td>6.0</td>\n",
       "      <td>+/+</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>P/P</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BM</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>MAC</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>2009</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>No</td>\n",
       "      <td>Permissive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>29.23</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>43.245</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FK+ MMF +- others</td>\n",
       "      <td>No</td>\n",
       "      <td>M-M</td>\n",
       "      <td>2.0</td>\n",
       "      <td>White</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Unrelated</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>102.349</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Peripheral blood</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>MPN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>+/+</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAC</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2018</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>56.81</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>29.740</td>\n",
       "      <td>2.0</td>\n",
       "      <td>TDEPLETION +- other</td>\n",
       "      <td>No</td>\n",
       "      <td>M-F</td>\n",
       "      <td>2.0</td>\n",
       "      <td>American Indian or Alaska Native</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Related</td>\n",
       "      <td>MEL</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.223</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                       dri_score psych_disturb    cyto_score diabetes  \\\n",
       "0   0  N/A - non-malignant indication            No           NaN       No   \n",
       "1   1                    Intermediate            No  Intermediate       No   \n",
       "2   2  N/A - non-malignant indication            No           NaN       No   \n",
       "3   3                            High            No  Intermediate       No   \n",
       "4   4                            High            No           NaN       No   \n",
       "\n",
       "   hla_match_c_high  hla_high_res_8          tbi_status arrhythmia  \\\n",
       "0               NaN             NaN              No TBI         No   \n",
       "1               2.0             8.0  TBI +- Other, >cGy         No   \n",
       "2               2.0             8.0              No TBI         No   \n",
       "3               2.0             8.0              No TBI         No   \n",
       "4               2.0             8.0              No TBI         No   \n",
       "\n",
       "   hla_low_res_6        graft_type vent_hist renal_issue pulm_severe  \\\n",
       "0            6.0       Bone marrow        No          No          No   \n",
       "1            6.0  Peripheral blood        No          No          No   \n",
       "2            6.0       Bone marrow        No          No          No   \n",
       "3            6.0       Bone marrow        No          No          No   \n",
       "4            6.0  Peripheral blood        No          No          No   \n",
       "\n",
       "  prim_disease_hct  hla_high_res_6 cmv_status  hla_high_res_10  \\\n",
       "0              IEA             6.0        +/+              NaN   \n",
       "1              AML             6.0        +/+             10.0   \n",
       "2              HIS             6.0        +/+             10.0   \n",
       "3              ALL             6.0        +/+             10.0   \n",
       "4              MPN             6.0        +/+             10.0   \n",
       "\n",
       "   hla_match_dqb1_high tce_imm_match  hla_nmdp_6  hla_match_c_low rituximab  \\\n",
       "0                  2.0           NaN         6.0              2.0        No   \n",
       "1                  2.0           P/P         6.0              2.0        No   \n",
       "2                  2.0           P/P         6.0              2.0        No   \n",
       "3                  2.0           P/P         6.0              2.0        No   \n",
       "4                  2.0           NaN         5.0              2.0        No   \n",
       "\n",
       "   hla_match_drb1_low  hla_match_dqb1_low prod_type cyto_score_detail  \\\n",
       "0                 2.0                 2.0        BM               NaN   \n",
       "1                 2.0                 2.0        PB      Intermediate   \n",
       "2                 2.0                 2.0        BM               NaN   \n",
       "3                 2.0                 2.0        BM      Intermediate   \n",
       "4                 2.0                 2.0        PB               NaN   \n",
       "\n",
       "  conditioning_intensity               ethnicity  year_hct obesity   mrd_hct  \\\n",
       "0                    NaN  Not Hispanic or Latino      2016      No       NaN   \n",
       "1                    MAC  Not Hispanic or Latino      2008      No  Positive   \n",
       "2                    NaN  Not Hispanic or Latino      2019      No       NaN   \n",
       "3                    MAC  Not Hispanic or Latino      2009      No  Positive   \n",
       "4                    MAC      Hispanic or Latino      2018      No       NaN   \n",
       "\n",
       "  in_vivo_tcd   tce_match  hla_match_a_high hepatic_severe  donor_age  \\\n",
       "0         Yes         NaN               2.0             No        NaN   \n",
       "1          No  Permissive               2.0             No      72.29   \n",
       "2         Yes         NaN               2.0             No        NaN   \n",
       "3          No  Permissive               2.0             No      29.23   \n",
       "4         Yes         NaN               2.0             No      56.81   \n",
       "\n",
       "  prior_tumor  hla_match_b_low peptic_ulcer  age_at_hct  hla_match_a_low  \\\n",
       "0          No              2.0           No       9.942              2.0   \n",
       "1          No              2.0           No      43.705              2.0   \n",
       "2          No              2.0           No      33.997              2.0   \n",
       "3          No              2.0           No      43.245              2.0   \n",
       "4          No              2.0           No      29.740              2.0   \n",
       "\n",
       "               gvhd_proph rheum_issue sex_match  hla_match_b_high  \\\n",
       "0                 FKalone          No       M-F               2.0   \n",
       "1  Other GVHD Prophylaxis          No       F-F               2.0   \n",
       "2  Cyclophosphamide alone          No       F-M               2.0   \n",
       "3       FK+ MMF +- others          No       M-M               2.0   \n",
       "4     TDEPLETION +- other          No       M-F               2.0   \n",
       "\n",
       "                         race_group  comorbidity_score  karnofsky_score  \\\n",
       "0                More than one race                0.0             90.0   \n",
       "1                             Asian                3.0             90.0   \n",
       "2                More than one race                0.0             90.0   \n",
       "3                             White                0.0             90.0   \n",
       "4  American Indian or Alaska Native                1.0             90.0   \n",
       "\n",
       "  hepatic_mild          tce_div_match donor_related      melphalan_dose  \\\n",
       "0           No                    NaN     Unrelated  N/A, Mel not given   \n",
       "1           No  Permissive mismatched       Related  N/A, Mel not given   \n",
       "2           No  Permissive mismatched       Related  N/A, Mel not given   \n",
       "3          Yes  Permissive mismatched     Unrelated  N/A, Mel not given   \n",
       "4           No  Permissive mismatched       Related                 MEL   \n",
       "\n",
       "   hla_low_res_8 cardiac  hla_match_drb1_high pulm_moderate  hla_low_res_10  \\\n",
       "0            8.0      No                  2.0            No            10.0   \n",
       "1            8.0      No                  2.0           Yes            10.0   \n",
       "2            8.0      No                  2.0            No            10.0   \n",
       "3            8.0      No                  2.0            No            10.0   \n",
       "4            8.0      No                  2.0            No            10.0   \n",
       "\n",
       "   efs  efs_time  fold  \n",
       "0    0    42.356     3  \n",
       "1    1     4.672     7  \n",
       "2    0    19.793     6  \n",
       "3    0   102.349     1  \n",
       "4    0    16.223     4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "print(\"Test shape:\", test_df.shape )\n",
    "\n",
    "train_df = pd.read_excel(\"/kaggle/input/cibmtr-2024-dataset/random_folding.xlsx\")\n",
    "print(\"Train shape:\",train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf8e03",
   "metadata": {
    "_cell_guid": "4001c0b6-a391-498c-9042-d52096e147a4",
    "_uuid": "ff046806-7265-4782-9d4a-d48eda71d477",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011911,
     "end_time": "2025-01-12T15:17:53.454712",
     "exception": false,
     "start_time": "2025-01-12T15:17:53.442801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4: Traget preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c557736",
   "metadata": {
    "_cell_guid": "561a9a6c-3f51-4052-aadd-464125a72c30",
    "_uuid": "f35f6325-e19a-47a1-b72e-cb06d603b17c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:53.481831Z",
     "iopub.status.busy": "2025-01-12T15:17:53.480704Z",
     "iopub.status.idle": "2025-01-12T15:17:54.244873Z",
     "shell.execute_reply": "2025-01-12T15:17:54.243398Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.782452,
     "end_time": "2025-01-12T15:17:54.249558",
     "exception": false,
     "start_time": "2025-01-12T15:17:53.467106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"y\"] = train_df.efs_time.values\n",
    "mx = train_df.loc[train_df.efs==1,\"efs_time\"].max()\n",
    "mn = train_df.loc[train_df.efs==0,\"efs_time\"].min()\n",
    "train_df.loc[train_df.efs==0,\"y\"] = train_df.loc[train_df.efs==0,\"y\"] + mx - mn\n",
    "train_df.y = train_df.y.rank()\n",
    "train_df.loc[train_df.efs==0,\"y\"] += 2*len(train_df)\n",
    "train_df.y = train_df.y / train_df.y.max()\n",
    "train_df.y = np.log( train_df.y )\n",
    "train_df.y -= train_df.y.mean()\n",
    "train_df.y *= -1.0\n",
    "\n",
    "plt.hist(train_df.loc[train_df.efs==1,\"y\"],bins=100,label=\"efs=1, Yes Event\")\n",
    "plt.hist(train_df.loc[train_df.efs==0,\"y\"],bins=100,label=\"efs=0, Maybe Event\")\n",
    "plt.xlim((-5,5))\n",
    "plt.xlabel(\"Transformed Target y\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Transformed Target y using both efs and efs_time.\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4117fb98",
   "metadata": {
    "_cell_guid": "2d39ee67-9ffb-40bf-8fcd-c3e1153016f6",
    "_uuid": "3cc22a12-1155-4d62-b15e-5c4024979be4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:54.283359Z",
     "iopub.status.busy": "2025-01-12T15:17:54.282869Z",
     "iopub.status.idle": "2025-01-12T15:17:54.287566Z",
     "shell.execute_reply": "2025-01-12T15:17:54.286364Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020864,
     "end_time": "2025-01-12T15:17:54.289597",
     "exception": false,
     "start_time": "2025-01-12T15:17:54.268733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from lifelines import NelsonAalenFitter\n",
    "\n",
    "# FOLDS = 10\n",
    "# oof_preds = np.zeros(len(train))\n",
    "# for fold in range(FOLDS):\n",
    "#     train_data = train[train[\"fold\"]!=fold].copy()\n",
    "#     valid_data = train[train[\"fold\"]==fold].copy()\n",
    "    \n",
    "#     naf = NelsonAalenFitter()\n",
    "#     naf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n",
    "    \n",
    "#     oof_preds[valid_data.index] = -naf.cumulative_hazard_at_times(valid_data['efs_time']).values\n",
    "\n",
    "# train['target3'] = oof_preds  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ddaf9c",
   "metadata": {
    "_cell_guid": "16a6bd99-3af0-4a85-8bf5-7cf445665282",
    "_uuid": "59794a24-b402-4a12-97a6-2c76becf9469",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011581,
     "end_time": "2025-01-12T15:17:54.314188",
     "exception": false,
     "start_time": "2025-01-12T15:17:54.302607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5: Features preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e021d37",
   "metadata": {
    "_cell_guid": "6c02561f-bcf6-4f70-90d7-d896d00ea765",
    "_uuid": "7bf2caf4-3ad8-42b1-bf13-5e412a2a8127",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:54.340781Z",
     "iopub.status.busy": "2025-01-12T15:17:54.340287Z",
     "iopub.status.idle": "2025-01-12T15:17:54.346346Z",
     "shell.execute_reply": "2025-01-12T15:17:54.345045Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021906,
     "end_time": "2025-01-12T15:17:54.348507",
     "exception": false,
     "start_time": "2025-01-12T15:17:54.326601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57 FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'hla_match_c_high', 'hla_high_res_8', 'tbi_status', 'arrhythmia', 'hla_low_res_6', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'hla_high_res_6', 'cmv_status', 'hla_high_res_10', 'hla_match_dqb1_high', 'tce_imm_match', 'hla_nmdp_6', 'hla_match_c_low', 'rituximab', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'year_hct', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hla_match_a_high', 'hepatic_severe', 'donor_age', 'prior_tumor', 'hla_match_b_low', 'peptic_ulcer', 'age_at_hct', 'hla_match_a_low', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hla_match_b_high', 'race_group', 'comorbidity_score', 'karnofsky_score', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'hla_low_res_8', 'cardiac', 'hla_match_drb1_high', 'pulm_moderate', 'hla_low_res_10']\n"
     ]
    }
   ],
   "source": [
    "RMV = [\"ID\",\"efs\",\"efs_time\",\"y\",\"fold\"]\n",
    "FEATURES = [c for c in train_df.columns if not c in RMV]\n",
    "print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f52904",
   "metadata": {
    "_cell_guid": "fac9793e-41fd-4156-8707-2dadddae8635",
    "_uuid": "62c1bdf1-7676-43e1-86fc-5b5237f4bf4f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:54.374838Z",
     "iopub.status.busy": "2025-01-12T15:17:54.374315Z",
     "iopub.status.idle": "2025-01-12T15:17:54.503803Z",
     "shell.execute_reply": "2025-01-12T15:17:54.502245Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.145623,
     "end_time": "2025-01-12T15:17:54.506580",
     "exception": false,
     "start_time": "2025-01-12T15:17:54.360957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In these features, there are 35 CATEGORICAL FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status', 'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe', 'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match', 'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'cardiac', 'pulm_moderate']\n"
     ]
    }
   ],
   "source": [
    "CATS = []\n",
    "for c in FEATURES:\n",
    "    if train_df[c].dtype==\"object\":\n",
    "        CATS.append(c)\n",
    "        train_df[c] = train_df[c].fillna(\"NAN\")\n",
    "        test_df[c]  = test_df[c].fillna(\"NAN\")\n",
    "    else:\n",
    "        train_df[c] = train_df[c].fillna(-1)\n",
    "        test_df[c]  = test_df[c].fillna(-1)\n",
    "print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66576f2",
   "metadata": {
    "_cell_guid": "77422609-d68b-48e6-b0d3-1e5705cf548a",
    "_uuid": "e723d9c8-24fe-4101-8323-f39db250b23f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:54.535534Z",
     "iopub.status.busy": "2025-01-12T15:17:54.534961Z",
     "iopub.status.idle": "2025-01-12T15:17:54.763706Z",
     "shell.execute_reply": "2025-01-12T15:17:54.762198Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.246198,
     "end_time": "2025-01-12T15:17:54.765871",
     "exception": false,
     "start_time": "2025-01-12T15:17:54.519673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We LABEL ENCODE the CATEGORICAL FEATURES: dri_score, psych_disturb, cyto_score, diabetes, tbi_status, arrhythmia, graft_type, vent_hist, renal_issue, pulm_severe, prim_disease_hct, cmv_status, tce_imm_match, rituximab, prod_type, cyto_score_detail, conditioning_intensity, ethnicity, obesity, mrd_hct, in_vivo_tcd, tce_match, hepatic_severe, prior_tumor, peptic_ulcer, gvhd_proph, rheum_issue, sex_match, race_group, hepatic_mild, tce_div_match, donor_related, melphalan_dose, cardiac, pulm_moderate, "
     ]
    }
   ],
   "source": [
    "combined = pd.concat([train_df,test_df],axis=0,ignore_index=True)\n",
    "#print(\"Combined data shape:\", combined.shape )\n",
    "\n",
    "# LABEL ENCODE CATEGORICAL FEATURES\n",
    "print(\"We LABEL ENCODE the CATEGORICAL FEATURES: \",end=\"\")\n",
    "for c in FEATURES:\n",
    "\n",
    "    # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "    if c in CATS:\n",
    "        print(f\"{c}, \",end=\"\")\n",
    "        combined[c],_ = combined[c].factorize()\n",
    "        combined[c] -= combined[c].min()\n",
    "        combined[c] = combined[c].astype(\"int32\")\n",
    "        combined[c] = combined[c].astype(\"category\")\n",
    "\n",
    "    # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n",
    "    else:\n",
    "        if combined[c].dtype==\"float64\":\n",
    "            combined[c] = combined[c].astype(\"float32\")\n",
    "        if combined[c].dtype==\"int64\":\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "    \n",
    "train_df = combined.iloc[:len(train_df)].copy()\n",
    "test_df = combined.iloc[len(train_df):].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd2b9f8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:54.793378Z",
     "iopub.status.busy": "2025-01-12T15:17:54.792890Z",
     "iopub.status.idle": "2025-01-12T15:17:54.797873Z",
     "shell.execute_reply": "2025-01-12T15:17:54.796876Z"
    },
    "papermill": {
     "duration": 0.020558,
     "end_time": "2025-01-12T15:17:54.799778",
     "exception": false,
     "start_time": "2025-01-12T15:17:54.779220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUMS = [c for c in FEATURES if not c in CATS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5334b",
   "metadata": {
    "_cell_guid": "c741ac55-cb90-47b5-ae30-af924cd94e92",
    "_uuid": "2e460c73-3bfd-4e7b-8438-9214f48cdf10",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011899,
     "end_time": "2025-01-12T15:17:54.824314",
     "exception": false,
     "start_time": "2025-01-12T15:17:54.812415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 6: Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93e8f4d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:54.850680Z",
     "iopub.status.busy": "2025-01-12T15:17:54.850193Z",
     "iopub.status.idle": "2025-01-12T15:17:54.854771Z",
     "shell.execute_reply": "2025-01-12T15:17:54.853620Z"
    },
    "papermill": {
     "duration": 0.020175,
     "end_time": "2025-01-12T15:17:54.856813",
     "exception": false,
     "start_time": "2025-01-12T15:17:54.836638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow.python.framework.ops import disable_eager_execution\n",
    "# disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7bae91c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:54.883687Z",
     "iopub.status.busy": "2025-01-12T15:17:54.883211Z",
     "iopub.status.idle": "2025-01-12T15:17:54.888902Z",
     "shell.execute_reply": "2025-01-12T15:17:54.887709Z"
    },
    "papermill": {
     "duration": 0.02174,
     "end_time": "2025-01-12T15:17:54.891222",
     "exception": false,
     "start_time": "2025-01-12T15:17:54.869482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d711b8dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:54.920939Z",
     "iopub.status.busy": "2025-01-12T15:17:54.920491Z",
     "iopub.status.idle": "2025-01-12T15:17:54.925979Z",
     "shell.execute_reply": "2025-01-12T15:17:54.924326Z"
    },
    "papermill": {
     "duration": 0.022726,
     "end_time": "2025-01-12T15:17:54.928455",
     "exception": false,
     "start_time": "2025-01-12T15:17:54.905729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a92a00b2",
   "metadata": {
    "_cell_guid": "f7785561-27f1-4b1f-8476-491cdde0d065",
    "_uuid": "42fe86fc-ecd4-47ce-93d8-b137aff3855a",
    "execution": {
     "iopub.execute_input": "2025-01-12T15:17:54.955817Z",
     "iopub.status.busy": "2025-01-12T15:17:54.955233Z",
     "iopub.status.idle": "2025-01-12T15:39:26.490941Z",
     "shell.execute_reply": "2025-01-12T15:39:26.489467Z"
    },
    "papermill": {
     "duration": 1291.552146,
     "end_time": "2025-01-12T15:39:26.493363",
     "exception": false,
     "start_time": "2025-01-12T15:17:54.941217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "#########################\n",
      "01-12 15:17:54 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7b26e39139a0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 15:17:54 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 15:17:55 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:17:55 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 15:17:55 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 15:17:55 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.024656057357788086s\n",
      "01-12 15:17:55 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 15:17:55 I deeptables.m.preprocessor.py 383 - Imputation taken 0.10887956619262695s\n",
      "01-12 15:17:55 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 15:17:55 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.06560087203979492s\n",
      "01-12 15:17:55 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.2625727653503418s\n",
      "01-12 15:17:55 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 15:17:55 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 15:17:55 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:17:55 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:17:55 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:17:56 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 15:17:56 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 9s 10ms/step - loss: 5065.7524 - root_mean_squared_error: 71.1741 - val_loss: 96.0296 - val_root_mean_squared_error: 9.7995\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 90.3471 - root_mean_squared_error: 9.5051 - val_loss: 85.7608 - val_root_mean_squared_error: 9.2607\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 82.9114 - root_mean_squared_error: 9.1056 - val_loss: 77.9014 - val_root_mean_squared_error: 8.8262\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 74.5052 - root_mean_squared_error: 8.6316 - val_loss: 68.7233 - val_root_mean_squared_error: 8.2900\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 65.3598 - root_mean_squared_error: 8.0845 - val_loss: 59.1648 - val_root_mean_squared_error: 7.6919\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 55.7941 - root_mean_squared_error: 7.4695 - val_loss: 50.3736 - val_root_mean_squared_error: 7.0974\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 46.5395 - root_mean_squared_error: 6.8220 - val_loss: 41.4111 - val_root_mean_squared_error: 6.4351\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 38.6219 - root_mean_squared_error: 6.2147 - val_loss: 34.7817 - val_root_mean_squared_error: 5.8976\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 32.6495 - root_mean_squared_error: 5.7140 - val_loss: 29.5748 - val_root_mean_squared_error: 5.4383\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 28.2400 - root_mean_squared_error: 5.3141 - val_loss: 25.8334 - val_root_mean_squared_error: 5.0827\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 24.7620 - root_mean_squared_error: 4.9761 - val_loss: 22.6744 - val_root_mean_squared_error: 4.7618\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 21.9014 - root_mean_squared_error: 4.6799 - val_loss: 20.0467 - val_root_mean_squared_error: 4.4774\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 19.3241 - root_mean_squared_error: 4.3959 - val_loss: 17.6217 - val_root_mean_squared_error: 4.1978\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 16.9671 - root_mean_squared_error: 4.1191 - val_loss: 15.3623 - val_root_mean_squared_error: 3.9195\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 14.8557 - root_mean_squared_error: 3.8543 - val_loss: 13.5103 - val_root_mean_squared_error: 3.6756\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 12.9504 - root_mean_squared_error: 3.5987 - val_loss: 11.6723 - val_root_mean_squared_error: 3.4165\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 11.3152 - root_mean_squared_error: 3.3638 - val_loss: 10.3251 - val_root_mean_squared_error: 3.2133\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 9.7930 - root_mean_squared_error: 3.1294 - val_loss: 8.8490 - val_root_mean_squared_error: 2.9747\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 8.5809 - root_mean_squared_error: 2.9293 - val_loss: 7.7152 - val_root_mean_squared_error: 2.7776\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 7.4273 - root_mean_squared_error: 2.7253 - val_loss: 6.7682 - val_root_mean_squared_error: 2.6016\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 6.5325 - root_mean_squared_error: 2.5559 - val_loss: 5.8636 - val_root_mean_squared_error: 2.4215\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.6921 - root_mean_squared_error: 2.3858 - val_loss: 5.0993 - val_root_mean_squared_error: 2.2582\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 5.0256 - root_mean_squared_error: 2.2418 - val_loss: 4.5341 - val_root_mean_squared_error: 2.1293\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.4244 - root_mean_squared_error: 2.1034 - val_loss: 3.9952 - val_root_mean_squared_error: 1.9988\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.9515 - root_mean_squared_error: 1.9878 - val_loss: 3.5927 - val_root_mean_squared_error: 1.8954\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.5732 - root_mean_squared_error: 1.8903 - val_loss: 3.1805 - val_root_mean_squared_error: 1.7834\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.2382 - root_mean_squared_error: 1.7995 - val_loss: 2.9472 - val_root_mean_squared_error: 1.7167\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.9817 - root_mean_squared_error: 1.7268 - val_loss: 2.7290 - val_root_mean_squared_error: 1.6520\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.7786 - root_mean_squared_error: 1.6669 - val_loss: 2.5288 - val_root_mean_squared_error: 1.5902\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.6089 - root_mean_squared_error: 1.6152 - val_loss: 2.4024 - val_root_mean_squared_error: 1.5500\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.4817 - root_mean_squared_error: 1.5753 - val_loss: 2.3058 - val_root_mean_squared_error: 1.5185\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.3810 - root_mean_squared_error: 1.5431 - val_loss: 2.1934 - val_root_mean_squared_error: 1.4810\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.2862 - root_mean_squared_error: 1.5120 - val_loss: 2.1354 - val_root_mean_squared_error: 1.4613\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.2274 - root_mean_squared_error: 1.4925 - val_loss: 2.1005 - val_root_mean_squared_error: 1.4493\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.1851 - root_mean_squared_error: 1.4782 - val_loss: 2.0495 - val_root_mean_squared_error: 1.4316\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.1539 - root_mean_squared_error: 1.4676 - val_loss: 2.0430 - val_root_mean_squared_error: 1.4293\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1165 - root_mean_squared_error: 1.4548 - val_loss: 2.0139 - val_root_mean_squared_error: 1.4191\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0960 - root_mean_squared_error: 1.4478 - val_loss: 1.9693 - val_root_mean_squared_error: 1.4033\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0874 - root_mean_squared_error: 1.4448 - val_loss: 1.9670 - val_root_mean_squared_error: 1.4025\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0634 - root_mean_squared_error: 1.4365 - val_loss: 1.9874 - val_root_mean_squared_error: 1.4097\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0532 - root_mean_squared_error: 1.4329 - val_loss: 1.9444 - val_root_mean_squared_error: 1.3944\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0430 - root_mean_squared_error: 1.4293 - val_loss: 1.9684 - val_root_mean_squared_error: 1.4030\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0263 - root_mean_squared_error: 1.4235 - val_loss: 1.9252 - val_root_mean_squared_error: 1.3875\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0235 - root_mean_squared_error: 1.4225 - val_loss: 1.9336 - val_root_mean_squared_error: 1.3905\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0201 - root_mean_squared_error: 1.4213 - val_loss: 2.0355 - val_root_mean_squared_error: 1.4267\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0108 - root_mean_squared_error: 1.4180 - val_loss: 1.9620 - val_root_mean_squared_error: 1.4007\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0055 - root_mean_squared_error: 1.4162 - val_loss: 1.9577 - val_root_mean_squared_error: 1.3992\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0209 - root_mean_squared_error: 1.4216 - val_loss: 1.9381 - val_root_mean_squared_error: 1.3922\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9938 - root_mean_squared_error: 1.4120 - val_loss: 1.8877 - val_root_mean_squared_error: 1.3739\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0187 - root_mean_squared_error: 1.4208 - val_loss: 1.9110 - val_root_mean_squared_error: 1.3824\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0157 - root_mean_squared_error: 1.4198 - val_loss: 1.9538 - val_root_mean_squared_error: 1.3978\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9817 - root_mean_squared_error: 1.4077 - val_loss: 1.8762 - val_root_mean_squared_error: 1.3697\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0024 - root_mean_squared_error: 1.4151 - val_loss: 1.8982 - val_root_mean_squared_error: 1.3778\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9828 - root_mean_squared_error: 1.4081 - val_loss: 1.8960 - val_root_mean_squared_error: 1.3770\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9799 - root_mean_squared_error: 1.4071 - val_loss: 1.8635 - val_root_mean_squared_error: 1.3651\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9935 - root_mean_squared_error: 1.4119 - val_loss: 2.0070 - val_root_mean_squared_error: 1.4167\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9832 - root_mean_squared_error: 1.4083 - val_loss: 1.8854 - val_root_mean_squared_error: 1.3731\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9897 - root_mean_squared_error: 1.4106 - val_loss: 2.0428 - val_root_mean_squared_error: 1.4293\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9935 - root_mean_squared_error: 1.4119 - val_loss: 1.8834 - val_root_mean_squared_error: 1.3724\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9910 - root_mean_squared_error: 1.4110 - val_loss: 1.8871 - val_root_mean_squared_error: 1.3737\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9709 - root_mean_squared_error: 1.4039 - val_loss: 1.8731 - val_root_mean_squared_error: 1.3686\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9603 - root_mean_squared_error: 1.4001 - val_loss: 1.9005 - val_root_mean_squared_error: 1.3786\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9930 - root_mean_squared_error: 1.4117 - val_loss: 1.8942 - val_root_mean_squared_error: 1.3763\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0014 - root_mean_squared_error: 1.4147 - val_loss: 1.8952 - val_root_mean_squared_error: 1.3767\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9569 - root_mean_squared_error: 1.3989 - val_loss: 1.9789 - val_root_mean_squared_error: 1.4067\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9907 - root_mean_squared_error: 1.4109 - val_loss: 1.8743 - val_root_mean_squared_error: 1.3691\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9618 - root_mean_squared_error: 1.4007 - val_loss: 1.8435 - val_root_mean_squared_error: 1.3578\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0020 - root_mean_squared_error: 1.4149 - val_loss: 2.0141 - val_root_mean_squared_error: 1.4192\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9567 - root_mean_squared_error: 1.3988 - val_loss: 1.8710 - val_root_mean_squared_error: 1.3678\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9872 - root_mean_squared_error: 1.4097 - val_loss: 1.8769 - val_root_mean_squared_error: 1.3700\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9845 - root_mean_squared_error: 1.4087 - val_loss: 1.8621 - val_root_mean_squared_error: 1.3646\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9659 - root_mean_squared_error: 1.4021 - val_loss: 1.8595 - val_root_mean_squared_error: 1.3636\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9759 - root_mean_squared_error: 1.4057 - val_loss: 1.9903 - val_root_mean_squared_error: 1.4108\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0022 - root_mean_squared_error: 1.4150 - val_loss: 2.0886 - val_root_mean_squared_error: 1.4452\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9966 - root_mean_squared_error: 1.4130 - val_loss: 1.8655 - val_root_mean_squared_error: 1.3658\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9743 - root_mean_squared_error: 1.4051 - val_loss: 2.2912 - val_root_mean_squared_error: 1.5137\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9675 - root_mean_squared_error: 1.4027 - val_loss: 1.9333 - val_root_mean_squared_error: 1.3904\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9729 - root_mean_squared_error: 1.4046 - val_loss: 1.8830 - val_root_mean_squared_error: 1.3722\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9522 - root_mean_squared_error: 1.3972 - val_loss: 1.8765 - val_root_mean_squared_error: 1.3699\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9626 - root_mean_squared_error: 1.4009 - val_loss: 1.8810 - val_root_mean_squared_error: 1.3715\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9355 - root_mean_squared_error: 1.3912 - val_loss: 1.8775 - val_root_mean_squared_error: 1.3702\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9379 - root_mean_squared_error: 1.3921 - val_loss: 1.8781 - val_root_mean_squared_error: 1.3704\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9894 - root_mean_squared_error: 1.4105 - val_loss: 1.9366 - val_root_mean_squared_error: 1.3916\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9592 - root_mean_squared_error: 1.3997 - val_loss: 1.9561 - val_root_mean_squared_error: 1.3986\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9659 - root_mean_squared_error: 1.4021 - val_loss: 2.0204 - val_root_mean_squared_error: 1.4214\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9639 - root_mean_squared_error: 1.4014 - val_loss: 1.8651 - val_root_mean_squared_error: 1.3657\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9764 - root_mean_squared_error: 1.4059 - val_loss: 1.8540 - val_root_mean_squared_error: 1.3616\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9594 - root_mean_squared_error: 1.3998 - val_loss: 1.8835 - val_root_mean_squared_error: 1.3724\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9364 - root_mean_squared_error: 1.3915 - val_loss: 1.8546 - val_root_mean_squared_error: 1.3619\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9543 - root_mean_squared_error: 1.3980 - val_loss: 1.9611 - val_root_mean_squared_error: 1.4004\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9868 - root_mean_squared_error: 1.4096 - val_loss: 2.0565 - val_root_mean_squared_error: 1.4340\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9579 - root_mean_squared_error: 1.3993 - val_loss: 1.8714 - val_root_mean_squared_error: 1.3680\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9413 - root_mean_squared_error: 1.3933 - val_loss: 1.8677 - val_root_mean_squared_error: 1.3666\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9896 - root_mean_squared_error: 1.4105 - val_loss: 1.9431 - val_root_mean_squared_error: 1.3939\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9722 - root_mean_squared_error: 1.4043 - val_loss: 1.8630 - val_root_mean_squared_error: 1.3649\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9338 - root_mean_squared_error: 1.3906 - val_loss: 2.0132 - val_root_mean_squared_error: 1.4189\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9540 - root_mean_squared_error: 1.3979 - val_loss: 1.9341 - val_root_mean_squared_error: 1.3907\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9502 - root_mean_squared_error: 1.3965 - val_loss: 1.8828 - val_root_mean_squared_error: 1.3721\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9526 - root_mean_squared_error: 1.3974 - val_loss: 1.9241 - val_root_mean_squared_error: 1.3871\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9498 - root_mean_squared_error: 1.3963 - val_loss: 1.8672 - val_root_mean_squared_error: 1.3664\n",
      "01-12 15:20:04 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 15:20:04 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 15:20:04 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112151754_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 15:20:04 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:20:04 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 15:20:05 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:20:05 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 2\n",
      "#########################\n",
      "01-12 15:20:05 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7b266e8a18a0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 15:20:05 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 15:20:05 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:20:05 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 15:20:05 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 15:20:05 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.023524761199951172s\n",
      "01-12 15:20:05 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 15:20:05 I deeptables.m.preprocessor.py 383 - Imputation taken 0.1021125316619873s\n",
      "01-12 15:20:05 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 15:20:05 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.06698369979858398s\n",
      "01-12 15:20:05 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.25444912910461426s\n",
      "01-12 15:20:05 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 15:20:05 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 15:20:05 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:20:05 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:20:05 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:20:06 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 15:20:06 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 8s 10ms/step - loss: 2.3228 - root_mean_squared_error: 1.5241 - val_loss: 2.1565 - val_root_mean_squared_error: 1.4685\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.2656 - root_mean_squared_error: 1.5052 - val_loss: 2.1182 - val_root_mean_squared_error: 1.4554\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1866 - root_mean_squared_error: 1.4787 - val_loss: 2.0023 - val_root_mean_squared_error: 1.4150\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1066 - root_mean_squared_error: 1.4514 - val_loss: 1.9436 - val_root_mean_squared_error: 1.3941\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0498 - root_mean_squared_error: 1.4317 - val_loss: 1.8992 - val_root_mean_squared_error: 1.3781\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0164 - root_mean_squared_error: 1.4200 - val_loss: 1.9128 - val_root_mean_squared_error: 1.3831\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9832 - root_mean_squared_error: 1.4083 - val_loss: 1.8408 - val_root_mean_squared_error: 1.3567\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9627 - root_mean_squared_error: 1.4010 - val_loss: 1.8362 - val_root_mean_squared_error: 1.3551\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9512 - root_mean_squared_error: 1.3969 - val_loss: 1.8269 - val_root_mean_squared_error: 1.3516\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9514 - root_mean_squared_error: 1.3969 - val_loss: 1.8285 - val_root_mean_squared_error: 1.3522\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9395 - root_mean_squared_error: 1.3927 - val_loss: 1.8086 - val_root_mean_squared_error: 1.3448\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9346 - root_mean_squared_error: 1.3909 - val_loss: 1.7943 - val_root_mean_squared_error: 1.3395\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9366 - root_mean_squared_error: 1.3916 - val_loss: 1.8254 - val_root_mean_squared_error: 1.3511\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9404 - root_mean_squared_error: 1.3930 - val_loss: 1.8136 - val_root_mean_squared_error: 1.3467\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9333 - root_mean_squared_error: 1.3904 - val_loss: 1.8372 - val_root_mean_squared_error: 1.3554\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9342 - root_mean_squared_error: 1.3908 - val_loss: 1.8176 - val_root_mean_squared_error: 1.3482\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9291 - root_mean_squared_error: 1.3889 - val_loss: 1.8203 - val_root_mean_squared_error: 1.3492\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9362 - root_mean_squared_error: 1.3915 - val_loss: 1.8116 - val_root_mean_squared_error: 1.3460\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9283 - root_mean_squared_error: 1.3886 - val_loss: 1.8364 - val_root_mean_squared_error: 1.3551\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9257 - root_mean_squared_error: 1.3877 - val_loss: 1.8253 - val_root_mean_squared_error: 1.3510\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9344 - root_mean_squared_error: 1.3908 - val_loss: 1.8069 - val_root_mean_squared_error: 1.3442\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9263 - root_mean_squared_error: 1.3879 - val_loss: 1.8278 - val_root_mean_squared_error: 1.3520\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9197 - root_mean_squared_error: 1.3855 - val_loss: 1.8218 - val_root_mean_squared_error: 1.3497\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9274 - root_mean_squared_error: 1.3883 - val_loss: 1.8026 - val_root_mean_squared_error: 1.3426\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9219 - root_mean_squared_error: 1.3863 - val_loss: 1.8152 - val_root_mean_squared_error: 1.3473\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9262 - root_mean_squared_error: 1.3879 - val_loss: 1.8073 - val_root_mean_squared_error: 1.3443\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9249 - root_mean_squared_error: 1.3874 - val_loss: 1.8130 - val_root_mean_squared_error: 1.3465\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9199 - root_mean_squared_error: 1.3856 - val_loss: 1.8362 - val_root_mean_squared_error: 1.3551\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9199 - root_mean_squared_error: 1.3856 - val_loss: 1.8432 - val_root_mean_squared_error: 1.3576\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9262 - root_mean_squared_error: 1.3879 - val_loss: 1.8271 - val_root_mean_squared_error: 1.3517\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9329 - root_mean_squared_error: 1.3903 - val_loss: 1.8273 - val_root_mean_squared_error: 1.3518\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9169 - root_mean_squared_error: 1.3845 - val_loss: 1.9437 - val_root_mean_squared_error: 1.3942\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9273 - root_mean_squared_error: 1.3883 - val_loss: 1.8163 - val_root_mean_squared_error: 1.3477\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9189 - root_mean_squared_error: 1.3853 - val_loss: 1.8119 - val_root_mean_squared_error: 1.3461\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9231 - root_mean_squared_error: 1.3867 - val_loss: 1.8135 - val_root_mean_squared_error: 1.3467\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9239 - root_mean_squared_error: 1.3870 - val_loss: 1.8254 - val_root_mean_squared_error: 1.3511\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9186 - root_mean_squared_error: 1.3852 - val_loss: 1.8254 - val_root_mean_squared_error: 1.3511\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9237 - root_mean_squared_error: 1.3870 - val_loss: 1.8161 - val_root_mean_squared_error: 1.3476\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9211 - root_mean_squared_error: 1.3861 - val_loss: 1.8151 - val_root_mean_squared_error: 1.3473\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9219 - root_mean_squared_error: 1.3863 - val_loss: 1.8212 - val_root_mean_squared_error: 1.3495\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9205 - root_mean_squared_error: 1.3858 - val_loss: 1.8102 - val_root_mean_squared_error: 1.3454\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9191 - root_mean_squared_error: 1.3853 - val_loss: 1.8055 - val_root_mean_squared_error: 1.3437\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9156 - root_mean_squared_error: 1.3841 - val_loss: 1.8267 - val_root_mean_squared_error: 1.3515\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9161 - root_mean_squared_error: 1.3842 - val_loss: 1.8135 - val_root_mean_squared_error: 1.3467\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9201 - root_mean_squared_error: 1.3857 - val_loss: 1.8144 - val_root_mean_squared_error: 1.3470\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9222 - root_mean_squared_error: 1.3864 - val_loss: 1.8038 - val_root_mean_squared_error: 1.3431\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9227 - root_mean_squared_error: 1.3866 - val_loss: 1.8456 - val_root_mean_squared_error: 1.3585\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9238 - root_mean_squared_error: 1.3870 - val_loss: 1.8373 - val_root_mean_squared_error: 1.3555\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9141 - root_mean_squared_error: 1.3835 - val_loss: 1.8190 - val_root_mean_squared_error: 1.3487\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9206 - root_mean_squared_error: 1.3858 - val_loss: 1.8144 - val_root_mean_squared_error: 1.3470\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9145 - root_mean_squared_error: 1.3837 - val_loss: 1.8295 - val_root_mean_squared_error: 1.3526\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9238 - root_mean_squared_error: 1.3870 - val_loss: 1.8304 - val_root_mean_squared_error: 1.3529\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9174 - root_mean_squared_error: 1.3847 - val_loss: 1.8204 - val_root_mean_squared_error: 1.3492\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9164 - root_mean_squared_error: 1.3843 - val_loss: 1.8282 - val_root_mean_squared_error: 1.3521\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9189 - root_mean_squared_error: 1.3852 - val_loss: 1.8398 - val_root_mean_squared_error: 1.3564\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9171 - root_mean_squared_error: 1.3846 - val_loss: 1.8209 - val_root_mean_squared_error: 1.3494\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9152 - root_mean_squared_error: 1.3839 - val_loss: 1.8102 - val_root_mean_squared_error: 1.3454\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9218 - root_mean_squared_error: 1.3863 - val_loss: 1.8218 - val_root_mean_squared_error: 1.3497\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9194 - root_mean_squared_error: 1.3854 - val_loss: 1.8198 - val_root_mean_squared_error: 1.3490\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9217 - root_mean_squared_error: 1.3863 - val_loss: 1.8125 - val_root_mean_squared_error: 1.3463\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9195 - root_mean_squared_error: 1.3855 - val_loss: 1.8637 - val_root_mean_squared_error: 1.3652\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9149 - root_mean_squared_error: 1.3838 - val_loss: 1.8250 - val_root_mean_squared_error: 1.3509\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9169 - root_mean_squared_error: 1.3845 - val_loss: 1.8234 - val_root_mean_squared_error: 1.3503\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9259 - root_mean_squared_error: 1.3878 - val_loss: 1.8119 - val_root_mean_squared_error: 1.3461\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9141 - root_mean_squared_error: 1.3835 - val_loss: 1.8583 - val_root_mean_squared_error: 1.3632\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9165 - root_mean_squared_error: 1.3844 - val_loss: 1.8153 - val_root_mean_squared_error: 1.3473\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9164 - root_mean_squared_error: 1.3843 - val_loss: 1.8237 - val_root_mean_squared_error: 1.3504\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9158 - root_mean_squared_error: 1.3841 - val_loss: 1.8103 - val_root_mean_squared_error: 1.3455\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9182 - root_mean_squared_error: 1.3850 - val_loss: 1.8258 - val_root_mean_squared_error: 1.3512\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9209 - root_mean_squared_error: 1.3860 - val_loss: 1.8093 - val_root_mean_squared_error: 1.3451\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9168 - root_mean_squared_error: 1.3845 - val_loss: 1.8365 - val_root_mean_squared_error: 1.3552\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9117 - root_mean_squared_error: 1.3827 - val_loss: 1.8108 - val_root_mean_squared_error: 1.3456\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9153 - root_mean_squared_error: 1.3840 - val_loss: 1.8721 - val_root_mean_squared_error: 1.3682\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9225 - root_mean_squared_error: 1.3865 - val_loss: 1.8309 - val_root_mean_squared_error: 1.3531\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9190 - root_mean_squared_error: 1.3853 - val_loss: 1.8122 - val_root_mean_squared_error: 1.3462\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9179 - root_mean_squared_error: 1.3849 - val_loss: 1.7974 - val_root_mean_squared_error: 1.3407\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9148 - root_mean_squared_error: 1.3838 - val_loss: 1.8218 - val_root_mean_squared_error: 1.3498\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9127 - root_mean_squared_error: 1.3830 - val_loss: 1.8119 - val_root_mean_squared_error: 1.3461\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9246 - root_mean_squared_error: 1.3873 - val_loss: 1.8613 - val_root_mean_squared_error: 1.3643\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9157 - root_mean_squared_error: 1.3841 - val_loss: 1.8115 - val_root_mean_squared_error: 1.3459\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9099 - root_mean_squared_error: 1.3820 - val_loss: 1.8356 - val_root_mean_squared_error: 1.3549\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9142 - root_mean_squared_error: 1.3836 - val_loss: 1.8181 - val_root_mean_squared_error: 1.3484\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9211 - root_mean_squared_error: 1.3860 - val_loss: 1.8148 - val_root_mean_squared_error: 1.3471\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9195 - root_mean_squared_error: 1.3855 - val_loss: 1.8256 - val_root_mean_squared_error: 1.3512\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9198 - root_mean_squared_error: 1.3856 - val_loss: 1.8181 - val_root_mean_squared_error: 1.3484\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9190 - root_mean_squared_error: 1.3853 - val_loss: 1.8605 - val_root_mean_squared_error: 1.3640\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9224 - root_mean_squared_error: 1.3865 - val_loss: 1.8272 - val_root_mean_squared_error: 1.3517\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9201 - root_mean_squared_error: 1.3857 - val_loss: 1.8007 - val_root_mean_squared_error: 1.3419\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9099 - root_mean_squared_error: 1.3820 - val_loss: 1.8082 - val_root_mean_squared_error: 1.3447\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9219 - root_mean_squared_error: 1.3863 - val_loss: 1.8260 - val_root_mean_squared_error: 1.3513\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9182 - root_mean_squared_error: 1.3850 - val_loss: 1.8079 - val_root_mean_squared_error: 1.3446\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9131 - root_mean_squared_error: 1.3832 - val_loss: 1.8298 - val_root_mean_squared_error: 1.3527\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9080 - root_mean_squared_error: 1.3813 - val_loss: 1.8129 - val_root_mean_squared_error: 1.3464\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9173 - root_mean_squared_error: 1.3847 - val_loss: 1.8231 - val_root_mean_squared_error: 1.3502\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9190 - root_mean_squared_error: 1.3853 - val_loss: 1.8082 - val_root_mean_squared_error: 1.3447\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9158 - root_mean_squared_error: 1.3841 - val_loss: 1.8184 - val_root_mean_squared_error: 1.3485\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9180 - root_mean_squared_error: 1.3849 - val_loss: 1.8228 - val_root_mean_squared_error: 1.3501\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9154 - root_mean_squared_error: 1.3840 - val_loss: 1.8117 - val_root_mean_squared_error: 1.3460\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9150 - root_mean_squared_error: 1.3838 - val_loss: 1.8142 - val_root_mean_squared_error: 1.3469\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9228 - root_mean_squared_error: 1.3867 - val_loss: 1.8047 - val_root_mean_squared_error: 1.3434\n",
      "01-12 15:22:14 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 15:22:14 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 15:22:14 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112152005_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 15:22:14 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:22:14 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 15:22:14 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:22:14 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 3\n",
      "#########################\n",
      "01-12 15:22:14 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7b267493f0a0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 15:22:14 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 15:22:14 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:22:14 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 15:22:14 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 15:22:14 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.023424386978149414s\n",
      "01-12 15:22:14 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 15:22:14 I deeptables.m.preprocessor.py 383 - Imputation taken 0.1022951602935791s\n",
      "01-12 15:22:14 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 15:22:14 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.06243729591369629s\n",
      "01-12 15:22:14 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.24863171577453613s\n",
      "01-12 15:22:14 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 15:22:14 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 15:22:14 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:22:14 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:22:14 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:22:15 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 15:22:15 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 7s 9ms/step - loss: 5397.1978 - root_mean_squared_error: 73.4656 - val_loss: 83.8392 - val_root_mean_squared_error: 9.1564\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 29.7376 - root_mean_squared_error: 5.4532 - val_loss: 21.9983 - val_root_mean_squared_error: 4.6902\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 21.5767 - root_mean_squared_error: 4.6451 - val_loss: 20.6897 - val_root_mean_squared_error: 4.5486\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 20.1373 - root_mean_squared_error: 4.4875 - val_loss: 19.1651 - val_root_mean_squared_error: 4.3778\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 18.5092 - root_mean_squared_error: 4.3022 - val_loss: 17.2721 - val_root_mean_squared_error: 4.1560\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 16.8402 - root_mean_squared_error: 4.1037 - val_loss: 15.7989 - val_root_mean_squared_error: 3.9748\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 15.1931 - root_mean_squared_error: 3.8978 - val_loss: 14.1544 - val_root_mean_squared_error: 3.7622\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 13.5763 - root_mean_squared_error: 3.6846 - val_loss: 12.5714 - val_root_mean_squared_error: 3.5456\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 12.0966 - root_mean_squared_error: 3.4780 - val_loss: 11.2671 - val_root_mean_squared_error: 3.3567\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 10.7196 - root_mean_squared_error: 3.2741 - val_loss: 9.9992 - val_root_mean_squared_error: 3.1622\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 9.5393 - root_mean_squared_error: 3.0886 - val_loss: 8.8297 - val_root_mean_squared_error: 2.9715\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 8.5447 - root_mean_squared_error: 2.9231 - val_loss: 7.9896 - val_root_mean_squared_error: 2.8266\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 7.7015 - root_mean_squared_error: 2.7752 - val_loss: 7.2190 - val_root_mean_squared_error: 2.6868\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 7.0193 - root_mean_squared_error: 2.6494 - val_loss: 6.5935 - val_root_mean_squared_error: 2.5678\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 6.4357 - root_mean_squared_error: 2.5369 - val_loss: 6.0745 - val_root_mean_squared_error: 2.4647\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 5.9596 - root_mean_squared_error: 2.4412 - val_loss: 5.5984 - val_root_mean_squared_error: 2.3661\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 5.5167 - root_mean_squared_error: 2.3488 - val_loss: 5.1877 - val_root_mean_squared_error: 2.2777\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 5.1201 - root_mean_squared_error: 2.2628 - val_loss: 4.8531 - val_root_mean_squared_error: 2.2030\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.7762 - root_mean_squared_error: 2.1855 - val_loss: 4.5298 - val_root_mean_squared_error: 2.1283\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.4670 - root_mean_squared_error: 2.1135 - val_loss: 4.1792 - val_root_mean_squared_error: 2.0443\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.1686 - root_mean_squared_error: 2.0417 - val_loss: 3.9298 - val_root_mean_squared_error: 1.9824\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.8833 - root_mean_squared_error: 1.9706 - val_loss: 3.6250 - val_root_mean_squared_error: 1.9039\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.6361 - root_mean_squared_error: 1.9069 - val_loss: 3.4081 - val_root_mean_squared_error: 1.8461\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.4186 - root_mean_squared_error: 1.8490 - val_loss: 3.1751 - val_root_mean_squared_error: 1.7819\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.2100 - root_mean_squared_error: 1.7917 - val_loss: 3.0188 - val_root_mean_squared_error: 1.7375\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.0272 - root_mean_squared_error: 1.7399 - val_loss: 2.8409 - val_root_mean_squared_error: 1.6855\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.8663 - root_mean_squared_error: 1.6930 - val_loss: 2.7103 - val_root_mean_squared_error: 1.6463\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.7335 - root_mean_squared_error: 1.6533 - val_loss: 2.5299 - val_root_mean_squared_error: 1.5906\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.5976 - root_mean_squared_error: 1.6117 - val_loss: 2.4296 - val_root_mean_squared_error: 1.5587\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.5042 - root_mean_squared_error: 1.5825 - val_loss: 2.3169 - val_root_mean_squared_error: 1.5221\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.4069 - root_mean_squared_error: 1.5514 - val_loss: 2.2557 - val_root_mean_squared_error: 1.5019\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.3237 - root_mean_squared_error: 1.5244 - val_loss: 2.1914 - val_root_mean_squared_error: 1.4803\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.2628 - root_mean_squared_error: 1.5043 - val_loss: 2.1292 - val_root_mean_squared_error: 1.4592\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.2098 - root_mean_squared_error: 1.4865 - val_loss: 2.0556 - val_root_mean_squared_error: 1.4337\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1606 - root_mean_squared_error: 1.4699 - val_loss: 2.0529 - val_root_mean_squared_error: 1.4328\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1300 - root_mean_squared_error: 1.4594 - val_loss: 2.0021 - val_root_mean_squared_error: 1.4149\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 2.0970 - root_mean_squared_error: 1.4481 - val_loss: 1.9726 - val_root_mean_squared_error: 1.4045\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0723 - root_mean_squared_error: 1.4396 - val_loss: 1.9550 - val_root_mean_squared_error: 1.3982\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0485 - root_mean_squared_error: 1.4312 - val_loss: 1.9350 - val_root_mean_squared_error: 1.3910\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0451 - root_mean_squared_error: 1.4301 - val_loss: 1.9157 - val_root_mean_squared_error: 1.3841\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0291 - root_mean_squared_error: 1.4245 - val_loss: 1.9255 - val_root_mean_squared_error: 1.3876\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0321 - root_mean_squared_error: 1.4255 - val_loss: 1.9041 - val_root_mean_squared_error: 1.3799\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0136 - root_mean_squared_error: 1.4190 - val_loss: 1.8942 - val_root_mean_squared_error: 1.3763\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0191 - root_mean_squared_error: 1.4210 - val_loss: 1.9426 - val_root_mean_squared_error: 1.3938\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9974 - root_mean_squared_error: 1.4133 - val_loss: 1.8971 - val_root_mean_squared_error: 1.3774\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9903 - root_mean_squared_error: 1.4108 - val_loss: 1.9453 - val_root_mean_squared_error: 1.3947\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9966 - root_mean_squared_error: 1.4130 - val_loss: 1.8991 - val_root_mean_squared_error: 1.3781\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9807 - root_mean_squared_error: 1.4074 - val_loss: 1.8529 - val_root_mean_squared_error: 1.3612\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9832 - root_mean_squared_error: 1.4083 - val_loss: 1.8627 - val_root_mean_squared_error: 1.3648\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9827 - root_mean_squared_error: 1.4081 - val_loss: 1.8618 - val_root_mean_squared_error: 1.3645\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9802 - root_mean_squared_error: 1.4072 - val_loss: 1.8664 - val_root_mean_squared_error: 1.3662\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9609 - root_mean_squared_error: 1.4003 - val_loss: 1.8517 - val_root_mean_squared_error: 1.3608\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9836 - root_mean_squared_error: 1.4084 - val_loss: 1.8623 - val_root_mean_squared_error: 1.3647\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9713 - root_mean_squared_error: 1.4040 - val_loss: 1.8646 - val_root_mean_squared_error: 1.3655\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9622 - root_mean_squared_error: 1.4008 - val_loss: 1.9109 - val_root_mean_squared_error: 1.3824\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9853 - root_mean_squared_error: 1.4090 - val_loss: 1.8518 - val_root_mean_squared_error: 1.3608\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9886 - root_mean_squared_error: 1.4102 - val_loss: 1.9086 - val_root_mean_squared_error: 1.3815\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9650 - root_mean_squared_error: 1.4018 - val_loss: 1.8764 - val_root_mean_squared_error: 1.3698\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9866 - root_mean_squared_error: 1.4095 - val_loss: 1.8432 - val_root_mean_squared_error: 1.3577\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9734 - root_mean_squared_error: 1.4048 - val_loss: 1.9519 - val_root_mean_squared_error: 1.3971\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9867 - root_mean_squared_error: 1.4095 - val_loss: 1.8459 - val_root_mean_squared_error: 1.3586\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9942 - root_mean_squared_error: 1.4121 - val_loss: 1.8933 - val_root_mean_squared_error: 1.3760\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9587 - root_mean_squared_error: 1.3995 - val_loss: 1.8407 - val_root_mean_squared_error: 1.3567\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9566 - root_mean_squared_error: 1.3988 - val_loss: 1.8307 - val_root_mean_squared_error: 1.3530\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9512 - root_mean_squared_error: 1.3969 - val_loss: 1.8859 - val_root_mean_squared_error: 1.3733\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9722 - root_mean_squared_error: 1.4044 - val_loss: 1.8360 - val_root_mean_squared_error: 1.3550\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9597 - root_mean_squared_error: 1.3999 - val_loss: 1.8466 - val_root_mean_squared_error: 1.3589\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9598 - root_mean_squared_error: 1.3999 - val_loss: 1.9071 - val_root_mean_squared_error: 1.3810\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9561 - root_mean_squared_error: 1.3986 - val_loss: 1.9507 - val_root_mean_squared_error: 1.3967\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9543 - root_mean_squared_error: 1.3980 - val_loss: 1.8498 - val_root_mean_squared_error: 1.3601\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9629 - root_mean_squared_error: 1.4010 - val_loss: 1.8059 - val_root_mean_squared_error: 1.3438\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9601 - root_mean_squared_error: 1.4000 - val_loss: 1.8196 - val_root_mean_squared_error: 1.3489\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9817 - root_mean_squared_error: 1.4077 - val_loss: 1.8349 - val_root_mean_squared_error: 1.3546\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9754 - root_mean_squared_error: 1.4055 - val_loss: 1.9467 - val_root_mean_squared_error: 1.3953\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9634 - root_mean_squared_error: 1.4012 - val_loss: 2.0245 - val_root_mean_squared_error: 1.4229\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9517 - root_mean_squared_error: 1.3970 - val_loss: 1.8288 - val_root_mean_squared_error: 1.3523\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9783 - root_mean_squared_error: 1.4065 - val_loss: 1.8548 - val_root_mean_squared_error: 1.3619\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9698 - root_mean_squared_error: 1.4035 - val_loss: 1.8965 - val_root_mean_squared_error: 1.3771\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9678 - root_mean_squared_error: 1.4028 - val_loss: 1.8519 - val_root_mean_squared_error: 1.3608\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9614 - root_mean_squared_error: 1.4005 - val_loss: 1.8250 - val_root_mean_squared_error: 1.3509\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9715 - root_mean_squared_error: 1.4041 - val_loss: 1.8571 - val_root_mean_squared_error: 1.3627\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9652 - root_mean_squared_error: 1.4018 - val_loss: 1.9349 - val_root_mean_squared_error: 1.3910\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9579 - root_mean_squared_error: 1.3992 - val_loss: 1.8431 - val_root_mean_squared_error: 1.3576\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9671 - root_mean_squared_error: 1.4025 - val_loss: 1.8375 - val_root_mean_squared_error: 1.3555\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9592 - root_mean_squared_error: 1.3997 - val_loss: 1.8363 - val_root_mean_squared_error: 1.3551\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9535 - root_mean_squared_error: 1.3977 - val_loss: 1.8710 - val_root_mean_squared_error: 1.3678\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9460 - root_mean_squared_error: 1.3950 - val_loss: 1.8258 - val_root_mean_squared_error: 1.3512\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9351 - root_mean_squared_error: 1.3911 - val_loss: 1.8845 - val_root_mean_squared_error: 1.3728\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9602 - root_mean_squared_error: 1.4001 - val_loss: 1.8286 - val_root_mean_squared_error: 1.3523\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9570 - root_mean_squared_error: 1.3989 - val_loss: 1.8513 - val_root_mean_squared_error: 1.3606\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9478 - root_mean_squared_error: 1.3956 - val_loss: 1.8522 - val_root_mean_squared_error: 1.3610\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9338 - root_mean_squared_error: 1.3906 - val_loss: 1.8531 - val_root_mean_squared_error: 1.3613\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9645 - root_mean_squared_error: 1.4016 - val_loss: 1.8375 - val_root_mean_squared_error: 1.3555\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9456 - root_mean_squared_error: 1.3949 - val_loss: 1.8595 - val_root_mean_squared_error: 1.3636\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9354 - root_mean_squared_error: 1.3912 - val_loss: 1.8206 - val_root_mean_squared_error: 1.3493\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9728 - root_mean_squared_error: 1.4046 - val_loss: 1.8337 - val_root_mean_squared_error: 1.3542\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9652 - root_mean_squared_error: 1.4019 - val_loss: 1.8201 - val_root_mean_squared_error: 1.3491\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9762 - root_mean_squared_error: 1.4058 - val_loss: 1.8375 - val_root_mean_squared_error: 1.3555\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9445 - root_mean_squared_error: 1.3944 - val_loss: 1.9006 - val_root_mean_squared_error: 1.3786\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9599 - root_mean_squared_error: 1.4000 - val_loss: 1.8711 - val_root_mean_squared_error: 1.3679\n",
      "01-12 15:24:17 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 15:24:17 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 15:24:17 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112152214_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 15:24:17 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:24:17 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 15:24:17 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:24:17 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 4\n",
      "#########################\n",
      "01-12 15:24:17 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7b26697a6c50>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 15:24:17 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 15:24:17 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:24:17 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 15:24:17 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 15:24:17 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.025034427642822266s\n",
      "01-12 15:24:17 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 15:24:17 I deeptables.m.preprocessor.py 383 - Imputation taken 0.10029053688049316s\n",
      "01-12 15:24:17 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 15:24:18 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.06537938117980957s\n",
      "01-12 15:24:18 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.2559349536895752s\n",
      "01-12 15:24:18 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 15:24:18 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 15:24:18 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:24:18 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:24:18 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:24:18 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 19, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 15:24:18 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 9s 10ms/step - loss: 7619.6274 - root_mean_squared_error: 87.2905 - val_loss: 141.8083 - val_root_mean_squared_error: 11.9083\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 41.5567 - root_mean_squared_error: 6.4465 - val_loss: 26.9102 - val_root_mean_squared_error: 5.1875\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 27.8271 - root_mean_squared_error: 5.2751 - val_loss: 26.4190 - val_root_mean_squared_error: 5.1399\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 26.9279 - root_mean_squared_error: 5.1892 - val_loss: 25.6291 - val_root_mean_squared_error: 5.0625\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 25.9369 - root_mean_squared_error: 5.0928 - val_loss: 24.5433 - val_root_mean_squared_error: 4.9541\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 24.8388 - root_mean_squared_error: 4.9839 - val_loss: 23.3074 - val_root_mean_squared_error: 4.8278\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 23.7289 - root_mean_squared_error: 4.8712 - val_loss: 22.1002 - val_root_mean_squared_error: 4.7011\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 22.5426 - root_mean_squared_error: 4.7479 - val_loss: 21.3497 - val_root_mean_squared_error: 4.6206\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 21.2969 - root_mean_squared_error: 4.6149 - val_loss: 20.1512 - val_root_mean_squared_error: 4.4890\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 20.0967 - root_mean_squared_error: 4.4829 - val_loss: 18.9175 - val_root_mean_squared_error: 4.3494\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 18.8653 - root_mean_squared_error: 4.3434 - val_loss: 17.3461 - val_root_mean_squared_error: 4.1649\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 17.6777 - root_mean_squared_error: 4.2045 - val_loss: 16.4954 - val_root_mean_squared_error: 4.0615\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 16.4597 - root_mean_squared_error: 4.0571 - val_loss: 15.2306 - val_root_mean_squared_error: 3.9026\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 15.2985 - root_mean_squared_error: 3.9113 - val_loss: 14.0771 - val_root_mean_squared_error: 3.7519\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 14.1874 - root_mean_squared_error: 3.7666 - val_loss: 13.1733 - val_root_mean_squared_error: 3.6295\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 13.1015 - root_mean_squared_error: 3.6196 - val_loss: 12.0622 - val_root_mean_squared_error: 3.4731\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 12.0461 - root_mean_squared_error: 3.4708 - val_loss: 11.1707 - val_root_mean_squared_error: 3.3423\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 11.0412 - root_mean_squared_error: 3.3228 - val_loss: 10.1516 - val_root_mean_squared_error: 3.1862\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 10.0833 - root_mean_squared_error: 3.1754 - val_loss: 9.2611 - val_root_mean_squared_error: 3.0432\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 9.2017 - root_mean_squared_error: 3.0334 - val_loss: 8.4608 - val_root_mean_squared_error: 2.9087\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 8.3318 - root_mean_squared_error: 2.8865 - val_loss: 7.6903 - val_root_mean_squared_error: 2.7731\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 7.5341 - root_mean_squared_error: 2.7448 - val_loss: 6.9707 - val_root_mean_squared_error: 2.6402\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 6.7818 - root_mean_squared_error: 2.6042 - val_loss: 6.1483 - val_root_mean_squared_error: 2.4796\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 6.0945 - root_mean_squared_error: 2.4687 - val_loss: 5.5396 - val_root_mean_squared_error: 2.3536\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 5.4622 - root_mean_squared_error: 2.3371 - val_loss: 4.9471 - val_root_mean_squared_error: 2.2242\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.9107 - root_mean_squared_error: 2.2160 - val_loss: 4.3948 - val_root_mean_squared_error: 2.0964\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.3904 - root_mean_squared_error: 2.0953 - val_loss: 4.0061 - val_root_mean_squared_error: 2.0015\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.9488 - root_mean_squared_error: 1.9872 - val_loss: 3.5914 - val_root_mean_squared_error: 1.8951\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.5784 - root_mean_squared_error: 1.8917 - val_loss: 3.2991 - val_root_mean_squared_error: 1.8164\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.2410 - root_mean_squared_error: 1.8003 - val_loss: 2.9993 - val_root_mean_squared_error: 1.7318\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.9614 - root_mean_squared_error: 1.7209 - val_loss: 2.7617 - val_root_mean_squared_error: 1.6618\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.7231 - root_mean_squared_error: 1.6502 - val_loss: 2.5553 - val_root_mean_squared_error: 1.5985\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.5417 - root_mean_squared_error: 1.5943 - val_loss: 2.4043 - val_root_mean_squared_error: 1.5506\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.4053 - root_mean_squared_error: 1.5509 - val_loss: 2.2967 - val_root_mean_squared_error: 1.5155\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.2780 - root_mean_squared_error: 1.5093 - val_loss: 2.1829 - val_root_mean_squared_error: 1.4775\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1857 - root_mean_squared_error: 1.4784 - val_loss: 2.0974 - val_root_mean_squared_error: 1.4483\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1149 - root_mean_squared_error: 1.4543 - val_loss: 2.1034 - val_root_mean_squared_error: 1.4503\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0607 - root_mean_squared_error: 1.4355 - val_loss: 2.0598 - val_root_mean_squared_error: 1.4352\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0341 - root_mean_squared_error: 1.4262 - val_loss: 2.0109 - val_root_mean_squared_error: 1.4181\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0185 - root_mean_squared_error: 1.4208 - val_loss: 2.0074 - val_root_mean_squared_error: 1.4168\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9902 - root_mean_squared_error: 1.4108 - val_loss: 1.9779 - val_root_mean_squared_error: 1.4064\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9843 - root_mean_squared_error: 1.4087 - val_loss: 1.9882 - val_root_mean_squared_error: 1.4100\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9790 - root_mean_squared_error: 1.4068 - val_loss: 1.9758 - val_root_mean_squared_error: 1.4056\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9566 - root_mean_squared_error: 1.3988 - val_loss: 1.9923 - val_root_mean_squared_error: 1.4115\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9646 - root_mean_squared_error: 1.4016 - val_loss: 1.9574 - val_root_mean_squared_error: 1.3991\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9685 - root_mean_squared_error: 1.4030 - val_loss: 1.9951 - val_root_mean_squared_error: 1.4125\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9594 - root_mean_squared_error: 1.3998 - val_loss: 1.9750 - val_root_mean_squared_error: 1.4053\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9560 - root_mean_squared_error: 1.3986 - val_loss: 1.9818 - val_root_mean_squared_error: 1.4078\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9554 - root_mean_squared_error: 1.3984 - val_loss: 1.9993 - val_root_mean_squared_error: 1.4140\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9433 - root_mean_squared_error: 1.3940 - val_loss: 1.9550 - val_root_mean_squared_error: 1.3982\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9365 - root_mean_squared_error: 1.3916 - val_loss: 1.9774 - val_root_mean_squared_error: 1.4062\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9501 - root_mean_squared_error: 1.3965 - val_loss: 1.9858 - val_root_mean_squared_error: 1.4092\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9576 - root_mean_squared_error: 1.3991 - val_loss: 2.0907 - val_root_mean_squared_error: 1.4459\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9403 - root_mean_squared_error: 1.3930 - val_loss: 1.9846 - val_root_mean_squared_error: 1.4088\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9442 - root_mean_squared_error: 1.3943 - val_loss: 1.9588 - val_root_mean_squared_error: 1.3996\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9452 - root_mean_squared_error: 1.3947 - val_loss: 2.0459 - val_root_mean_squared_error: 1.4304\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9589 - root_mean_squared_error: 1.3996 - val_loss: 1.9742 - val_root_mean_squared_error: 1.4050\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9651 - root_mean_squared_error: 1.4018 - val_loss: 1.9633 - val_root_mean_squared_error: 1.4012\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9505 - root_mean_squared_error: 1.3966 - val_loss: 1.9845 - val_root_mean_squared_error: 1.4087\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9398 - root_mean_squared_error: 1.3928 - val_loss: 1.9561 - val_root_mean_squared_error: 1.3986\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9345 - root_mean_squared_error: 1.3909 - val_loss: 1.9671 - val_root_mean_squared_error: 1.4025\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9541 - root_mean_squared_error: 1.3979 - val_loss: 2.1298 - val_root_mean_squared_error: 1.4594\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9725 - root_mean_squared_error: 1.4045 - val_loss: 2.0450 - val_root_mean_squared_error: 1.4300\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9665 - root_mean_squared_error: 1.4023 - val_loss: 2.0092 - val_root_mean_squared_error: 1.4175\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9624 - root_mean_squared_error: 1.4009 - val_loss: 2.0196 - val_root_mean_squared_error: 1.4211\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9618 - root_mean_squared_error: 1.4007 - val_loss: 2.0331 - val_root_mean_squared_error: 1.4259\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9469 - root_mean_squared_error: 1.3953 - val_loss: 2.0066 - val_root_mean_squared_error: 1.4165\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9376 - root_mean_squared_error: 1.3920 - val_loss: 2.0226 - val_root_mean_squared_error: 1.4222\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9538 - root_mean_squared_error: 1.3978 - val_loss: 1.9629 - val_root_mean_squared_error: 1.4010\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9552 - root_mean_squared_error: 1.3983 - val_loss: 1.9965 - val_root_mean_squared_error: 1.4130\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9762 - root_mean_squared_error: 1.4058 - val_loss: 2.0177 - val_root_mean_squared_error: 1.4204\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9345 - root_mean_squared_error: 1.3909 - val_loss: 1.9955 - val_root_mean_squared_error: 1.4126\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9216 - root_mean_squared_error: 1.3862 - val_loss: 1.9773 - val_root_mean_squared_error: 1.4062\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9424 - root_mean_squared_error: 1.3937 - val_loss: 2.0135 - val_root_mean_squared_error: 1.4190\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9428 - root_mean_squared_error: 1.3938 - val_loss: 1.9782 - val_root_mean_squared_error: 1.4065\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9204 - root_mean_squared_error: 1.3858 - val_loss: 1.9754 - val_root_mean_squared_error: 1.4055\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9341 - root_mean_squared_error: 1.3907 - val_loss: 1.9855 - val_root_mean_squared_error: 1.4091\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9727 - root_mean_squared_error: 1.4045 - val_loss: 1.9614 - val_root_mean_squared_error: 1.4005\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9311 - root_mean_squared_error: 1.3897 - val_loss: 2.1165 - val_root_mean_squared_error: 1.4548\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9629 - root_mean_squared_error: 1.4010 - val_loss: 1.9453 - val_root_mean_squared_error: 1.3947\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9185 - root_mean_squared_error: 1.3851 - val_loss: 1.9717 - val_root_mean_squared_error: 1.4042\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9403 - root_mean_squared_error: 1.3929 - val_loss: 1.9599 - val_root_mean_squared_error: 1.4000\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9282 - root_mean_squared_error: 1.3886 - val_loss: 2.0145 - val_root_mean_squared_error: 1.4193\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9490 - root_mean_squared_error: 1.3961 - val_loss: 2.0122 - val_root_mean_squared_error: 1.4185\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9360 - root_mean_squared_error: 1.3914 - val_loss: 2.0461 - val_root_mean_squared_error: 1.4304\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9259 - root_mean_squared_error: 1.3878 - val_loss: 2.0564 - val_root_mean_squared_error: 1.4340\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9317 - root_mean_squared_error: 1.3899 - val_loss: 1.9492 - val_root_mean_squared_error: 1.3961\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9280 - root_mean_squared_error: 1.3885 - val_loss: 2.0378 - val_root_mean_squared_error: 1.4275\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9329 - root_mean_squared_error: 1.3903 - val_loss: 1.9676 - val_root_mean_squared_error: 1.4027\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9197 - root_mean_squared_error: 1.3855 - val_loss: 1.9881 - val_root_mean_squared_error: 1.4100\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9553 - root_mean_squared_error: 1.3983 - val_loss: 2.0370 - val_root_mean_squared_error: 1.4272\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9537 - root_mean_squared_error: 1.3978 - val_loss: 1.9964 - val_root_mean_squared_error: 1.4129\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9289 - root_mean_squared_error: 1.3889 - val_loss: 1.9642 - val_root_mean_squared_error: 1.4015\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9421 - root_mean_squared_error: 1.3936 - val_loss: 1.9611 - val_root_mean_squared_error: 1.4004\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9397 - root_mean_squared_error: 1.3927 - val_loss: 2.0300 - val_root_mean_squared_error: 1.4248\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9727 - root_mean_squared_error: 1.4045 - val_loss: 1.9589 - val_root_mean_squared_error: 1.3996\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9026 - root_mean_squared_error: 1.3794 - val_loss: 1.9666 - val_root_mean_squared_error: 1.4024\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9476 - root_mean_squared_error: 1.3956 - val_loss: 1.9369 - val_root_mean_squared_error: 1.3917\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9418 - root_mean_squared_error: 1.3935 - val_loss: 1.9647 - val_root_mean_squared_error: 1.4017\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9311 - root_mean_squared_error: 1.3896 - val_loss: 2.0444 - val_root_mean_squared_error: 1.4298\n",
      "01-12 15:26:28 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 15:26:28 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 15:26:28 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112152417_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 15:26:28 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:26:28 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 15:26:28 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:26:28 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 5\n",
      "#########################\n",
      "01-12 15:26:28 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7b266e94ed40>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 15:26:28 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 15:26:28 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:26:28 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 15:26:28 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 15:26:28 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.024079561233520508s\n",
      "01-12 15:26:28 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 15:26:28 I deeptables.m.preprocessor.py 383 - Imputation taken 0.10550928115844727s\n",
      "01-12 15:26:28 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 15:26:28 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.06475162506103516s\n",
      "01-12 15:26:28 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.2571134567260742s\n",
      "01-12 15:26:28 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 15:26:28 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 15:26:28 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:26:28 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:26:28 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:26:29 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 15:26:29 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 8s 10ms/step - loss: 502.4462 - root_mean_squared_error: 22.4153 - val_loss: 100.2286 - val_root_mean_squared_error: 10.0114\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 85.9168 - root_mean_squared_error: 9.2691 - val_loss: 68.3586 - val_root_mean_squared_error: 8.2679\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 56.2222 - root_mean_squared_error: 7.4981 - val_loss: 43.1736 - val_root_mean_squared_error: 6.5707\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 37.5510 - root_mean_squared_error: 6.1279 - val_loss: 30.2432 - val_root_mean_squared_error: 5.4994\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 27.3875 - root_mean_squared_error: 5.2333 - val_loss: 22.1218 - val_root_mean_squared_error: 4.7034\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 20.4395 - root_mean_squared_error: 4.5210 - val_loss: 16.5498 - val_root_mean_squared_error: 4.0681\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 15.2126 - root_mean_squared_error: 3.9003 - val_loss: 12.3720 - val_root_mean_squared_error: 3.5174\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 11.3899 - root_mean_squared_error: 3.3749 - val_loss: 9.3535 - val_root_mean_squared_error: 3.0584\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 8.6002 - root_mean_squared_error: 2.9326 - val_loss: 7.0305 - val_root_mean_squared_error: 2.6515\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 6.5812 - root_mean_squared_error: 2.5654 - val_loss: 5.5078 - val_root_mean_squared_error: 2.3469\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.1650 - root_mean_squared_error: 2.2727 - val_loss: 4.3860 - val_root_mean_squared_error: 2.0943\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.1715 - root_mean_squared_error: 2.0424 - val_loss: 3.6496 - val_root_mean_squared_error: 1.9104\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.4995 - root_mean_squared_error: 1.8707 - val_loss: 3.1362 - val_root_mean_squared_error: 1.7709\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.0455 - root_mean_squared_error: 1.7451 - val_loss: 2.8149 - val_root_mean_squared_error: 1.6778\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.7375 - root_mean_squared_error: 1.6546 - val_loss: 2.5647 - val_root_mean_squared_error: 1.6015\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.5196 - root_mean_squared_error: 1.5873 - val_loss: 2.4395 - val_root_mean_squared_error: 1.5619\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.3901 - root_mean_squared_error: 1.5460 - val_loss: 2.3463 - val_root_mean_squared_error: 1.5318\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.2882 - root_mean_squared_error: 1.5127 - val_loss: 2.2883 - val_root_mean_squared_error: 1.5127\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.2309 - root_mean_squared_error: 1.4936 - val_loss: 2.2118 - val_root_mean_squared_error: 1.4872\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1954 - root_mean_squared_error: 1.4817 - val_loss: 2.2166 - val_root_mean_squared_error: 1.4888\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1748 - root_mean_squared_error: 1.4747 - val_loss: 2.1505 - val_root_mean_squared_error: 1.4664\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.1389 - root_mean_squared_error: 1.4625 - val_loss: 2.1538 - val_root_mean_squared_error: 1.4676\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.1222 - root_mean_squared_error: 1.4568 - val_loss: 2.1569 - val_root_mean_squared_error: 1.4687\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0973 - root_mean_squared_error: 1.4482 - val_loss: 2.1132 - val_root_mean_squared_error: 1.4537\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.1002 - root_mean_squared_error: 1.4492 - val_loss: 2.1157 - val_root_mean_squared_error: 1.4546\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0876 - root_mean_squared_error: 1.4449 - val_loss: 2.1341 - val_root_mean_squared_error: 1.4608\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0860 - root_mean_squared_error: 1.4443 - val_loss: 2.0997 - val_root_mean_squared_error: 1.4490\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0684 - root_mean_squared_error: 1.4382 - val_loss: 2.0876 - val_root_mean_squared_error: 1.4449\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0427 - root_mean_squared_error: 1.4292 - val_loss: 2.0797 - val_root_mean_squared_error: 1.4421\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0651 - root_mean_squared_error: 1.4370 - val_loss: 2.1029 - val_root_mean_squared_error: 1.4501\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0336 - root_mean_squared_error: 1.4261 - val_loss: 2.0660 - val_root_mean_squared_error: 1.4374\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0226 - root_mean_squared_error: 1.4222 - val_loss: 2.0353 - val_root_mean_squared_error: 1.4266\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0071 - root_mean_squared_error: 1.4167 - val_loss: 2.0273 - val_root_mean_squared_error: 1.4238\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0034 - root_mean_squared_error: 1.4154 - val_loss: 2.0186 - val_root_mean_squared_error: 1.4208\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 2.0032 - root_mean_squared_error: 1.4153 - val_loss: 2.0038 - val_root_mean_squared_error: 1.4156\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9866 - root_mean_squared_error: 1.4095 - val_loss: 2.0152 - val_root_mean_squared_error: 1.4196\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 2.0071 - root_mean_squared_error: 1.4167 - val_loss: 2.1224 - val_root_mean_squared_error: 1.4569\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9971 - root_mean_squared_error: 1.4132 - val_loss: 1.9864 - val_root_mean_squared_error: 1.4094\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9901 - root_mean_squared_error: 1.4107 - val_loss: 2.0202 - val_root_mean_squared_error: 1.4213\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9964 - root_mean_squared_error: 1.4129 - val_loss: 2.0374 - val_root_mean_squared_error: 1.4274\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9630 - root_mean_squared_error: 1.4011 - val_loss: 2.0927 - val_root_mean_squared_error: 1.4466\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9724 - root_mean_squared_error: 1.4044 - val_loss: 1.9946 - val_root_mean_squared_error: 1.4123\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0231 - root_mean_squared_error: 1.4224 - val_loss: 1.9725 - val_root_mean_squared_error: 1.4045\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0025 - root_mean_squared_error: 1.4151 - val_loss: 2.0534 - val_root_mean_squared_error: 1.4330\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9676 - root_mean_squared_error: 1.4027 - val_loss: 1.9923 - val_root_mean_squared_error: 1.4115\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9584 - root_mean_squared_error: 1.3994 - val_loss: 1.9620 - val_root_mean_squared_error: 1.4007\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9669 - root_mean_squared_error: 1.4024 - val_loss: 1.9745 - val_root_mean_squared_error: 1.4052\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0466 - root_mean_squared_error: 1.4306 - val_loss: 1.9883 - val_root_mean_squared_error: 1.4101\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9680 - root_mean_squared_error: 1.4029 - val_loss: 1.9364 - val_root_mean_squared_error: 1.3915\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9910 - root_mean_squared_error: 1.4110 - val_loss: 1.9616 - val_root_mean_squared_error: 1.4006\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9501 - root_mean_squared_error: 1.3965 - val_loss: 2.0675 - val_root_mean_squared_error: 1.4379\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9605 - root_mean_squared_error: 1.4002 - val_loss: 2.1201 - val_root_mean_squared_error: 1.4561\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0056 - root_mean_squared_error: 1.4162 - val_loss: 2.1527 - val_root_mean_squared_error: 1.4672\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9593 - root_mean_squared_error: 1.3998 - val_loss: 1.9635 - val_root_mean_squared_error: 1.4012\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9621 - root_mean_squared_error: 1.4007 - val_loss: 2.0259 - val_root_mean_squared_error: 1.4233\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9515 - root_mean_squared_error: 1.3970 - val_loss: 2.0950 - val_root_mean_squared_error: 1.4474\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9900 - root_mean_squared_error: 1.4107 - val_loss: 2.0360 - val_root_mean_squared_error: 1.4269\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0051 - root_mean_squared_error: 1.4160 - val_loss: 2.2228 - val_root_mean_squared_error: 1.4909\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0185 - root_mean_squared_error: 1.4207 - val_loss: 2.3435 - val_root_mean_squared_error: 1.5308\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0096 - root_mean_squared_error: 1.4176 - val_loss: 1.9619 - val_root_mean_squared_error: 1.4007\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9548 - root_mean_squared_error: 1.3981 - val_loss: 1.9913 - val_root_mean_squared_error: 1.4111\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9955 - root_mean_squared_error: 1.4126 - val_loss: 2.1447 - val_root_mean_squared_error: 1.4645\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9727 - root_mean_squared_error: 1.4045 - val_loss: 2.0067 - val_root_mean_squared_error: 1.4166\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9490 - root_mean_squared_error: 1.3961 - val_loss: 1.9691 - val_root_mean_squared_error: 1.4033\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9922 - root_mean_squared_error: 1.4114 - val_loss: 2.2327 - val_root_mean_squared_error: 1.4942\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9815 - root_mean_squared_error: 1.4077 - val_loss: 2.0149 - val_root_mean_squared_error: 1.4195\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9720 - root_mean_squared_error: 1.4043 - val_loss: 1.9745 - val_root_mean_squared_error: 1.4052\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9699 - root_mean_squared_error: 1.4035 - val_loss: 2.0432 - val_root_mean_squared_error: 1.4294\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9771 - root_mean_squared_error: 1.4061 - val_loss: 1.9469 - val_root_mean_squared_error: 1.3953\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9625 - root_mean_squared_error: 1.4009 - val_loss: 1.9898 - val_root_mean_squared_error: 1.4106\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9385 - root_mean_squared_error: 1.3923 - val_loss: 1.9756 - val_root_mean_squared_error: 1.4056\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9661 - root_mean_squared_error: 1.4022 - val_loss: 2.0888 - val_root_mean_squared_error: 1.4453\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9706 - root_mean_squared_error: 1.4038 - val_loss: 1.9559 - val_root_mean_squared_error: 1.3985\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9656 - root_mean_squared_error: 1.4020 - val_loss: 1.9685 - val_root_mean_squared_error: 1.4030\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9935 - root_mean_squared_error: 1.4119 - val_loss: 1.9431 - val_root_mean_squared_error: 1.3940\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9993 - root_mean_squared_error: 1.4140 - val_loss: 2.4071 - val_root_mean_squared_error: 1.5515\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9603 - root_mean_squared_error: 1.4001 - val_loss: 2.0957 - val_root_mean_squared_error: 1.4476\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9383 - root_mean_squared_error: 1.3922 - val_loss: 1.9487 - val_root_mean_squared_error: 1.3960\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9281 - root_mean_squared_error: 1.3886 - val_loss: 1.9422 - val_root_mean_squared_error: 1.3936\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9765 - root_mean_squared_error: 1.4059 - val_loss: 1.9795 - val_root_mean_squared_error: 1.4069\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9350 - root_mean_squared_error: 1.3911 - val_loss: 1.9516 - val_root_mean_squared_error: 1.3970\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9350 - root_mean_squared_error: 1.3910 - val_loss: 1.9443 - val_root_mean_squared_error: 1.3944\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9270 - root_mean_squared_error: 1.3882 - val_loss: 2.1407 - val_root_mean_squared_error: 1.4631\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9372 - root_mean_squared_error: 1.3919 - val_loss: 2.1410 - val_root_mean_squared_error: 1.4632\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9721 - root_mean_squared_error: 1.4043 - val_loss: 2.1912 - val_root_mean_squared_error: 1.4803\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9422 - root_mean_squared_error: 1.3936 - val_loss: 2.0218 - val_root_mean_squared_error: 1.4219\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9406 - root_mean_squared_error: 1.3931 - val_loss: 1.9551 - val_root_mean_squared_error: 1.3982\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9409 - root_mean_squared_error: 1.3932 - val_loss: 2.1206 - val_root_mean_squared_error: 1.4562\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9382 - root_mean_squared_error: 1.3922 - val_loss: 2.0110 - val_root_mean_squared_error: 1.4181\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9171 - root_mean_squared_error: 1.3846 - val_loss: 1.9376 - val_root_mean_squared_error: 1.3920\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9264 - root_mean_squared_error: 1.3880 - val_loss: 1.9906 - val_root_mean_squared_error: 1.4109\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9266 - root_mean_squared_error: 1.3880 - val_loss: 1.9384 - val_root_mean_squared_error: 1.3923\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9116 - root_mean_squared_error: 1.3826 - val_loss: 1.9749 - val_root_mean_squared_error: 1.4053\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9213 - root_mean_squared_error: 1.3861 - val_loss: 1.9865 - val_root_mean_squared_error: 1.4094\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9234 - root_mean_squared_error: 1.3869 - val_loss: 2.0091 - val_root_mean_squared_error: 1.4174\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9134 - root_mean_squared_error: 1.3833 - val_loss: 2.0236 - val_root_mean_squared_error: 1.4225\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9027 - root_mean_squared_error: 1.3794 - val_loss: 2.0071 - val_root_mean_squared_error: 1.4167\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9078 - root_mean_squared_error: 1.3812 - val_loss: 1.9774 - val_root_mean_squared_error: 1.4062\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9225 - root_mean_squared_error: 1.3866 - val_loss: 1.9600 - val_root_mean_squared_error: 1.4000\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9091 - root_mean_squared_error: 1.3817 - val_loss: 2.0115 - val_root_mean_squared_error: 1.4183\n",
      "01-12 15:28:36 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 15:28:36 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 15:28:36 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112152628_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 15:28:36 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:28:36 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 15:28:36 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:28:36 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 6\n",
      "#########################\n",
      "01-12 15:28:36 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7b266bbaf3d0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 15:28:36 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 15:28:36 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:28:36 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 15:28:36 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 15:28:36 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.02573251724243164s\n",
      "01-12 15:28:36 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 15:28:36 I deeptables.m.preprocessor.py 383 - Imputation taken 0.10820889472961426s\n",
      "01-12 15:28:36 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 15:28:36 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.064117431640625s\n",
      "01-12 15:28:36 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.26132822036743164s\n",
      "01-12 15:28:36 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 15:28:36 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 15:28:36 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:28:36 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:28:36 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:28:37 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 15:28:37 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 8s 11ms/step - loss: 13.9992 - root_mean_squared_error: 3.7416 - val_loss: 6.5576 - val_root_mean_squared_error: 2.5608\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 5.8505 - root_mean_squared_error: 2.4188 - val_loss: 4.7113 - val_root_mean_squared_error: 2.1705\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.3167 - root_mean_squared_error: 2.0777 - val_loss: 3.6607 - val_root_mean_squared_error: 1.9133\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.4444 - root_mean_squared_error: 1.8559 - val_loss: 3.0467 - val_root_mean_squared_error: 1.7455\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.9391 - root_mean_squared_error: 1.7144 - val_loss: 2.7196 - val_root_mean_squared_error: 1.6491\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.6142 - root_mean_squared_error: 1.6169 - val_loss: 2.4623 - val_root_mean_squared_error: 1.5692\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.4067 - root_mean_squared_error: 1.5514 - val_loss: 2.3259 - val_root_mean_squared_error: 1.5251\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.2664 - root_mean_squared_error: 1.5054 - val_loss: 2.1966 - val_root_mean_squared_error: 1.4821\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.1742 - root_mean_squared_error: 1.4745 - val_loss: 2.1188 - val_root_mean_squared_error: 1.4556\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1132 - root_mean_squared_error: 1.4537 - val_loss: 2.0948 - val_root_mean_squared_error: 1.4473\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0661 - root_mean_squared_error: 1.4374 - val_loss: 2.0592 - val_root_mean_squared_error: 1.4350\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0315 - root_mean_squared_error: 1.4253 - val_loss: 2.0137 - val_root_mean_squared_error: 1.4191\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0025 - root_mean_squared_error: 1.4151 - val_loss: 2.0268 - val_root_mean_squared_error: 1.4236\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9804 - root_mean_squared_error: 1.4073 - val_loss: 1.9831 - val_root_mean_squared_error: 1.4082\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9621 - root_mean_squared_error: 1.4007 - val_loss: 1.9819 - val_root_mean_squared_error: 1.4078\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9515 - root_mean_squared_error: 1.3970 - val_loss: 1.9701 - val_root_mean_squared_error: 1.4036\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9436 - root_mean_squared_error: 1.3941 - val_loss: 1.9597 - val_root_mean_squared_error: 1.3999\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9339 - root_mean_squared_error: 1.3907 - val_loss: 1.9601 - val_root_mean_squared_error: 1.4000\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9245 - root_mean_squared_error: 1.3873 - val_loss: 1.9307 - val_root_mean_squared_error: 1.3895\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9253 - root_mean_squared_error: 1.3876 - val_loss: 1.9451 - val_root_mean_squared_error: 1.3947\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9209 - root_mean_squared_error: 1.3860 - val_loss: 1.9411 - val_root_mean_squared_error: 1.3932\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9125 - root_mean_squared_error: 1.3829 - val_loss: 1.9558 - val_root_mean_squared_error: 1.3985\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9209 - root_mean_squared_error: 1.3860 - val_loss: 1.9565 - val_root_mean_squared_error: 1.3987\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9137 - root_mean_squared_error: 1.3834 - val_loss: 1.9411 - val_root_mean_squared_error: 1.3932\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9110 - root_mean_squared_error: 1.3824 - val_loss: 1.9017 - val_root_mean_squared_error: 1.3790\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9107 - root_mean_squared_error: 1.3823 - val_loss: 1.9103 - val_root_mean_squared_error: 1.3821\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9076 - root_mean_squared_error: 1.3811 - val_loss: 1.9141 - val_root_mean_squared_error: 1.3835\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8990 - root_mean_squared_error: 1.3780 - val_loss: 1.9171 - val_root_mean_squared_error: 1.3846\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9076 - root_mean_squared_error: 1.3812 - val_loss: 1.9240 - val_root_mean_squared_error: 1.3871\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9020 - root_mean_squared_error: 1.3791 - val_loss: 1.9290 - val_root_mean_squared_error: 1.3889\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9040 - root_mean_squared_error: 1.3799 - val_loss: 1.9248 - val_root_mean_squared_error: 1.3874\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9043 - root_mean_squared_error: 1.3800 - val_loss: 1.9402 - val_root_mean_squared_error: 1.3929\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9150 - root_mean_squared_error: 1.3838 - val_loss: 1.9212 - val_root_mean_squared_error: 1.3861\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9075 - root_mean_squared_error: 1.3811 - val_loss: 1.9577 - val_root_mean_squared_error: 1.3992\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8977 - root_mean_squared_error: 1.3776 - val_loss: 1.9334 - val_root_mean_squared_error: 1.3905\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8989 - root_mean_squared_error: 1.3780 - val_loss: 1.9697 - val_root_mean_squared_error: 1.4035\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9033 - root_mean_squared_error: 1.3796 - val_loss: 1.9399 - val_root_mean_squared_error: 1.3928\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8907 - root_mean_squared_error: 1.3750 - val_loss: 1.9341 - val_root_mean_squared_error: 1.3907\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8992 - root_mean_squared_error: 1.3781 - val_loss: 1.9272 - val_root_mean_squared_error: 1.3882\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8994 - root_mean_squared_error: 1.3782 - val_loss: 1.9134 - val_root_mean_squared_error: 1.3833\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8973 - root_mean_squared_error: 1.3774 - val_loss: 1.9404 - val_root_mean_squared_error: 1.3930\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8975 - root_mean_squared_error: 1.3775 - val_loss: 1.8926 - val_root_mean_squared_error: 1.3757\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9014 - root_mean_squared_error: 1.3789 - val_loss: 1.9257 - val_root_mean_squared_error: 1.3877\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8976 - root_mean_squared_error: 1.3775 - val_loss: 1.9212 - val_root_mean_squared_error: 1.3861\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9001 - root_mean_squared_error: 1.3784 - val_loss: 1.9312 - val_root_mean_squared_error: 1.3897\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9084 - root_mean_squared_error: 1.3815 - val_loss: 1.9673 - val_root_mean_squared_error: 1.4026\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8979 - root_mean_squared_error: 1.3777 - val_loss: 1.9354 - val_root_mean_squared_error: 1.3912\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9057 - root_mean_squared_error: 1.3805 - val_loss: 1.9143 - val_root_mean_squared_error: 1.3836\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9052 - root_mean_squared_error: 1.3803 - val_loss: 1.9413 - val_root_mean_squared_error: 1.3933\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9032 - root_mean_squared_error: 1.3796 - val_loss: 1.9267 - val_root_mean_squared_error: 1.3880\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9039 - root_mean_squared_error: 1.3798 - val_loss: 1.9331 - val_root_mean_squared_error: 1.3904\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9016 - root_mean_squared_error: 1.3790 - val_loss: 1.9728 - val_root_mean_squared_error: 1.4046\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9040 - root_mean_squared_error: 1.3799 - val_loss: 1.9350 - val_root_mean_squared_error: 1.3910\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9157 - root_mean_squared_error: 1.3841 - val_loss: 1.9161 - val_root_mean_squared_error: 1.3842\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8998 - root_mean_squared_error: 1.3783 - val_loss: 1.9108 - val_root_mean_squared_error: 1.3823\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9038 - root_mean_squared_error: 1.3798 - val_loss: 1.9142 - val_root_mean_squared_error: 1.3836\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8943 - root_mean_squared_error: 1.3763 - val_loss: 1.9391 - val_root_mean_squared_error: 1.3925\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8948 - root_mean_squared_error: 1.3765 - val_loss: 1.9311 - val_root_mean_squared_error: 1.3896\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8983 - root_mean_squared_error: 1.3778 - val_loss: 1.9211 - val_root_mean_squared_error: 1.3860\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9003 - root_mean_squared_error: 1.3785 - val_loss: 1.9373 - val_root_mean_squared_error: 1.3919\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8940 - root_mean_squared_error: 1.3762 - val_loss: 1.9222 - val_root_mean_squared_error: 1.3864\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9039 - root_mean_squared_error: 1.3798 - val_loss: 1.9358 - val_root_mean_squared_error: 1.3913\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8910 - root_mean_squared_error: 1.3751 - val_loss: 1.9314 - val_root_mean_squared_error: 1.3898\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8929 - root_mean_squared_error: 1.3758 - val_loss: 1.9244 - val_root_mean_squared_error: 1.3872\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8933 - root_mean_squared_error: 1.3760 - val_loss: 1.9142 - val_root_mean_squared_error: 1.3835\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.8945 - root_mean_squared_error: 1.3764 - val_loss: 1.9372 - val_root_mean_squared_error: 1.3918\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9037 - root_mean_squared_error: 1.3798 - val_loss: 1.9386 - val_root_mean_squared_error: 1.3923\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8944 - root_mean_squared_error: 1.3764 - val_loss: 1.9192 - val_root_mean_squared_error: 1.3853\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8903 - root_mean_squared_error: 1.3749 - val_loss: 1.9156 - val_root_mean_squared_error: 1.3841\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9005 - root_mean_squared_error: 1.3786 - val_loss: 1.9451 - val_root_mean_squared_error: 1.3947\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8913 - root_mean_squared_error: 1.3753 - val_loss: 1.9211 - val_root_mean_squared_error: 1.3860\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8953 - root_mean_squared_error: 1.3767 - val_loss: 1.9427 - val_root_mean_squared_error: 1.3938\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9012 - root_mean_squared_error: 1.3788 - val_loss: 1.9113 - val_root_mean_squared_error: 1.3825\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8924 - root_mean_squared_error: 1.3756 - val_loss: 1.9333 - val_root_mean_squared_error: 1.3904\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8972 - root_mean_squared_error: 1.3774 - val_loss: 1.9325 - val_root_mean_squared_error: 1.3901\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8901 - root_mean_squared_error: 1.3748 - val_loss: 1.9295 - val_root_mean_squared_error: 1.3891\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8925 - root_mean_squared_error: 1.3757 - val_loss: 1.9179 - val_root_mean_squared_error: 1.3849\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8938 - root_mean_squared_error: 1.3761 - val_loss: 1.9429 - val_root_mean_squared_error: 1.3939\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8987 - root_mean_squared_error: 1.3779 - val_loss: 1.9314 - val_root_mean_squared_error: 1.3897\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8950 - root_mean_squared_error: 1.3766 - val_loss: 1.9494 - val_root_mean_squared_error: 1.3962\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8909 - root_mean_squared_error: 1.3751 - val_loss: 1.9264 - val_root_mean_squared_error: 1.3880\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8947 - root_mean_squared_error: 1.3765 - val_loss: 1.9225 - val_root_mean_squared_error: 1.3865\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8952 - root_mean_squared_error: 1.3767 - val_loss: 1.9190 - val_root_mean_squared_error: 1.3853\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8923 - root_mean_squared_error: 1.3756 - val_loss: 1.9210 - val_root_mean_squared_error: 1.3860\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8987 - root_mean_squared_error: 1.3779 - val_loss: 1.9141 - val_root_mean_squared_error: 1.3835\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8919 - root_mean_squared_error: 1.3755 - val_loss: 1.9431 - val_root_mean_squared_error: 1.3939\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8914 - root_mean_squared_error: 1.3753 - val_loss: 1.9132 - val_root_mean_squared_error: 1.3832\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8943 - root_mean_squared_error: 1.3763 - val_loss: 1.8991 - val_root_mean_squared_error: 1.3781\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8911 - root_mean_squared_error: 1.3752 - val_loss: 1.9334 - val_root_mean_squared_error: 1.3905\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8901 - root_mean_squared_error: 1.3748 - val_loss: 1.9478 - val_root_mean_squared_error: 1.3957\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8911 - root_mean_squared_error: 1.3752 - val_loss: 1.9388 - val_root_mean_squared_error: 1.3924\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8925 - root_mean_squared_error: 1.3757 - val_loss: 1.9146 - val_root_mean_squared_error: 1.3837\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.8894 - root_mean_squared_error: 1.3745 - val_loss: 1.9373 - val_root_mean_squared_error: 1.3919\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9001 - root_mean_squared_error: 1.3784 - val_loss: 1.9333 - val_root_mean_squared_error: 1.3904\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8923 - root_mean_squared_error: 1.3756 - val_loss: 1.9251 - val_root_mean_squared_error: 1.3875\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8889 - root_mean_squared_error: 1.3744 - val_loss: 1.9254 - val_root_mean_squared_error: 1.3876\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8928 - root_mean_squared_error: 1.3758 - val_loss: 1.9358 - val_root_mean_squared_error: 1.3913\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8871 - root_mean_squared_error: 1.3737 - val_loss: 1.9300 - val_root_mean_squared_error: 1.3892\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8973 - root_mean_squared_error: 1.3774 - val_loss: 1.9279 - val_root_mean_squared_error: 1.3885\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8897 - root_mean_squared_error: 1.3747 - val_loss: 1.9502 - val_root_mean_squared_error: 1.3965\n",
      "01-12 15:30:42 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 15:30:42 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 15:30:43 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112152836_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 15:30:43 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:30:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 15:30:43 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:30:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 7\n",
      "#########################\n",
      "01-12 15:30:43 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7b2669890790>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 15:30:43 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 15:30:43 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:30:43 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 15:30:43 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 15:30:43 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.023921728134155273s\n",
      "01-12 15:30:43 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 15:30:43 I deeptables.m.preprocessor.py 383 - Imputation taken 0.11208081245422363s\n",
      "01-12 15:30:43 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 15:30:43 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.06564044952392578s\n",
      "01-12 15:30:43 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.2704153060913086s\n",
      "01-12 15:30:43 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 15:30:43 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 15:30:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:30:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:30:43 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:30:44 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 15:30:44 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 8s 11ms/step - loss: 36.5441 - root_mean_squared_error: 6.0452 - val_loss: 7.6981 - val_root_mean_squared_error: 2.7745\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 6.9686 - root_mean_squared_error: 2.6398 - val_loss: 5.9329 - val_root_mean_squared_error: 2.4358\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.2743 - root_mean_squared_error: 2.2966 - val_loss: 4.5410 - val_root_mean_squared_error: 2.1310\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.2228 - root_mean_squared_error: 2.0549 - val_loss: 3.8401 - val_root_mean_squared_error: 1.9596\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.5589 - root_mean_squared_error: 1.8865 - val_loss: 3.2740 - val_root_mean_squared_error: 1.8094\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.0733 - root_mean_squared_error: 1.7531 - val_loss: 2.8878 - val_root_mean_squared_error: 1.6994\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.7333 - root_mean_squared_error: 1.6533 - val_loss: 2.6201 - val_root_mean_squared_error: 1.6187\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.4830 - root_mean_squared_error: 1.5757 - val_loss: 2.4062 - val_root_mean_squared_error: 1.5512\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.3096 - root_mean_squared_error: 1.5197 - val_loss: 2.2790 - val_root_mean_squared_error: 1.5096\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.1854 - root_mean_squared_error: 1.4783 - val_loss: 2.1755 - val_root_mean_squared_error: 1.4750\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1021 - root_mean_squared_error: 1.4499 - val_loss: 2.0867 - val_root_mean_squared_error: 1.4445\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0409 - root_mean_squared_error: 1.4286 - val_loss: 2.0625 - val_root_mean_squared_error: 1.4361\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9996 - root_mean_squared_error: 1.4141 - val_loss: 2.0209 - val_root_mean_squared_error: 1.4216\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9730 - root_mean_squared_error: 1.4046 - val_loss: 2.0000 - val_root_mean_squared_error: 1.4142\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9615 - root_mean_squared_error: 1.4005 - val_loss: 1.9805 - val_root_mean_squared_error: 1.4073\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9459 - root_mean_squared_error: 1.3950 - val_loss: 1.9745 - val_root_mean_squared_error: 1.4052\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9372 - root_mean_squared_error: 1.3918 - val_loss: 1.9687 - val_root_mean_squared_error: 1.4031\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9368 - root_mean_squared_error: 1.3917 - val_loss: 1.9766 - val_root_mean_squared_error: 1.4059\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9331 - root_mean_squared_error: 1.3904 - val_loss: 1.9951 - val_root_mean_squared_error: 1.4125\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9271 - root_mean_squared_error: 1.3882 - val_loss: 1.9534 - val_root_mean_squared_error: 1.3976\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9202 - root_mean_squared_error: 1.3857 - val_loss: 1.9649 - val_root_mean_squared_error: 1.4018\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9302 - root_mean_squared_error: 1.3893 - val_loss: 1.9678 - val_root_mean_squared_error: 1.4028\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9127 - root_mean_squared_error: 1.3830 - val_loss: 1.9479 - val_root_mean_squared_error: 1.3957\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9173 - root_mean_squared_error: 1.3847 - val_loss: 1.9393 - val_root_mean_squared_error: 1.3926\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9181 - root_mean_squared_error: 1.3849 - val_loss: 2.0082 - val_root_mean_squared_error: 1.4171\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9123 - root_mean_squared_error: 1.3829 - val_loss: 1.9449 - val_root_mean_squared_error: 1.3946\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9358 - root_mean_squared_error: 1.3913 - val_loss: 2.0427 - val_root_mean_squared_error: 1.4292\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9198 - root_mean_squared_error: 1.3856 - val_loss: 1.9512 - val_root_mean_squared_error: 1.3968\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9136 - root_mean_squared_error: 1.3833 - val_loss: 1.9482 - val_root_mean_squared_error: 1.3958\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9136 - root_mean_squared_error: 1.3833 - val_loss: 1.9221 - val_root_mean_squared_error: 1.3864\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9075 - root_mean_squared_error: 1.3811 - val_loss: 1.9260 - val_root_mean_squared_error: 1.3878\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9002 - root_mean_squared_error: 1.3785 - val_loss: 2.0134 - val_root_mean_squared_error: 1.4189\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9147 - root_mean_squared_error: 1.3837 - val_loss: 1.9504 - val_root_mean_squared_error: 1.3966\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9194 - root_mean_squared_error: 1.3854 - val_loss: 1.9485 - val_root_mean_squared_error: 1.3959\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9073 - root_mean_squared_error: 1.3811 - val_loss: 1.9556 - val_root_mean_squared_error: 1.3984\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9297 - root_mean_squared_error: 1.3891 - val_loss: 1.9518 - val_root_mean_squared_error: 1.3971\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9175 - root_mean_squared_error: 1.3848 - val_loss: 1.9892 - val_root_mean_squared_error: 1.4104\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9043 - root_mean_squared_error: 1.3800 - val_loss: 1.9723 - val_root_mean_squared_error: 1.4044\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9044 - root_mean_squared_error: 1.3800 - val_loss: 1.9649 - val_root_mean_squared_error: 1.4017\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9083 - root_mean_squared_error: 1.3814 - val_loss: 1.9355 - val_root_mean_squared_error: 1.3912\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9015 - root_mean_squared_error: 1.3789 - val_loss: 1.9385 - val_root_mean_squared_error: 1.3923\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8896 - root_mean_squared_error: 1.3746 - val_loss: 1.9435 - val_root_mean_squared_error: 1.3941\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9061 - root_mean_squared_error: 1.3806 - val_loss: 2.0272 - val_root_mean_squared_error: 1.4238\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9038 - root_mean_squared_error: 1.3798 - val_loss: 1.9321 - val_root_mean_squared_error: 1.3900\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9057 - root_mean_squared_error: 1.3805 - val_loss: 2.0567 - val_root_mean_squared_error: 1.4341\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8988 - root_mean_squared_error: 1.3780 - val_loss: 1.9343 - val_root_mean_squared_error: 1.3908\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8889 - root_mean_squared_error: 1.3744 - val_loss: 1.9849 - val_root_mean_squared_error: 1.4089\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8939 - root_mean_squared_error: 1.3762 - val_loss: 1.9382 - val_root_mean_squared_error: 1.3922\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9041 - root_mean_squared_error: 1.3799 - val_loss: 1.9413 - val_root_mean_squared_error: 1.3933\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9068 - root_mean_squared_error: 1.3809 - val_loss: 1.9350 - val_root_mean_squared_error: 1.3910\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9110 - root_mean_squared_error: 1.3824 - val_loss: 1.9384 - val_root_mean_squared_error: 1.3923\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8883 - root_mean_squared_error: 1.3742 - val_loss: 2.0568 - val_root_mean_squared_error: 1.4342\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9318 - root_mean_squared_error: 1.3899 - val_loss: 1.9356 - val_root_mean_squared_error: 1.3913\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8991 - root_mean_squared_error: 1.3781 - val_loss: 2.0406 - val_root_mean_squared_error: 1.4285\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9061 - root_mean_squared_error: 1.3806 - val_loss: 1.9016 - val_root_mean_squared_error: 1.3790\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8883 - root_mean_squared_error: 1.3742 - val_loss: 1.9731 - val_root_mean_squared_error: 1.4047\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8914 - root_mean_squared_error: 1.3753 - val_loss: 1.9726 - val_root_mean_squared_error: 1.4045\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9040 - root_mean_squared_error: 1.3799 - val_loss: 1.9408 - val_root_mean_squared_error: 1.3931\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8952 - root_mean_squared_error: 1.3767 - val_loss: 1.9431 - val_root_mean_squared_error: 1.3939\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8925 - root_mean_squared_error: 1.3757 - val_loss: 1.9475 - val_root_mean_squared_error: 1.3955\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8984 - root_mean_squared_error: 1.3778 - val_loss: 1.9894 - val_root_mean_squared_error: 1.4104\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8903 - root_mean_squared_error: 1.3749 - val_loss: 1.9308 - val_root_mean_squared_error: 1.3895\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8868 - root_mean_squared_error: 1.3736 - val_loss: 1.9376 - val_root_mean_squared_error: 1.3920\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8954 - root_mean_squared_error: 1.3768 - val_loss: 1.9404 - val_root_mean_squared_error: 1.3930\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8935 - root_mean_squared_error: 1.3760 - val_loss: 1.9342 - val_root_mean_squared_error: 1.3908\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8884 - root_mean_squared_error: 1.3742 - val_loss: 1.9517 - val_root_mean_squared_error: 1.3970\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9018 - root_mean_squared_error: 1.3791 - val_loss: 1.9709 - val_root_mean_squared_error: 1.4039\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8898 - root_mean_squared_error: 1.3747 - val_loss: 1.9551 - val_root_mean_squared_error: 1.3982\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8998 - root_mean_squared_error: 1.3783 - val_loss: 1.9185 - val_root_mean_squared_error: 1.3851\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8857 - root_mean_squared_error: 1.3732 - val_loss: 1.9537 - val_root_mean_squared_error: 1.3978\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8973 - root_mean_squared_error: 1.3774 - val_loss: 1.9387 - val_root_mean_squared_error: 1.3924\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8868 - root_mean_squared_error: 1.3736 - val_loss: 1.9452 - val_root_mean_squared_error: 1.3947\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8811 - root_mean_squared_error: 1.3715 - val_loss: 1.9279 - val_root_mean_squared_error: 1.3885\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8929 - root_mean_squared_error: 1.3758 - val_loss: 1.9466 - val_root_mean_squared_error: 1.3952\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8983 - root_mean_squared_error: 1.3778 - val_loss: 1.9261 - val_root_mean_squared_error: 1.3878\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8878 - root_mean_squared_error: 1.3740 - val_loss: 1.9597 - val_root_mean_squared_error: 1.3999\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8823 - root_mean_squared_error: 1.3720 - val_loss: 1.9221 - val_root_mean_squared_error: 1.3864\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8919 - root_mean_squared_error: 1.3755 - val_loss: 1.9477 - val_root_mean_squared_error: 1.3956\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8892 - root_mean_squared_error: 1.3745 - val_loss: 2.0268 - val_root_mean_squared_error: 1.4237\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8939 - root_mean_squared_error: 1.3762 - val_loss: 1.9755 - val_root_mean_squared_error: 1.4055\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8859 - root_mean_squared_error: 1.3733 - val_loss: 1.9289 - val_root_mean_squared_error: 1.3889\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8826 - root_mean_squared_error: 1.3721 - val_loss: 1.9523 - val_root_mean_squared_error: 1.3973\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8998 - root_mean_squared_error: 1.3783 - val_loss: 1.9456 - val_root_mean_squared_error: 1.3949\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8791 - root_mean_squared_error: 1.3708 - val_loss: 1.9418 - val_root_mean_squared_error: 1.3935\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8920 - root_mean_squared_error: 1.3755 - val_loss: 1.9459 - val_root_mean_squared_error: 1.3950\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8843 - root_mean_squared_error: 1.3727 - val_loss: 1.9511 - val_root_mean_squared_error: 1.3968\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8861 - root_mean_squared_error: 1.3734 - val_loss: 1.9424 - val_root_mean_squared_error: 1.3937\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8885 - root_mean_squared_error: 1.3742 - val_loss: 1.9288 - val_root_mean_squared_error: 1.3888\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8917 - root_mean_squared_error: 1.3754 - val_loss: 1.9216 - val_root_mean_squared_error: 1.3862\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8807 - root_mean_squared_error: 1.3714 - val_loss: 1.9411 - val_root_mean_squared_error: 1.3932\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8899 - root_mean_squared_error: 1.3747 - val_loss: 1.9282 - val_root_mean_squared_error: 1.3886\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8843 - root_mean_squared_error: 1.3727 - val_loss: 1.9382 - val_root_mean_squared_error: 1.3922\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8875 - root_mean_squared_error: 1.3738 - val_loss: 1.9408 - val_root_mean_squared_error: 1.3931\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8887 - root_mean_squared_error: 1.3743 - val_loss: 1.9813 - val_root_mean_squared_error: 1.4076\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8870 - root_mean_squared_error: 1.3737 - val_loss: 1.9039 - val_root_mean_squared_error: 1.3798\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8864 - root_mean_squared_error: 1.3735 - val_loss: 1.9531 - val_root_mean_squared_error: 1.3975\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8898 - root_mean_squared_error: 1.3747 - val_loss: 1.9164 - val_root_mean_squared_error: 1.3843\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.8824 - root_mean_squared_error: 1.3720 - val_loss: 1.9801 - val_root_mean_squared_error: 1.4072\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8970 - root_mean_squared_error: 1.3773 - val_loss: 2.0014 - val_root_mean_squared_error: 1.4147\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.8834 - root_mean_squared_error: 1.3724 - val_loss: 1.9341 - val_root_mean_squared_error: 1.3907\n",
      "01-12 15:32:53 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 15:32:53 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 15:32:53 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112153043_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 15:32:53 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:32:53 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 15:32:53 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:32:53 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 8\n",
      "#########################\n",
      "01-12 15:32:53 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7b2669ee98a0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 15:32:53 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 15:32:53 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:32:53 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 15:32:53 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 15:32:53 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.02523517608642578s\n",
      "01-12 15:32:53 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 15:32:53 I deeptables.m.preprocessor.py 383 - Imputation taken 0.10299134254455566s\n",
      "01-12 15:32:53 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 15:32:53 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.06461954116821289s\n",
      "01-12 15:32:53 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.2578606605529785s\n",
      "01-12 15:32:53 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 15:32:53 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 15:32:53 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:32:53 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:32:53 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:32:54 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 15:32:54 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 9s 11ms/step - loss: 404650.7188 - root_mean_squared_error: 636.1216 - val_loss: 155623.1094 - val_root_mean_squared_error: 394.4909\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 69300.2500 - root_mean_squared_error: 263.2494 - val_loss: 20063.4980 - val_root_mean_squared_error: 141.6457\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 7422.1294 - root_mean_squared_error: 86.1518 - val_loss: 1544.1759 - val_root_mean_squared_error: 39.2960\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 626.6535 - root_mean_squared_error: 25.0330 - val_loss: 298.9440 - val_root_mean_squared_error: 17.2900\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 261.0495 - root_mean_squared_error: 16.1570 - val_loss: 253.5824 - val_root_mean_squared_error: 15.9243\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 245.6630 - root_mean_squared_error: 15.6736 - val_loss: 245.4764 - val_root_mean_squared_error: 15.6677\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 238.1712 - root_mean_squared_error: 15.4328 - val_loss: 236.1687 - val_root_mean_squared_error: 15.3678\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 230.8130 - root_mean_squared_error: 15.1925 - val_loss: 227.4110 - val_root_mean_squared_error: 15.0802\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 220.9798 - root_mean_squared_error: 14.8654 - val_loss: 217.9136 - val_root_mean_squared_error: 14.7619\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 211.1342 - root_mean_squared_error: 14.5305 - val_loss: 208.4480 - val_root_mean_squared_error: 14.4377\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 200.8767 - root_mean_squared_error: 14.1731 - val_loss: 198.6733 - val_root_mean_squared_error: 14.0952\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 191.8966 - root_mean_squared_error: 13.8527 - val_loss: 188.0073 - val_root_mean_squared_error: 13.7116\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 181.7415 - root_mean_squared_error: 13.4812 - val_loss: 177.2799 - val_root_mean_squared_error: 13.3147\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 172.4373 - root_mean_squared_error: 13.1315 - val_loss: 166.7363 - val_root_mean_squared_error: 12.9126\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 161.9562 - root_mean_squared_error: 12.7262 - val_loss: 155.5569 - val_root_mean_squared_error: 12.4722\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 152.5616 - root_mean_squared_error: 12.3516 - val_loss: 147.4392 - val_root_mean_squared_error: 12.1425\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 142.3930 - root_mean_squared_error: 11.9329 - val_loss: 137.2012 - val_root_mean_squared_error: 11.7133\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 132.7487 - root_mean_squared_error: 11.5217 - val_loss: 127.3622 - val_root_mean_squared_error: 11.2855\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 124.7864 - root_mean_squared_error: 11.1708 - val_loss: 118.5459 - val_root_mean_squared_error: 10.8879\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 116.4162 - root_mean_squared_error: 10.7896 - val_loss: 110.1041 - val_root_mean_squared_error: 10.4931\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 108.7245 - root_mean_squared_error: 10.4271 - val_loss: 101.4862 - val_root_mean_squared_error: 10.0740\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 101.1346 - root_mean_squared_error: 10.0566 - val_loss: 94.7137 - val_root_mean_squared_error: 9.7321\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 94.3246 - root_mean_squared_error: 9.7121 - val_loss: 87.9502 - val_root_mean_squared_error: 9.3782\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 88.2064 - root_mean_squared_error: 9.3918 - val_loss: 82.1644 - val_root_mean_squared_error: 9.0645\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 82.8366 - root_mean_squared_error: 9.1015 - val_loss: 76.8715 - val_root_mean_squared_error: 8.7676\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 77.0637 - root_mean_squared_error: 8.7786 - val_loss: 70.3910 - val_root_mean_squared_error: 8.3899\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 71.8694 - root_mean_squared_error: 8.4776 - val_loss: 65.8518 - val_root_mean_squared_error: 8.1149\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 67.8880 - root_mean_squared_error: 8.2394 - val_loss: 61.1180 - val_root_mean_squared_error: 7.8178\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 62.5090 - root_mean_squared_error: 7.9063 - val_loss: 56.4309 - val_root_mean_squared_error: 7.5121\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 58.5042 - root_mean_squared_error: 7.6488 - val_loss: 51.5691 - val_root_mean_squared_error: 7.1812\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 54.5018 - root_mean_squared_error: 7.3825 - val_loss: 47.5346 - val_root_mean_squared_error: 6.8945\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 50.3303 - root_mean_squared_error: 7.0944 - val_loss: 43.3874 - val_root_mean_squared_error: 6.5869\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 46.2134 - root_mean_squared_error: 6.7980 - val_loss: 39.8171 - val_root_mean_squared_error: 6.3101\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 42.8517 - root_mean_squared_error: 6.5461 - val_loss: 36.0156 - val_root_mean_squared_error: 6.0013\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 39.1944 - root_mean_squared_error: 6.2605 - val_loss: 32.2481 - val_root_mean_squared_error: 5.6787\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 35.5601 - root_mean_squared_error: 5.9632 - val_loss: 29.1393 - val_root_mean_squared_error: 5.3981\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 32.8502 - root_mean_squared_error: 5.7315 - val_loss: 26.2964 - val_root_mean_squared_error: 5.1280\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 29.4554 - root_mean_squared_error: 5.4273 - val_loss: 22.9731 - val_root_mean_squared_error: 4.7930\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 26.9472 - root_mean_squared_error: 5.1911 - val_loss: 20.5262 - val_root_mean_squared_error: 4.5306\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 24.1758 - root_mean_squared_error: 4.9169 - val_loss: 18.1339 - val_root_mean_squared_error: 4.2584\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 22.1377 - root_mean_squared_error: 4.7051 - val_loss: 15.7912 - val_root_mean_squared_error: 3.9738\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 19.9530 - root_mean_squared_error: 4.4669 - val_loss: 13.8947 - val_root_mean_squared_error: 3.7276\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 17.8436 - root_mean_squared_error: 4.2242 - val_loss: 12.1373 - val_root_mean_squared_error: 3.4839\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 16.3012 - root_mean_squared_error: 4.0375 - val_loss: 10.6997 - val_root_mean_squared_error: 3.2710\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 14.6828 - root_mean_squared_error: 3.8318 - val_loss: 9.1896 - val_root_mean_squared_error: 3.0314\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 13.4532 - root_mean_squared_error: 3.6679 - val_loss: 8.0339 - val_root_mean_squared_error: 2.8344\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 11.9631 - root_mean_squared_error: 3.4588 - val_loss: 6.9941 - val_root_mean_squared_error: 2.6446\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 11.0702 - root_mean_squared_error: 3.3272 - val_loss: 6.1538 - val_root_mean_squared_error: 2.4807\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 10.0378 - root_mean_squared_error: 3.1682 - val_loss: 5.3621 - val_root_mean_squared_error: 2.3156\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 9.1310 - root_mean_squared_error: 3.0217 - val_loss: 4.8165 - val_root_mean_squared_error: 2.1946\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 8.3505 - root_mean_squared_error: 2.8897 - val_loss: 4.3965 - val_root_mean_squared_error: 2.0968\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 7.8524 - root_mean_squared_error: 2.8022 - val_loss: 3.8971 - val_root_mean_squared_error: 1.9741\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 7.1441 - root_mean_squared_error: 2.6729 - val_loss: 3.5467 - val_root_mean_squared_error: 1.8833\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 6.7808 - root_mean_squared_error: 2.6040 - val_loss: 3.2921 - val_root_mean_squared_error: 1.8144\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 6.2057 - root_mean_squared_error: 2.4911 - val_loss: 3.0522 - val_root_mean_squared_error: 1.7470\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 5.9275 - root_mean_squared_error: 2.4347 - val_loss: 2.8959 - val_root_mean_squared_error: 1.7017\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 5.4427 - root_mean_squared_error: 2.3330 - val_loss: 2.7257 - val_root_mean_squared_error: 1.6510\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 5.1951 - root_mean_squared_error: 2.2793 - val_loss: 2.6098 - val_root_mean_squared_error: 1.6155\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.8488 - root_mean_squared_error: 2.2020 - val_loss: 2.5144 - val_root_mean_squared_error: 1.5857\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.5884 - root_mean_squared_error: 2.1421 - val_loss: 2.4642 - val_root_mean_squared_error: 1.5698\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.3352 - root_mean_squared_error: 2.0821 - val_loss: 2.4079 - val_root_mean_squared_error: 1.5518\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.1151 - root_mean_squared_error: 2.0286 - val_loss: 2.3550 - val_root_mean_squared_error: 1.5346\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.9468 - root_mean_squared_error: 1.9867 - val_loss: 2.4891 - val_root_mean_squared_error: 1.5777\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.7276 - root_mean_squared_error: 1.9307 - val_loss: 2.2537 - val_root_mean_squared_error: 1.5012\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.5306 - root_mean_squared_error: 1.8790 - val_loss: 2.2133 - val_root_mean_squared_error: 1.4877\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.3698 - root_mean_squared_error: 1.8357 - val_loss: 2.1983 - val_root_mean_squared_error: 1.4827\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.1483 - root_mean_squared_error: 1.7743 - val_loss: 2.1692 - val_root_mean_squared_error: 1.4728\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.0574 - root_mean_squared_error: 1.7485 - val_loss: 2.2514 - val_root_mean_squared_error: 1.5005\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.0009 - root_mean_squared_error: 1.7323 - val_loss: 2.1842 - val_root_mean_squared_error: 1.4779\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.8582 - root_mean_squared_error: 1.6906 - val_loss: 2.1435 - val_root_mean_squared_error: 1.4641\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.7445 - root_mean_squared_error: 1.6567 - val_loss: 2.1061 - val_root_mean_squared_error: 1.4512\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.6712 - root_mean_squared_error: 1.6344 - val_loss: 2.0851 - val_root_mean_squared_error: 1.4440\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.6081 - root_mean_squared_error: 1.6150 - val_loss: 2.0681 - val_root_mean_squared_error: 1.4381\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.5561 - root_mean_squared_error: 1.5988 - val_loss: 2.0467 - val_root_mean_squared_error: 1.4306\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.4469 - root_mean_squared_error: 1.5642 - val_loss: 2.0567 - val_root_mean_squared_error: 1.4341\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.4230 - root_mean_squared_error: 1.5566 - val_loss: 2.1914 - val_root_mean_squared_error: 1.4804\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.3601 - root_mean_squared_error: 1.5363 - val_loss: 2.0453 - val_root_mean_squared_error: 1.4301\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.3137 - root_mean_squared_error: 1.5211 - val_loss: 2.0050 - val_root_mean_squared_error: 1.4160\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.2765 - root_mean_squared_error: 1.5088 - val_loss: 1.9910 - val_root_mean_squared_error: 1.4110\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.2270 - root_mean_squared_error: 1.4923 - val_loss: 2.0159 - val_root_mean_squared_error: 1.4198\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.2191 - root_mean_squared_error: 1.4897 - val_loss: 2.3150 - val_root_mean_squared_error: 1.5215\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1889 - root_mean_squared_error: 1.4795 - val_loss: 1.9634 - val_root_mean_squared_error: 1.4012\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1610 - root_mean_squared_error: 1.4700 - val_loss: 2.0355 - val_root_mean_squared_error: 1.4267\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1587 - root_mean_squared_error: 1.4692 - val_loss: 1.9825 - val_root_mean_squared_error: 1.4080\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1257 - root_mean_squared_error: 1.4580 - val_loss: 1.9608 - val_root_mean_squared_error: 1.4003\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.1160 - root_mean_squared_error: 1.4546 - val_loss: 2.3044 - val_root_mean_squared_error: 1.5180\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1004 - root_mean_squared_error: 1.4493 - val_loss: 1.9127 - val_root_mean_squared_error: 1.3830\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0863 - root_mean_squared_error: 1.4444 - val_loss: 1.9308 - val_root_mean_squared_error: 1.3895\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0674 - root_mean_squared_error: 1.4378 - val_loss: 1.9650 - val_root_mean_squared_error: 1.4018\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.1000 - root_mean_squared_error: 1.4491 - val_loss: 1.9136 - val_root_mean_squared_error: 1.3833\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0455 - root_mean_squared_error: 1.4302 - val_loss: 1.9110 - val_root_mean_squared_error: 1.3824\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0880 - root_mean_squared_error: 1.4450 - val_loss: 2.2256 - val_root_mean_squared_error: 1.4918\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.1116 - root_mean_squared_error: 1.4531 - val_loss: 2.0505 - val_root_mean_squared_error: 1.4319\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0533 - root_mean_squared_error: 1.4329 - val_loss: 2.0010 - val_root_mean_squared_error: 1.4146\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0132 - root_mean_squared_error: 1.4189 - val_loss: 1.8876 - val_root_mean_squared_error: 1.3739\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0243 - root_mean_squared_error: 1.4228 - val_loss: 1.9081 - val_root_mean_squared_error: 1.3814\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0113 - root_mean_squared_error: 1.4182 - val_loss: 2.1696 - val_root_mean_squared_error: 1.4730\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0357 - root_mean_squared_error: 1.4268 - val_loss: 1.9192 - val_root_mean_squared_error: 1.3853\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9920 - root_mean_squared_error: 1.4114 - val_loss: 1.8847 - val_root_mean_squared_error: 1.3729\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0594 - root_mean_squared_error: 1.4351 - val_loss: 2.0017 - val_root_mean_squared_error: 1.4148\n",
      "01-12 15:35:02 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 15:35:02 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 15:35:02 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112153253_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 15:35:02 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:35:02 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 15:35:02 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:35:02 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 9\n",
      "#########################\n",
      "01-12 15:35:02 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7b26697a6ce0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 15:35:02 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 15:35:02 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:35:02 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 15:35:02 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 15:35:02 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.02348494529724121s\n",
      "01-12 15:35:02 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 15:35:02 I deeptables.m.preprocessor.py 383 - Imputation taken 0.1076040267944336s\n",
      "01-12 15:35:02 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 15:35:02 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.06735563278198242s\n",
      "01-12 15:35:03 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.26430702209472656s\n",
      "01-12 15:35:03 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 15:35:03 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 15:35:03 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:35:03 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:35:03 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:35:03 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 15:35:03 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 8s 11ms/step - loss: 3367.1016 - root_mean_squared_error: 58.0267 - val_loss: 111.0310 - val_root_mean_squared_error: 10.5371\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 28.8668 - root_mean_squared_error: 5.3728 - val_loss: 15.4508 - val_root_mean_squared_error: 3.9307\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 15.0561 - root_mean_squared_error: 3.8802 - val_loss: 15.0028 - val_root_mean_squared_error: 3.8733\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 14.4997 - root_mean_squared_error: 3.8078 - val_loss: 14.2997 - val_root_mean_squared_error: 3.7815\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 13.8510 - root_mean_squared_error: 3.7217 - val_loss: 13.6616 - val_root_mean_squared_error: 3.6962\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 13.1882 - root_mean_squared_error: 3.6316 - val_loss: 13.0275 - val_root_mean_squared_error: 3.6094\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 12.4724 - root_mean_squared_error: 3.5316 - val_loss: 12.1291 - val_root_mean_squared_error: 3.4827\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 11.7283 - root_mean_squared_error: 3.4247 - val_loss: 11.5487 - val_root_mean_squared_error: 3.3983\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 11.0064 - root_mean_squared_error: 3.3176 - val_loss: 10.7160 - val_root_mean_squared_error: 3.2735\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 10.3082 - root_mean_squared_error: 3.2106 - val_loss: 10.1284 - val_root_mean_squared_error: 3.1825\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 9.6294 - root_mean_squared_error: 3.1031 - val_loss: 9.4561 - val_root_mean_squared_error: 3.0751\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 8.9894 - root_mean_squared_error: 2.9982 - val_loss: 8.8906 - val_root_mean_squared_error: 2.9817\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 8.4394 - root_mean_squared_error: 2.9051 - val_loss: 8.2294 - val_root_mean_squared_error: 2.8687\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 7.9280 - root_mean_squared_error: 2.8157 - val_loss: 7.8519 - val_root_mean_squared_error: 2.8021\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 7.4564 - root_mean_squared_error: 2.7306 - val_loss: 7.3430 - val_root_mean_squared_error: 2.7098\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 7.0303 - root_mean_squared_error: 2.6515 - val_loss: 6.9572 - val_root_mean_squared_error: 2.6376\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 6.6421 - root_mean_squared_error: 2.5772 - val_loss: 6.6140 - val_root_mean_squared_error: 2.5718\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 6.2552 - root_mean_squared_error: 2.5010 - val_loss: 6.2179 - val_root_mean_squared_error: 2.4936\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.9072 - root_mean_squared_error: 2.4305 - val_loss: 5.8468 - val_root_mean_squared_error: 2.4180\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.5301 - root_mean_squared_error: 2.3516 - val_loss: 5.5364 - val_root_mean_squared_error: 2.3530\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 5.2261 - root_mean_squared_error: 2.2861 - val_loss: 5.1420 - val_root_mean_squared_error: 2.2676\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.8920 - root_mean_squared_error: 2.2118 - val_loss: 4.8237 - val_root_mean_squared_error: 2.1963\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.5745 - root_mean_squared_error: 2.1388 - val_loss: 4.5227 - val_root_mean_squared_error: 2.1267\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.2961 - root_mean_squared_error: 2.0727 - val_loss: 4.1805 - val_root_mean_squared_error: 2.0446\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.0325 - root_mean_squared_error: 2.0081 - val_loss: 3.9213 - val_root_mean_squared_error: 1.9802\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.7753 - root_mean_squared_error: 1.9430 - val_loss: 3.6812 - val_root_mean_squared_error: 1.9186\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.5238 - root_mean_squared_error: 1.8772 - val_loss: 3.4406 - val_root_mean_squared_error: 1.8549\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.2959 - root_mean_squared_error: 1.8155 - val_loss: 3.1916 - val_root_mean_squared_error: 1.7865\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.0972 - root_mean_squared_error: 1.7599 - val_loss: 3.0638 - val_root_mean_squared_error: 1.7504\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.9134 - root_mean_squared_error: 1.7069 - val_loss: 2.8880 - val_root_mean_squared_error: 1.6994\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.7652 - root_mean_squared_error: 1.6629 - val_loss: 2.7054 - val_root_mean_squared_error: 1.6448\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.6357 - root_mean_squared_error: 1.6235 - val_loss: 2.5595 - val_root_mean_squared_error: 1.5998\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.4942 - root_mean_squared_error: 1.5793 - val_loss: 2.4316 - val_root_mean_squared_error: 1.5593\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.3960 - root_mean_squared_error: 1.5479 - val_loss: 2.3121 - val_root_mean_squared_error: 1.5206\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.3108 - root_mean_squared_error: 1.5201 - val_loss: 2.2282 - val_root_mean_squared_error: 1.4927\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 2.2220 - root_mean_squared_error: 1.4906 - val_loss: 2.1769 - val_root_mean_squared_error: 1.4754\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.1651 - root_mean_squared_error: 1.4714 - val_loss: 2.1184 - val_root_mean_squared_error: 1.4555\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.1176 - root_mean_squared_error: 1.4552 - val_loss: 2.0635 - val_root_mean_squared_error: 1.4365\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0940 - root_mean_squared_error: 1.4471 - val_loss: 2.0056 - val_root_mean_squared_error: 1.4162\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0474 - root_mean_squared_error: 1.4309 - val_loss: 1.9555 - val_root_mean_squared_error: 1.3984\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0203 - root_mean_squared_error: 1.4214 - val_loss: 1.9448 - val_root_mean_squared_error: 1.3946\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0066 - root_mean_squared_error: 1.4166 - val_loss: 1.9355 - val_root_mean_squared_error: 1.3912\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9912 - root_mean_squared_error: 1.4111 - val_loss: 1.9177 - val_root_mean_squared_error: 1.3848\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9844 - root_mean_squared_error: 1.4087 - val_loss: 1.8938 - val_root_mean_squared_error: 1.3762\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9726 - root_mean_squared_error: 1.4045 - val_loss: 1.9021 - val_root_mean_squared_error: 1.3792\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9693 - root_mean_squared_error: 1.4033 - val_loss: 1.8992 - val_root_mean_squared_error: 1.3781\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9619 - root_mean_squared_error: 1.4007 - val_loss: 1.9035 - val_root_mean_squared_error: 1.3797\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9575 - root_mean_squared_error: 1.3991 - val_loss: 1.8834 - val_root_mean_squared_error: 1.3724\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9612 - root_mean_squared_error: 1.4004 - val_loss: 1.8946 - val_root_mean_squared_error: 1.3764\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9485 - root_mean_squared_error: 1.3959 - val_loss: 1.8833 - val_root_mean_squared_error: 1.3723\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9553 - root_mean_squared_error: 1.3983 - val_loss: 1.8874 - val_root_mean_squared_error: 1.3738\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9539 - root_mean_squared_error: 1.3978 - val_loss: 1.8679 - val_root_mean_squared_error: 1.3667\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9558 - root_mean_squared_error: 1.3985 - val_loss: 1.8616 - val_root_mean_squared_error: 1.3644\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9445 - root_mean_squared_error: 1.3945 - val_loss: 1.8547 - val_root_mean_squared_error: 1.3619\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9489 - root_mean_squared_error: 1.3960 - val_loss: 1.8684 - val_root_mean_squared_error: 1.3669\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9406 - root_mean_squared_error: 1.3931 - val_loss: 1.8636 - val_root_mean_squared_error: 1.3651\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9518 - root_mean_squared_error: 1.3971 - val_loss: 1.8532 - val_root_mean_squared_error: 1.3613\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9446 - root_mean_squared_error: 1.3945 - val_loss: 1.9699 - val_root_mean_squared_error: 1.4035\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9331 - root_mean_squared_error: 1.3904 - val_loss: 1.8461 - val_root_mean_squared_error: 1.3587\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9347 - root_mean_squared_error: 1.3909 - val_loss: 1.8583 - val_root_mean_squared_error: 1.3632\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9516 - root_mean_squared_error: 1.3970 - val_loss: 1.8768 - val_root_mean_squared_error: 1.3700\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9332 - root_mean_squared_error: 1.3904 - val_loss: 1.9473 - val_root_mean_squared_error: 1.3954\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9444 - root_mean_squared_error: 1.3944 - val_loss: 1.8687 - val_root_mean_squared_error: 1.3670\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9313 - root_mean_squared_error: 1.3897 - val_loss: 1.8652 - val_root_mean_squared_error: 1.3657\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9368 - root_mean_squared_error: 1.3917 - val_loss: 1.9158 - val_root_mean_squared_error: 1.3841\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9444 - root_mean_squared_error: 1.3944 - val_loss: 1.8530 - val_root_mean_squared_error: 1.3612\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9240 - root_mean_squared_error: 1.3871 - val_loss: 1.8450 - val_root_mean_squared_error: 1.3583\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9559 - root_mean_squared_error: 1.3985 - val_loss: 1.9757 - val_root_mean_squared_error: 1.4056\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9417 - root_mean_squared_error: 1.3935 - val_loss: 1.8657 - val_root_mean_squared_error: 1.3659\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9394 - root_mean_squared_error: 1.3926 - val_loss: 1.8821 - val_root_mean_squared_error: 1.3719\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9262 - root_mean_squared_error: 1.3879 - val_loss: 1.8551 - val_root_mean_squared_error: 1.3620\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9194 - root_mean_squared_error: 1.3854 - val_loss: 1.8378 - val_root_mean_squared_error: 1.3556\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9413 - root_mean_squared_error: 1.3933 - val_loss: 1.8667 - val_root_mean_squared_error: 1.3663\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9226 - root_mean_squared_error: 1.3866 - val_loss: 1.8609 - val_root_mean_squared_error: 1.3642\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9287 - root_mean_squared_error: 1.3888 - val_loss: 1.8674 - val_root_mean_squared_error: 1.3665\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9430 - root_mean_squared_error: 1.3939 - val_loss: 1.8814 - val_root_mean_squared_error: 1.3716\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9350 - root_mean_squared_error: 1.3911 - val_loss: 1.8960 - val_root_mean_squared_error: 1.3770\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9119 - root_mean_squared_error: 1.3827 - val_loss: 1.8773 - val_root_mean_squared_error: 1.3702\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9212 - root_mean_squared_error: 1.3861 - val_loss: 1.8431 - val_root_mean_squared_error: 1.3576\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9164 - root_mean_squared_error: 1.3843 - val_loss: 1.8659 - val_root_mean_squared_error: 1.3660\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9240 - root_mean_squared_error: 1.3871 - val_loss: 1.8738 - val_root_mean_squared_error: 1.3689\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9224 - root_mean_squared_error: 1.3865 - val_loss: 1.8665 - val_root_mean_squared_error: 1.3662\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9178 - root_mean_squared_error: 1.3848 - val_loss: 1.9165 - val_root_mean_squared_error: 1.3844\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9308 - root_mean_squared_error: 1.3895 - val_loss: 2.1088 - val_root_mean_squared_error: 1.4522\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9507 - root_mean_squared_error: 1.3967 - val_loss: 1.8440 - val_root_mean_squared_error: 1.3580\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9243 - root_mean_squared_error: 1.3872 - val_loss: 1.9196 - val_root_mean_squared_error: 1.3855\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9252 - root_mean_squared_error: 1.3875 - val_loss: 1.9128 - val_root_mean_squared_error: 1.3830\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9278 - root_mean_squared_error: 1.3885 - val_loss: 1.8783 - val_root_mean_squared_error: 1.3705\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9114 - root_mean_squared_error: 1.3825 - val_loss: 1.8875 - val_root_mean_squared_error: 1.3738\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9266 - root_mean_squared_error: 1.3880 - val_loss: 1.8444 - val_root_mean_squared_error: 1.3581\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9319 - root_mean_squared_error: 1.3899 - val_loss: 1.8431 - val_root_mean_squared_error: 1.3576\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9256 - root_mean_squared_error: 1.3877 - val_loss: 1.8962 - val_root_mean_squared_error: 1.3770\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9163 - root_mean_squared_error: 1.3843 - val_loss: 1.8346 - val_root_mean_squared_error: 1.3545\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9252 - root_mean_squared_error: 1.3875 - val_loss: 1.8661 - val_root_mean_squared_error: 1.3661\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9339 - root_mean_squared_error: 1.3906 - val_loss: 1.8471 - val_root_mean_squared_error: 1.3591\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9134 - root_mean_squared_error: 1.3833 - val_loss: 1.8502 - val_root_mean_squared_error: 1.3602\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9047 - root_mean_squared_error: 1.3801 - val_loss: 1.8387 - val_root_mean_squared_error: 1.3560\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9159 - root_mean_squared_error: 1.3842 - val_loss: 1.8610 - val_root_mean_squared_error: 1.3642\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9165 - root_mean_squared_error: 1.3844 - val_loss: 1.8673 - val_root_mean_squared_error: 1.3665\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9214 - root_mean_squared_error: 1.3861 - val_loss: 1.8768 - val_root_mean_squared_error: 1.3700\n",
      "01-12 15:37:14 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 15:37:14 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 15:37:14 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112153502_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 15:37:14 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:37:14 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 15:37:15 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:37:15 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 10\n",
      "#########################\n",
      "01-12 15:37:15 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7b266b9b44c0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 15:37:15 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 15:37:15 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:37:15 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 15:37:15 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 15:37:15 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.027233600616455078s\n",
      "01-12 15:37:15 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 15:37:15 I deeptables.m.preprocessor.py 383 - Imputation taken 0.10564517974853516s\n",
      "01-12 15:37:15 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 15:37:15 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.05967354774475098s\n",
      "01-12 15:37:15 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.25165843963623047s\n",
      "01-12 15:37:15 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 15:37:15 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 15:37:15 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:37:15 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 15:37:15 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 15:37:16 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 15:37:16 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 8s 11ms/step - loss: 64.6548 - root_mean_squared_error: 8.0408 - val_loss: 33.7768 - val_root_mean_squared_error: 5.8118\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 19.8762 - root_mean_squared_error: 4.4583 - val_loss: 11.3564 - val_root_mean_squared_error: 3.3699\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 7.6897 - root_mean_squared_error: 2.7730 - val_loss: 5.0184 - val_root_mean_squared_error: 2.2402\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 4.0019 - root_mean_squared_error: 2.0005 - val_loss: 3.0263 - val_root_mean_squared_error: 1.7396\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.7456 - root_mean_squared_error: 1.6570 - val_loss: 2.4352 - val_root_mean_squared_error: 1.5605\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.3013 - root_mean_squared_error: 1.5170 - val_loss: 2.2018 - val_root_mean_squared_error: 1.4838\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 2.1541 - root_mean_squared_error: 1.4677 - val_loss: 2.0690 - val_root_mean_squared_error: 1.4384\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 2.1190 - root_mean_squared_error: 1.4557 - val_loss: 2.0310 - val_root_mean_squared_error: 1.4251\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0758 - root_mean_squared_error: 1.4407 - val_loss: 2.0290 - val_root_mean_squared_error: 1.4244\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0332 - root_mean_squared_error: 1.4259 - val_loss: 2.0128 - val_root_mean_squared_error: 1.4187\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0232 - root_mean_squared_error: 1.4224 - val_loss: 2.0389 - val_root_mean_squared_error: 1.4279\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0099 - root_mean_squared_error: 1.4177 - val_loss: 2.0403 - val_root_mean_squared_error: 1.4284\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9914 - root_mean_squared_error: 1.4112 - val_loss: 1.9679 - val_root_mean_squared_error: 1.4028\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0454 - root_mean_squared_error: 1.4302 - val_loss: 1.9531 - val_root_mean_squared_error: 1.3976\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0293 - root_mean_squared_error: 1.4245 - val_loss: 1.9717 - val_root_mean_squared_error: 1.4042\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9952 - root_mean_squared_error: 1.4125 - val_loss: 2.0291 - val_root_mean_squared_error: 1.4245\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0368 - root_mean_squared_error: 1.4271 - val_loss: 1.9800 - val_root_mean_squared_error: 1.4071\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9934 - root_mean_squared_error: 1.4119 - val_loss: 2.2630 - val_root_mean_squared_error: 1.5043\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9674 - root_mean_squared_error: 1.4027 - val_loss: 1.9192 - val_root_mean_squared_error: 1.3854\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9619 - root_mean_squared_error: 1.4007 - val_loss: 1.9776 - val_root_mean_squared_error: 1.4063\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9846 - root_mean_squared_error: 1.4088 - val_loss: 1.9282 - val_root_mean_squared_error: 1.3886\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9651 - root_mean_squared_error: 1.4018 - val_loss: 1.9755 - val_root_mean_squared_error: 1.4055\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0114 - root_mean_squared_error: 1.4182 - val_loss: 2.2348 - val_root_mean_squared_error: 1.4949\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.0129 - root_mean_squared_error: 1.4188 - val_loss: 1.9287 - val_root_mean_squared_error: 1.3888\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9655 - root_mean_squared_error: 1.4019 - val_loss: 2.1598 - val_root_mean_squared_error: 1.4696\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0314 - root_mean_squared_error: 1.4253 - val_loss: 1.9220 - val_root_mean_squared_error: 1.3864\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0420 - root_mean_squared_error: 1.4290 - val_loss: 2.0085 - val_root_mean_squared_error: 1.4172\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9638 - root_mean_squared_error: 1.4014 - val_loss: 1.9392 - val_root_mean_squared_error: 1.3925\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9570 - root_mean_squared_error: 1.3989 - val_loss: 1.9335 - val_root_mean_squared_error: 1.3905\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9744 - root_mean_squared_error: 1.4051 - val_loss: 1.9547 - val_root_mean_squared_error: 1.3981\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9915 - root_mean_squared_error: 1.4112 - val_loss: 1.9441 - val_root_mean_squared_error: 1.3943\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9725 - root_mean_squared_error: 1.4045 - val_loss: 1.8862 - val_root_mean_squared_error: 1.3734\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.0285 - root_mean_squared_error: 1.4243 - val_loss: 2.1203 - val_root_mean_squared_error: 1.4561\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9542 - root_mean_squared_error: 1.3979 - val_loss: 1.9416 - val_root_mean_squared_error: 1.3934\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9783 - root_mean_squared_error: 1.4065 - val_loss: 1.9506 - val_root_mean_squared_error: 1.3966\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9693 - root_mean_squared_error: 1.4033 - val_loss: 1.9161 - val_root_mean_squared_error: 1.3842\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9574 - root_mean_squared_error: 1.3991 - val_loss: 1.9982 - val_root_mean_squared_error: 1.4136\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9438 - root_mean_squared_error: 1.3942 - val_loss: 1.9372 - val_root_mean_squared_error: 1.3918\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9567 - root_mean_squared_error: 1.3988 - val_loss: 2.0323 - val_root_mean_squared_error: 1.4256\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9998 - root_mean_squared_error: 1.4141 - val_loss: 2.0751 - val_root_mean_squared_error: 1.4405\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9543 - root_mean_squared_error: 1.3980 - val_loss: 1.9245 - val_root_mean_squared_error: 1.3873\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9544 - root_mean_squared_error: 1.3980 - val_loss: 1.9304 - val_root_mean_squared_error: 1.3894\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9668 - root_mean_squared_error: 1.4024 - val_loss: 1.9632 - val_root_mean_squared_error: 1.4012\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9836 - root_mean_squared_error: 1.4084 - val_loss: 2.1860 - val_root_mean_squared_error: 1.4785\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9648 - root_mean_squared_error: 1.4017 - val_loss: 1.9807 - val_root_mean_squared_error: 1.4074\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9756 - root_mean_squared_error: 1.4056 - val_loss: 1.9568 - val_root_mean_squared_error: 1.3989\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9423 - root_mean_squared_error: 1.3937 - val_loss: 1.9269 - val_root_mean_squared_error: 1.3881\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9694 - root_mean_squared_error: 1.4033 - val_loss: 1.9313 - val_root_mean_squared_error: 1.3897\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9436 - root_mean_squared_error: 1.3941 - val_loss: 1.9161 - val_root_mean_squared_error: 1.3842\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9375 - root_mean_squared_error: 1.3920 - val_loss: 1.9562 - val_root_mean_squared_error: 1.3987\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9334 - root_mean_squared_error: 1.3905 - val_loss: 1.9227 - val_root_mean_squared_error: 1.3866\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9679 - root_mean_squared_error: 1.4028 - val_loss: 1.9695 - val_root_mean_squared_error: 1.4034\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9797 - root_mean_squared_error: 1.4070 - val_loss: 1.9775 - val_root_mean_squared_error: 1.4062\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9482 - root_mean_squared_error: 1.3958 - val_loss: 2.4470 - val_root_mean_squared_error: 1.5643\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9722 - root_mean_squared_error: 1.4043 - val_loss: 2.0217 - val_root_mean_squared_error: 1.4219\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9330 - root_mean_squared_error: 1.3903 - val_loss: 1.9135 - val_root_mean_squared_error: 1.3833\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9753 - root_mean_squared_error: 1.4055 - val_loss: 1.8993 - val_root_mean_squared_error: 1.3782\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9216 - root_mean_squared_error: 1.3862 - val_loss: 1.9246 - val_root_mean_squared_error: 1.3873\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9368 - root_mean_squared_error: 1.3917 - val_loss: 1.9150 - val_root_mean_squared_error: 1.3838\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9277 - root_mean_squared_error: 1.3884 - val_loss: 1.8975 - val_root_mean_squared_error: 1.3775\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9470 - root_mean_squared_error: 1.3953 - val_loss: 1.9290 - val_root_mean_squared_error: 1.3889\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9205 - root_mean_squared_error: 1.3858 - val_loss: 1.9245 - val_root_mean_squared_error: 1.3873\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9314 - root_mean_squared_error: 1.3898 - val_loss: 1.9295 - val_root_mean_squared_error: 1.3891\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9415 - root_mean_squared_error: 1.3934 - val_loss: 1.9301 - val_root_mean_squared_error: 1.3893\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9399 - root_mean_squared_error: 1.3928 - val_loss: 2.0120 - val_root_mean_squared_error: 1.4184\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9304 - root_mean_squared_error: 1.3894 - val_loss: 1.9109 - val_root_mean_squared_error: 1.3824\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9279 - root_mean_squared_error: 1.3885 - val_loss: 1.9699 - val_root_mean_squared_error: 1.4035\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9401 - root_mean_squared_error: 1.3929 - val_loss: 2.0226 - val_root_mean_squared_error: 1.4222\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9374 - root_mean_squared_error: 1.3919 - val_loss: 1.9284 - val_root_mean_squared_error: 1.3887\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9240 - root_mean_squared_error: 1.3871 - val_loss: 1.9192 - val_root_mean_squared_error: 1.3854\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9221 - root_mean_squared_error: 1.3864 - val_loss: 2.0228 - val_root_mean_squared_error: 1.4222\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9156 - root_mean_squared_error: 1.3840 - val_loss: 1.9227 - val_root_mean_squared_error: 1.3866\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9548 - root_mean_squared_error: 1.3982 - val_loss: 1.9400 - val_root_mean_squared_error: 1.3928\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9243 - root_mean_squared_error: 1.3872 - val_loss: 1.9063 - val_root_mean_squared_error: 1.3807\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9244 - root_mean_squared_error: 1.3872 - val_loss: 1.9393 - val_root_mean_squared_error: 1.3926\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9254 - root_mean_squared_error: 1.3876 - val_loss: 1.9269 - val_root_mean_squared_error: 1.3881\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9316 - root_mean_squared_error: 1.3898 - val_loss: 1.9188 - val_root_mean_squared_error: 1.3852\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9285 - root_mean_squared_error: 1.3887 - val_loss: 1.9191 - val_root_mean_squared_error: 1.3853\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9124 - root_mean_squared_error: 1.3829 - val_loss: 1.9203 - val_root_mean_squared_error: 1.3857\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9312 - root_mean_squared_error: 1.3897 - val_loss: 1.9233 - val_root_mean_squared_error: 1.3868\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9189 - root_mean_squared_error: 1.3852 - val_loss: 1.9086 - val_root_mean_squared_error: 1.3815\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9229 - root_mean_squared_error: 1.3867 - val_loss: 1.9336 - val_root_mean_squared_error: 1.3906\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9206 - root_mean_squared_error: 1.3859 - val_loss: 1.9235 - val_root_mean_squared_error: 1.3869\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9163 - root_mean_squared_error: 1.3843 - val_loss: 1.9954 - val_root_mean_squared_error: 1.4126\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9123 - root_mean_squared_error: 1.3828 - val_loss: 1.8916 - val_root_mean_squared_error: 1.3753\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9255 - root_mean_squared_error: 1.3876 - val_loss: 1.9323 - val_root_mean_squared_error: 1.3901\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9252 - root_mean_squared_error: 1.3875 - val_loss: 1.9241 - val_root_mean_squared_error: 1.3871\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9212 - root_mean_squared_error: 1.3861 - val_loss: 1.9110 - val_root_mean_squared_error: 1.3824\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9201 - root_mean_squared_error: 1.3857 - val_loss: 2.0608 - val_root_mean_squared_error: 1.4355\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9273 - root_mean_squared_error: 1.3883 - val_loss: 1.9246 - val_root_mean_squared_error: 1.3873\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9166 - root_mean_squared_error: 1.3844 - val_loss: 1.8861 - val_root_mean_squared_error: 1.3733\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9196 - root_mean_squared_error: 1.3855 - val_loss: 1.9225 - val_root_mean_squared_error: 1.3865\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9221 - root_mean_squared_error: 1.3864 - val_loss: 1.9400 - val_root_mean_squared_error: 1.3929\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9288 - root_mean_squared_error: 1.3888 - val_loss: 1.9284 - val_root_mean_squared_error: 1.3887\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9170 - root_mean_squared_error: 1.3845 - val_loss: 1.9280 - val_root_mean_squared_error: 1.3885\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9156 - root_mean_squared_error: 1.3841 - val_loss: 1.9166 - val_root_mean_squared_error: 1.3844\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9129 - root_mean_squared_error: 1.3831 - val_loss: 1.9470 - val_root_mean_squared_error: 1.3954\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9213 - root_mean_squared_error: 1.3861 - val_loss: 1.9069 - val_root_mean_squared_error: 1.3809\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 1.9152 - root_mean_squared_error: 1.3839 - val_loss: 1.9219 - val_root_mean_squared_error: 1.3863\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 1.9185 - root_mean_squared_error: 1.3851 - val_loss: 1.9118 - val_root_mean_squared_error: 1.3827\n",
      "01-12 15:39:25 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 15:39:25 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 15:39:26 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112153715_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 15:39:26 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:39:26 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 15:39:26 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 15:39:26 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "CPU times: user 36min 24s, sys: 1min 47s, total: 38min 12s\n",
      "Wall time: 21min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FOLDS = 10\n",
    "    \n",
    "oof_lgb  = np.zeros(len(train_df))\n",
    "pred_lgb = np.zeros(len(test_df))\n",
    "\n",
    "models = {}\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    \n",
    "    print(\"#\"*25)\n",
    "    print(f\"### Fold {fold+1}\")\n",
    "    print(\"#\"*25)\n",
    "\n",
    "    x_train = train_df[train_df['fold']!=fold][FEATURES].copy()\n",
    "    y_train = train_df[train_df['fold']!=fold][\"y\"].copy()\n",
    "    x_valid = train_df[train_df['fold']==fold][FEATURES].copy()\n",
    "    y_valid = train_df[train_df['fold']==fold][\"y\"].copy()\n",
    "    x_test  = test_df[FEATURES].copy()\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "    config      = deeptable.ModelConfig(\n",
    "                                            categorical_columns='auto',\n",
    "                                            nets =['linear'], \n",
    "                                            stacking_op = 'add', \n",
    "                                            earlystopping_patience=15, \n",
    "                                            metrics=[\"RootMeanSquaredError\"],\n",
    "                                            optimizer=optimizer\n",
    "                                        )\n",
    "\n",
    "    dt             = deeptable.DeepTable(config=config)\n",
    "    model, history = dt.fit(x_train, y_train, epochs=100)\n",
    "\n",
    "    # INFER OOF\n",
    "    print(f\"predicting on valid data\")\n",
    "    model_out              = model.predict(x_valid)\n",
    "    oof_lgb[x_valid.index] = model_out.flatten()\n",
    "    # INFER TEST\n",
    "    print(f\"predicting on test data\")\n",
    "    pred_lgb              += (model.predict(x_test)).flatten()\n",
    "\n",
    "    models[fold] = model\n",
    "\n",
    "# COMPUTE AVERAGE TEST PREDS\n",
    "pred_lgb /= FOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a0b35",
   "metadata": {
    "papermill": {
     "duration": 1.534201,
     "end_time": "2025-01-12T15:39:29.598798",
     "exception": false,
     "start_time": "2025-01-12T15:39:28.064597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 7: Model score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0838f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:39:32.686912Z",
     "iopub.status.busy": "2025-01-12T15:39:32.686445Z",
     "iopub.status.idle": "2025-01-12T15:39:32.940841Z",
     "shell.execute_reply": "2025-01-12T15:39:32.939355Z"
    },
    "papermill": {
     "duration": 1.789121,
     "end_time": "2025-01-12T15:39:32.943238",
     "exception": false,
     "start_time": "2025-01-12T15:39:31.154117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To evaluate the equitable prediction of transplant survival outcomes,\n",
    "we use the concordance index (C-index) between a series of event\n",
    "times and a predicted score across each race group.\n",
    " \n",
    "It represents the global assessment of the model discrimination power:\n",
    "this is the model’s ability to correctly provide a reliable ranking\n",
    "of the survival times based on the individual risk scores.\n",
    " \n",
    "The concordance index is a value between 0 and 1 where:\n",
    " \n",
    "0.5 is the expected result from random predictions,\n",
    "1.0 is perfect concordance (with no censoring, otherwise <1.0),\n",
    "0.0 is perfect anti-concordance (with no censoring, otherwise >0.0)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import numpy as np\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = \"id\"\n",
    "    >>> y_pred = {'prediction': {0: 1.0, 1: 0.0, 2: 1.0}}\n",
    "    >>> y_pred = pd.DataFrame(y_pred)\n",
    "    >>> y_pred.insert(0, row_id_column_name, range(len(y_pred)))\n",
    "    >>> y_true = { 'efs': {0: 1.0, 1: 0.0, 2: 0.0}, 'efs_time': {0: 25.1234,1: 250.1234,2: 2500.1234}, 'race_group': {0: 'race_group_1', 1: 'race_group_1', 2: 'race_group_1'}}\n",
    "    >>> y_true = pd.DataFrame(y_true)\n",
    "    >>> y_true.insert(0, row_id_column_name, range(len(y_true)))\n",
    "    >>> score(y_true.copy(), y_pred.copy(), row_id_column_name)\n",
    "    0.75\n",
    "    \"\"\"\n",
    "    \n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "    \n",
    "    event_label = 'efs'\n",
    "    interval_label = 'efs_time'\n",
    "    prediction_label = 'prediction'\n",
    "    for col in submission.columns:\n",
    "        if not pandas.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "    # Merging solution and submission dfs on ID\n",
    "    merged_df = pd.concat([solution, submission], axis=1)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n",
    "    metric_list = []\n",
    "    for race in merged_df_race_dict.keys():\n",
    "        # Retrieving values from y_test based on index\n",
    "        indices = sorted(merged_df_race_dict[race])\n",
    "        merged_df_race = merged_df.iloc[indices]\n",
    "        # Calculate the concordance index\n",
    "        c_index_race = concordance_index(\n",
    "                        merged_df_race[interval_label],\n",
    "                        -merged_df_race[prediction_label],\n",
    "                        merged_df_race[event_label])\n",
    "        metric_list.append(c_index_race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466297c",
   "metadata": {
    "papermill": {
     "duration": 1.540666,
     "end_time": "2025-01-12T15:39:36.088218",
     "exception": false,
     "start_time": "2025-01-12T15:39:34.547552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 8: Model CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41551201",
   "metadata": {
    "_cell_guid": "a16eceda-8973-4c17-8f13-0423fe0cb225",
    "_uuid": "ee05ecfe-1315-47c7-aac9-090b3cd60b56",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T15:39:39.137581Z",
     "iopub.status.busy": "2025-01-12T15:39:39.137031Z",
     "iopub.status.idle": "2025-01-12T15:39:39.594855Z",
     "shell.execute_reply": "2025-01-12T15:39:39.593174Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2.000369,
     "end_time": "2025-01-12T15:39:39.597491",
     "exception": false,
     "start_time": "2025-01-12T15:39:37.597122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-a1aa375cd29b>:53: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall CV for DeepTables = 0.6471932382706278\n"
     ]
    }
   ],
   "source": [
    "y_true = train_df[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "y_pred = train_df[[\"ID\"]].copy()\n",
    "y_pred[\"prediction\"] = oof_lgb\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(f\"\\nOverall CV for DeepTables =\",m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa310f",
   "metadata": {
    "papermill": {
     "duration": 1.36882,
     "end_time": "2025-01-12T15:39:42.528504",
     "exception": false,
     "start_time": "2025-01-12T15:39:41.159684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 9: Create submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b681eea7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:39:45.572680Z",
     "iopub.status.busy": "2025-01-12T15:39:45.572235Z",
     "iopub.status.idle": "2025-01-12T15:39:45.597811Z",
     "shell.execute_reply": "2025-01-12T15:39:45.596411Z"
    },
    "papermill": {
     "duration": 1.559681,
     "end_time": "2025-01-12T15:39:45.600074",
     "exception": false,
     "start_time": "2025-01-12T15:39:44.040393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub shape: (3, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28800</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28801</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28802</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  prediction\n",
       "0  28800         2.0\n",
       "1  28801         3.0\n",
       "2  28802         1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n",
    "sub.prediction = rankdata(pred_lgb) \n",
    "sub.to_csv(\"submission.csv\",index=False)\n",
    "print(\"Sub shape:\",sub.shape)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c1553",
   "metadata": {
    "papermill": {
     "duration": 1.509176,
     "end_time": "2025-01-12T15:39:48.622488",
     "exception": false,
     "start_time": "2025-01-12T15:39:47.113312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 10: Save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c35e3a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:39:51.578792Z",
     "iopub.status.busy": "2025-01-12T15:39:51.577972Z",
     "iopub.status.idle": "2025-01-12T15:39:51.585393Z",
     "shell.execute_reply": "2025-01-12T15:39:51.583606Z"
    },
    "papermill": {
     "duration": 1.481412,
     "end_time": "2025-01-12T15:39:51.588206",
     "exception": false,
     "start_time": "2025-01-12T15:39:50.106794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7acafb71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:39:54.577733Z",
     "iopub.status.busy": "2025-01-12T15:39:54.577312Z",
     "iopub.status.idle": "2025-01-12T15:39:55.334817Z",
     "shell.execute_reply": "2025-01-12T15:39:55.333143Z"
    },
    "papermill": {
     "duration": 2.277969,
     "end_time": "2025-01-12T15:39:55.337924",
     "exception": false,
     "start_time": "2025-01-12T15:39:53.059955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    }
   ],
   "source": [
    "for fold, model in models.items():\n",
    "    model.save(f'model_fold_{fold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62e1cd1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:39:58.353295Z",
     "iopub.status.busy": "2025-01-12T15:39:58.352755Z",
     "iopub.status.idle": "2025-01-12T15:40:30.053833Z",
     "shell.execute_reply": "2025-01-12T15:40:30.052185Z"
    },
    "papermill": {
     "duration": 33.226801,
     "end_time": "2025-01-12T15:40:30.056566",
     "exception": false,
     "start_time": "2025-01-12T15:39:56.829765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['predictions'] = oof_lgb\n",
    "train_df.to_excel(\"dt_exp_01_oof.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d932d49",
   "metadata": {
    "papermill": {
     "duration": 1.52382,
     "end_time": "2025-01-12T15:40:33.127501",
     "exception": false,
     "start_time": "2025-01-12T15:40:31.603681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    },
    {
     "datasetId": 4074593,
     "sourceId": 7074842,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4021289,
     "sourceId": 7570020,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6415434,
     "sourceId": 10370860,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 211322530,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1551.99237,
   "end_time": "2025-01-12T15:40:38.310465",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-12T15:14:46.318095",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
