{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555b3d98",
   "metadata": {
    "_cell_guid": "2971c992-4673-42e1-940b-a51f340798f7",
    "_uuid": "8728c17a-ca9b-4936-a2a3-16ef9d697979",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007212,
     "end_time": "2025-01-12T17:19:37.868110",
     "exception": false,
     "start_time": "2025-01-12T17:19:37.860898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c9caf6",
   "metadata": {
    "_cell_guid": "80d5b93f-de27-4388-9119-7a66db28cda6",
    "_uuid": "94ef81be-2b03-4e3b-9b24-6620bd3876c2",
    "execution": {
     "iopub.execute_input": "2025-01-12T17:19:37.882604Z",
     "iopub.status.busy": "2025-01-12T17:19:37.882218Z",
     "iopub.status.idle": "2025-01-12T17:21:25.106756Z",
     "shell.execute_reply": "2025-01-12T17:21:25.105064Z"
    },
    "papermill": {
     "duration": 107.234228,
     "end_time": "2025-01-12T17:21:25.109026",
     "exception": false,
     "start_time": "2025-01-12T17:19:37.874798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd==1.7.0) (1.26.4)\r\n",
      "autograd is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Processing /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: autograd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.7.0)\r\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.13.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd>=1.2.0->autograd-gamma==0.5.0) (1.26.4)\r\n",
      "Building wheels for collected packages: autograd-gamma\r\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=f314011f2b8e0eca0ec4afd76b738d95acf67b12f6ea0ea76bc5c0c71225fcb9\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/b5/e0/4c79e15c0b5f2c15ecf613c720bb20daab20a666eb67135155\r\n",
      "Successfully built autograd-gamma\r\n",
      "Installing collected packages: autograd-gamma\r\n",
      "Successfully installed autograd-gamma-0.5.0\r\n",
      "Processing /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\r\n",
      "Installing collected packages: interface-meta\r\n",
      "Successfully installed interface-meta-1.3.0\r\n",
      "Processing /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (2.1.4)\r\n",
      "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->formulaic==1.0.2) (1.16.0)\r\n",
      "Installing collected packages: formulaic\r\n",
      "Successfully installed formulaic-1.0.2\r\n",
      "Processing /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.13.1)\r\n",
      "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (2.1.4)\r\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (3.7.1)\r\n",
      "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.7.0)\r\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (0.5.0)\r\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.0.2)\r\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.16.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (4.53.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (24.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (3.1.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines==0.30.0) (1.16.0)\r\n",
      "Installing collected packages: lifelines\r\n",
      "Successfully installed lifelines-0.30.0\r\n",
      "Looking in links: /kaggle/input/tensorflow-2-15/tensorflow\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from tensorflow==2.15.0)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from tensorflow==2.15.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/tensorboard-2.15.1-py3-none-any.whl (from tensorflow==2.15.0)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/tensorflow_estimator-2.15.0-py2.py3-none-any.whl (from tensorflow==2.15.0)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/keras-2.15.0-py3-none-any.whl (from tensorflow==2.15.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.4)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\r\n",
      "Installing collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\r\n",
      "  Attempting uninstall: wrapt\r\n",
      "    Found existing installation: wrapt 1.16.0\r\n",
      "    Uninstalling wrapt-1.16.0:\r\n",
      "      Successfully uninstalled wrapt-1.16.0\r\n",
      "  Attempting uninstall: ml-dtypes\r\n",
      "    Found existing installation: ml-dtypes 0.4.1\r\n",
      "    Uninstalling ml-dtypes-0.4.1:\r\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 3.4.1\r\n",
      "    Uninstalling keras-3.4.1:\r\n",
      "      Successfully uninstalled keras-3.4.1\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.17.0\r\n",
      "    Uninstalling tensorboard-2.17.0:\r\n",
      "      Successfully uninstalled tensorboard-2.17.0\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.17.0\r\n",
      "    Uninstalling tensorflow-2.17.0:\r\n",
      "      Successfully uninstalled tensorflow-2.17.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.15.0 which is incompatible.\r\n",
      "tensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.15.0 which is incompatible.\r\n",
      "tensorstore 0.1.65 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\r\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\r\n",
      "Looking in links: /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/deeptables-0.2.5-py3-none-any.whl\r\n",
      "\u001b[33mWARNING: Package 'deeptables' has an invalid Requires-Python: Invalid specifier: '>=3.6.*'\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (24.1)\r\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (1.13.1)\r\n",
      "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (2.1.4)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (1.2.2)\r\n",
      "Requirement already satisfied: lightgbm>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (4.5.0)\r\n",
      "Requirement already satisfied: category-encoders>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (2.6.4)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/hypernets-0.3.1-py3-none-any.whl (from deeptables==0.2.5)\r\n",
      "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (3.11.0)\r\n",
      "Requirement already satisfied: eli5 in /usr/local/lib/python3.10/dist-packages (from deeptables==0.2.5) (0.13.0)\r\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders>=2.1.0->deeptables==0.2.5) (0.14.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category-encoders>=2.1.0->deeptables==0.2.5) (0.5.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (2024.6.1)\r\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (7.34.0)\r\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (5.7.1)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/XlsxWriter-3.1.9-py3-none-any.whl (from hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (5.9.5)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (6.0.2)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/paramiko-3.4.0-py3-none-any.whl (from hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (2.32.3)\r\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (6.3.3)\r\n",
      "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (3.11.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (4.66.5)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (1.4.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->deeptables==0.2.5) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->deeptables==0.2.5) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->deeptables==0.2.5) (2024.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->deeptables==0.2.5) (3.5.0)\r\n",
      "Requirement already satisfied: attrs>17.1.0 in /usr/local/lib/python3.10/dist-packages (from eli5->deeptables==0.2.5) (24.2.0)\r\n",
      "Requirement already satisfied: jinja2>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from eli5->deeptables==0.2.5) (3.1.4)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from eli5->deeptables==0.2.5) (1.16.0)\r\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from eli5->deeptables==0.2.5) (0.20.3)\r\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from eli5->deeptables==0.2.5) (0.9.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.0.0->eli5->deeptables==0.2.5) (2.1.5)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (71.0.4)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.19.2)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (4.4.2)\r\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (3.0.47)\r\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (2.18.0)\r\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.2.0)\r\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.1.7)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (4.9.0)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (43.0.1)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->hypernets>=0.2.5.1->deeptables==0.2.5) (0.2.13)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (2024.8.30)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (1.17.1)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.8.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.7.0)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (2.22)\r\n",
      "Installing collected packages: XlsxWriter, bcrypt, pynacl, paramiko, hypernets, deeptables\r\n",
      "Successfully installed XlsxWriter-3.1.9 bcrypt-4.1.2 deeptables-0.2.5 hypernets-0.3.1 paramiko-3.4.0 pynacl-1.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
    "!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\n",
    "!pip install --no-index -U --find-links=/kaggle/input/tensorflow-2-15/tensorflow tensorflow==2.15.0\n",
    "!pip install --no-index -U --find-links=/kaggle/input/deeptables-v0-2-5/deeptables-0.2.5 deeptables==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a39d38f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:21:25.130468Z",
     "iopub.status.busy": "2025-01-12T17:21:25.130108Z",
     "iopub.status.idle": "2025-01-12T17:21:25.134962Z",
     "shell.execute_reply": "2025-01-12T17:21:25.133805Z"
    },
    "papermill": {
     "duration": 0.017603,
     "end_time": "2025-01-12T17:21:25.136814",
     "exception": false,
     "start_time": "2025-01-12T17:21:25.119211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install deeptables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a44ed",
   "metadata": {
    "_cell_guid": "13bd92b6-c130-451f-89f5-13538b54ff91",
    "_uuid": "b5920c89-c860-498c-8335-057d15d2a5ce",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010268,
     "end_time": "2025-01-12T17:21:25.158570",
     "exception": false,
     "start_time": "2025-01-12T17:21:25.148302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892b0742",
   "metadata": {
    "_cell_guid": "149452d5-3116-45fe-9dc8-f0574edca449",
    "_uuid": "15f750c5-85f5-4eb5-9059-d4cf826746b1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T17:21:25.180608Z",
     "iopub.status.busy": "2025-01-12T17:21:25.180196Z",
     "iopub.status.idle": "2025-01-12T17:21:36.637832Z",
     "shell.execute_reply": "2025-01-12T17:21:36.636126Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 11.471573,
     "end_time": "2025-01-12T17:21:36.639992",
     "exception": false,
     "start_time": "2025-01-12T17:21:25.168419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.15.0\n",
      "DeepTables version: 0.2.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import deeptables as dt\n",
    "from deeptables.models import deeptable, deepnets\n",
    "from deeptables.datasets import dsutils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print('Tensorflow version:',tf.__version__)\n",
    "print('DeepTables version:',dt.__version__)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b1551",
   "metadata": {
    "_cell_guid": "d1996c37-ed51-40d0-80bf-683972841d04",
    "_uuid": "28c2f5c0-6627-44b8-9a87-f5ac3380cdc5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009662,
     "end_time": "2025-01-12T17:21:36.660580",
     "exception": false,
     "start_time": "2025-01-12T17:21:36.650918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "664422a2",
   "metadata": {
    "_cell_guid": "2c1f4cb8-2e2b-4555-9412-81aa90dabb84",
    "_uuid": "0cc2587e-3d0a-4177-bcbe-c980ffe47773",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T17:21:36.683210Z",
     "iopub.status.busy": "2025-01-12T17:21:36.682294Z",
     "iopub.status.idle": "2025-01-12T17:22:10.147387Z",
     "shell.execute_reply": "2025-01-12T17:22:10.146196Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 33.489085,
     "end_time": "2025-01-12T17:22:10.160147",
     "exception": false,
     "start_time": "2025-01-12T17:21:36.671062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (3, 58)\n",
      "Train shape: (28800, 61)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>dri_score</th>\n",
       "      <th>psych_disturb</th>\n",
       "      <th>cyto_score</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>hla_match_c_high</th>\n",
       "      <th>hla_high_res_8</th>\n",
       "      <th>tbi_status</th>\n",
       "      <th>arrhythmia</th>\n",
       "      <th>hla_low_res_6</th>\n",
       "      <th>graft_type</th>\n",
       "      <th>vent_hist</th>\n",
       "      <th>renal_issue</th>\n",
       "      <th>pulm_severe</th>\n",
       "      <th>prim_disease_hct</th>\n",
       "      <th>hla_high_res_6</th>\n",
       "      <th>cmv_status</th>\n",
       "      <th>hla_high_res_10</th>\n",
       "      <th>hla_match_dqb1_high</th>\n",
       "      <th>tce_imm_match</th>\n",
       "      <th>hla_nmdp_6</th>\n",
       "      <th>hla_match_c_low</th>\n",
       "      <th>rituximab</th>\n",
       "      <th>hla_match_drb1_low</th>\n",
       "      <th>hla_match_dqb1_low</th>\n",
       "      <th>prod_type</th>\n",
       "      <th>cyto_score_detail</th>\n",
       "      <th>conditioning_intensity</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>year_hct</th>\n",
       "      <th>obesity</th>\n",
       "      <th>mrd_hct</th>\n",
       "      <th>in_vivo_tcd</th>\n",
       "      <th>tce_match</th>\n",
       "      <th>hla_match_a_high</th>\n",
       "      <th>hepatic_severe</th>\n",
       "      <th>donor_age</th>\n",
       "      <th>prior_tumor</th>\n",
       "      <th>hla_match_b_low</th>\n",
       "      <th>peptic_ulcer</th>\n",
       "      <th>age_at_hct</th>\n",
       "      <th>hla_match_a_low</th>\n",
       "      <th>gvhd_proph</th>\n",
       "      <th>rheum_issue</th>\n",
       "      <th>sex_match</th>\n",
       "      <th>hla_match_b_high</th>\n",
       "      <th>race_group</th>\n",
       "      <th>comorbidity_score</th>\n",
       "      <th>karnofsky_score</th>\n",
       "      <th>hepatic_mild</th>\n",
       "      <th>tce_div_match</th>\n",
       "      <th>donor_related</th>\n",
       "      <th>melphalan_dose</th>\n",
       "      <th>hla_low_res_8</th>\n",
       "      <th>cardiac</th>\n",
       "      <th>hla_match_drb1_high</th>\n",
       "      <th>pulm_moderate</th>\n",
       "      <th>hla_low_res_10</th>\n",
       "      <th>efs</th>\n",
       "      <th>efs_time</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>N/A - non-malignant indication</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Bone marrow</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>IEA</td>\n",
       "      <td>6.0</td>\n",
       "      <td>+/+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>2016</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>9.942</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FKalone</td>\n",
       "      <td>No</td>\n",
       "      <td>M-F</td>\n",
       "      <td>2.0</td>\n",
       "      <td>More than one race</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unrelated</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.356</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>No</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>TBI +- Other, &gt;cGy</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Peripheral blood</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>AML</td>\n",
       "      <td>6.0</td>\n",
       "      <td>+/+</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>P/P</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PB</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>MAC</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>2008</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>No</td>\n",
       "      <td>Permissive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>72.29</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>43.705</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Other GVHD Prophylaxis</td>\n",
       "      <td>No</td>\n",
       "      <td>F-F</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Asian</td>\n",
       "      <td>3.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Related</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.672</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>N/A - non-malignant indication</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Bone marrow</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>HIS</td>\n",
       "      <td>6.0</td>\n",
       "      <td>+/+</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>P/P</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>2019</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>33.997</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Cyclophosphamide alone</td>\n",
       "      <td>No</td>\n",
       "      <td>F-M</td>\n",
       "      <td>2.0</td>\n",
       "      <td>More than one race</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Related</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.793</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Bone marrow</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>ALL</td>\n",
       "      <td>6.0</td>\n",
       "      <td>+/+</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>P/P</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BM</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>MAC</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>2009</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>No</td>\n",
       "      <td>Permissive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>29.23</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>43.245</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FK+ MMF +- others</td>\n",
       "      <td>No</td>\n",
       "      <td>M-M</td>\n",
       "      <td>2.0</td>\n",
       "      <td>White</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Unrelated</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>102.349</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Peripheral blood</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>MPN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>+/+</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAC</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2018</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>56.81</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>29.740</td>\n",
       "      <td>2.0</td>\n",
       "      <td>TDEPLETION +- other</td>\n",
       "      <td>No</td>\n",
       "      <td>M-F</td>\n",
       "      <td>2.0</td>\n",
       "      <td>American Indian or Alaska Native</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Related</td>\n",
       "      <td>MEL</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.223</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                       dri_score psych_disturb    cyto_score diabetes  \\\n",
       "0   0  N/A - non-malignant indication            No           NaN       No   \n",
       "1   1                    Intermediate            No  Intermediate       No   \n",
       "2   2  N/A - non-malignant indication            No           NaN       No   \n",
       "3   3                            High            No  Intermediate       No   \n",
       "4   4                            High            No           NaN       No   \n",
       "\n",
       "   hla_match_c_high  hla_high_res_8          tbi_status arrhythmia  \\\n",
       "0               NaN             NaN              No TBI         No   \n",
       "1               2.0             8.0  TBI +- Other, >cGy         No   \n",
       "2               2.0             8.0              No TBI         No   \n",
       "3               2.0             8.0              No TBI         No   \n",
       "4               2.0             8.0              No TBI         No   \n",
       "\n",
       "   hla_low_res_6        graft_type vent_hist renal_issue pulm_severe  \\\n",
       "0            6.0       Bone marrow        No          No          No   \n",
       "1            6.0  Peripheral blood        No          No          No   \n",
       "2            6.0       Bone marrow        No          No          No   \n",
       "3            6.0       Bone marrow        No          No          No   \n",
       "4            6.0  Peripheral blood        No          No          No   \n",
       "\n",
       "  prim_disease_hct  hla_high_res_6 cmv_status  hla_high_res_10  \\\n",
       "0              IEA             6.0        +/+              NaN   \n",
       "1              AML             6.0        +/+             10.0   \n",
       "2              HIS             6.0        +/+             10.0   \n",
       "3              ALL             6.0        +/+             10.0   \n",
       "4              MPN             6.0        +/+             10.0   \n",
       "\n",
       "   hla_match_dqb1_high tce_imm_match  hla_nmdp_6  hla_match_c_low rituximab  \\\n",
       "0                  2.0           NaN         6.0              2.0        No   \n",
       "1                  2.0           P/P         6.0              2.0        No   \n",
       "2                  2.0           P/P         6.0              2.0        No   \n",
       "3                  2.0           P/P         6.0              2.0        No   \n",
       "4                  2.0           NaN         5.0              2.0        No   \n",
       "\n",
       "   hla_match_drb1_low  hla_match_dqb1_low prod_type cyto_score_detail  \\\n",
       "0                 2.0                 2.0        BM               NaN   \n",
       "1                 2.0                 2.0        PB      Intermediate   \n",
       "2                 2.0                 2.0        BM               NaN   \n",
       "3                 2.0                 2.0        BM      Intermediate   \n",
       "4                 2.0                 2.0        PB               NaN   \n",
       "\n",
       "  conditioning_intensity               ethnicity  year_hct obesity   mrd_hct  \\\n",
       "0                    NaN  Not Hispanic or Latino      2016      No       NaN   \n",
       "1                    MAC  Not Hispanic or Latino      2008      No  Positive   \n",
       "2                    NaN  Not Hispanic or Latino      2019      No       NaN   \n",
       "3                    MAC  Not Hispanic or Latino      2009      No  Positive   \n",
       "4                    MAC      Hispanic or Latino      2018      No       NaN   \n",
       "\n",
       "  in_vivo_tcd   tce_match  hla_match_a_high hepatic_severe  donor_age  \\\n",
       "0         Yes         NaN               2.0             No        NaN   \n",
       "1          No  Permissive               2.0             No      72.29   \n",
       "2         Yes         NaN               2.0             No        NaN   \n",
       "3          No  Permissive               2.0             No      29.23   \n",
       "4         Yes         NaN               2.0             No      56.81   \n",
       "\n",
       "  prior_tumor  hla_match_b_low peptic_ulcer  age_at_hct  hla_match_a_low  \\\n",
       "0          No              2.0           No       9.942              2.0   \n",
       "1          No              2.0           No      43.705              2.0   \n",
       "2          No              2.0           No      33.997              2.0   \n",
       "3          No              2.0           No      43.245              2.0   \n",
       "4          No              2.0           No      29.740              2.0   \n",
       "\n",
       "               gvhd_proph rheum_issue sex_match  hla_match_b_high  \\\n",
       "0                 FKalone          No       M-F               2.0   \n",
       "1  Other GVHD Prophylaxis          No       F-F               2.0   \n",
       "2  Cyclophosphamide alone          No       F-M               2.0   \n",
       "3       FK+ MMF +- others          No       M-M               2.0   \n",
       "4     TDEPLETION +- other          No       M-F               2.0   \n",
       "\n",
       "                         race_group  comorbidity_score  karnofsky_score  \\\n",
       "0                More than one race                0.0             90.0   \n",
       "1                             Asian                3.0             90.0   \n",
       "2                More than one race                0.0             90.0   \n",
       "3                             White                0.0             90.0   \n",
       "4  American Indian or Alaska Native                1.0             90.0   \n",
       "\n",
       "  hepatic_mild          tce_div_match donor_related      melphalan_dose  \\\n",
       "0           No                    NaN     Unrelated  N/A, Mel not given   \n",
       "1           No  Permissive mismatched       Related  N/A, Mel not given   \n",
       "2           No  Permissive mismatched       Related  N/A, Mel not given   \n",
       "3          Yes  Permissive mismatched     Unrelated  N/A, Mel not given   \n",
       "4           No  Permissive mismatched       Related                 MEL   \n",
       "\n",
       "   hla_low_res_8 cardiac  hla_match_drb1_high pulm_moderate  hla_low_res_10  \\\n",
       "0            8.0      No                  2.0            No            10.0   \n",
       "1            8.0      No                  2.0           Yes            10.0   \n",
       "2            8.0      No                  2.0            No            10.0   \n",
       "3            8.0      No                  2.0            No            10.0   \n",
       "4            8.0      No                  2.0            No            10.0   \n",
       "\n",
       "   efs  efs_time  fold  \n",
       "0    0    42.356     3  \n",
       "1    1     4.672     7  \n",
       "2    0    19.793     6  \n",
       "3    0   102.349     1  \n",
       "4    0    16.223     4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "print(\"Test shape:\", test_df.shape )\n",
    "\n",
    "train_df = pd.read_excel(\"/kaggle/input/cibmtr-2024-dataset/random_folding.xlsx\")\n",
    "print(\"Train shape:\",train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c8404",
   "metadata": {
    "_cell_guid": "4001c0b6-a391-498c-9042-d52096e147a4",
    "_uuid": "ff046806-7265-4782-9d4a-d48eda71d477",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010899,
     "end_time": "2025-01-12T17:22:10.183561",
     "exception": false,
     "start_time": "2025-01-12T17:22:10.172662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4: Traget preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4557441",
   "metadata": {
    "_cell_guid": "561a9a6c-3f51-4052-aadd-464125a72c30",
    "_uuid": "f35f6325-e19a-47a1-b72e-cb06d603b17c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T17:22:10.216700Z",
     "iopub.status.busy": "2025-01-12T17:22:10.215916Z",
     "iopub.status.idle": "2025-01-12T17:22:10.886871Z",
     "shell.execute_reply": "2025-01-12T17:22:10.885370Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.685514,
     "end_time": "2025-01-12T17:22:10.888904",
     "exception": false,
     "start_time": "2025-01-12T17:22:10.203390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABeSElEQVR4nO3dd1gU1/s28HsXpMouYCiiNEsMGBQFC/ZCBEVjwaiRKHajYO9JRGMSib0rpoj9q9GoSVRQFHvsiiaoWMFGUREQjIDsvH/4Mj9XOgILzP25rr3injk788zuAnfOnJmRCYIggIiIiEjC5JougIiIiEjTGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiEijzp8/jxYtWsDQ0BAymQwRERGaLqlErF+/HjKZDNHR0Zouhf4/TX4mgwYNQtWqVctkW7Nnz4ZMJsPTp09LfVsV9edXJpNh9uzZJba+169fY+rUqbC2toZcLkePHj1KbN154e+Ykqet6QKoZMhkskL1O3LkCNq1a1e6xRRSZmYmPvvsM+jp6WHJkiUwMDCAra2tpssqE9HR0bC3ty9U33v37sHOzq50CyqC/fv349y5cyX6B4WKbu7cuXB0dCyTP765kfLP77vWrVuHBQsWYPz48WjcuDFsbGxKbN2a/pylhIGokti0aZPa840bNyIsLCxHu4ODQ1mWla87d+4gJiYGP//8M4YNG6bpcsqUmZlZjs9m0aJFePjwIZYsWZKjb3myf/9+rFq1qsIFogEDBqBfv37Q1dXVdCklYu7cuejdu7fG/lBK+ef3XeHh4ahRo0aOn92SkNfnXNm+z+UBA1El8cUXX6g9P3PmDMLCwnK0v+vly5cwMDAozdLylJCQAAAwNjYusXWmpaXB0NCwxNZXWgwNDXN8Ntu2bcPz588L/MwKQ6VSISMjA3p6eu+9rspCS0sLWlpami6j0iiNn9+KKiEhoczfB36fSx7nEElIu3bt8PHHH+PixYto06YNDAwM8NVXXwEA/vjjD3h5ecHKygq6urqoXbs2vvvuO2RlZeW6jmvXrqF9+/YwMDBAjRo1MH/+/BzbW7FiBerXrw8DAwOYmJjA1dUVW7duBfBmTkXbtm0BAJ999hlkMpnaobzw8HC0bt0ahoaGMDY2Rvfu3XH9+nW19WfPlbh27Rr69+8PExMTtGrVCgBgZ2eHrl274ujRo3B1dYW+vj6cnJxw9OhRAMCuXbvg5OQEPT09uLi44PLlyznqv3HjBnr37g1TU1Po6enB1dUVf/75Z45+kZGR6NChA/T19VGzZk18//33UKlUhfxU8rdw4UK0aNEC1apVg76+PlxcXLBz584c/WQyGfz9/bFlyxbUr18furq6CA0NBQBcvXoVbdu2VasvODg41/kHISEh4vtuZGQELy8vREZGissHDRqEVatWidvMfuTF19cXH3zwATIzM3Ms69SpE+rVq5fv/tvZ2WHQoEE52tu1a5fj0G9+3zcg9zkX2d+TkydPomnTptDT00OtWrWwcePGHNssyvuYl7t378LDwwOGhoawsrLCnDlzIAiCWp+0tDRMmjQJ1tbW0NXVRb169bBw4UK1fjKZDGlpadiwYYP4Gbz7PiUlJWHQoEEwNjaGUqnE4MGD8fLly0LVefbsWXh6ekKpVMLAwABt27bFqVOnxOX5/fzGxcVh8ODBqFmzJnR1dVG9enV07969wPfo6tWrGDRoEGrVqgU9PT1YWlpiyJAhePbsmVq/7J/727dvF7h/6enpmDBhAszMzGBkZIRPP/0UDx8+LNR7kP36WbNmoU6dOtDV1YW1tTWmTp2K9PR0AG8OfctkMhw5cgSRkZHiZ5H9e2bbtm1wcXGBkZERFAoFnJycsGzZskJvP7/POb/vc1n83quMOEIkMc+ePUPnzp3Rr18/fPHFF7CwsADw5oeratWqmDhxIqpWrYrw8HAEBAQgJSUFCxYsUFvH8+fP4enpiV69eqFPnz7YuXMnpk2bBicnJ3Tu3BkA8PPPP2Ps2LHo3bs3xo0bh1evXuHq1as4e/Ys+vfvj5EjR6JGjRqYO3cuxo4diyZNmoi1HDp0CJ07d0atWrUwe/Zs/Pfff1ixYgVatmyJS5cu5ZhP89lnn6Fu3bqYO3eu2h+N27dvi9v64osvsHDhQnTr1g1BQUH46quvMHr0aABAYGAg+vTpg6ioKMjlb/4fITIyEi1btkSNGjUwffp0GBoa4rfffkOPHj3w+++/o2fPngDe/PJv3749Xr9+Lfb76aefoK+vXyKf17Jly/Dpp5/Cx8cHGRkZ2LZtGz777DPs3bsXXl5ean3Dw8Px22+/wd/fHx988AHs7Ozw6NEjtG/fHjKZDDNmzIChoSF++eWXXIfZN23aBF9fX3h4eGDevHl4+fIl1qxZg1atWuHy5cuws7PDyJEj8fjx41wPx+ZmwIAB2LhxIw4cOICuXbuK7XFxcQgPD8esWbPe/01Cwd+3/Ny+fRu9e/fG0KFD4evri3Xr1mHQoEFwcXFB/fr1AaBI72NesrKy4OnpiebNm2P+/PkIDQ3FrFmz8Pr1a8yZMwcAIAgCPv30Uxw5cgRDhw6Fs7MzDhw4gClTpuDRo0fiIZlNmzZh2LBhaNq0KUaMGAEAqF27ttr2+vTpA3t7ewQGBuLSpUv45ZdfYG5ujnnz5uVbZ3h4ODp37gwXFxfMmjULcrkcwcHB6NChA06cOIGmTZvm+/Pr7e2NyMhIjBkzBnZ2dkhISEBYWBju37+f71y4sLAw3L17F4MHD4alpSUiIyPx008/ITIyEmfOnMkRvAuzf8OGDcPmzZvRv39/tGjRAuHh4Tl+bvKiUqnw6aef4uTJkxgxYgQcHBzwzz//YMmSJbh58yb27NkjHvr+4YcfkJqaisDAQABvpiaEhYXh888/R8eOHcWarl+/jlOnTmHcuHGFqqEwn/O7yuL3XqUlUKXk5+cnvPvxtm3bVgAgBAUF5ej/8uXLHG0jR44UDAwMhFevXuVYx8aNG8W29PR0wdLSUvD29hbbunfvLtSvXz/fGo8cOSIAEHbs2KHW7uzsLJibmwvPnj0T265cuSLI5XJh4MCBYtusWbMEAMLnn3+eY922trYCAOHvv/8W2w4cOCAAEPT19YWYmBixfe3atQIA4ciRI2Jbx44dBScnJ7V9V6lUQosWLYS6deuKbePHjxcACGfPnhXbEhISBKVSKQAQ7t27l+978DYvLy/B1tZWre3dzyUjI0P4+OOPhQ4dOqi1AxDkcrkQGRmp1j5mzBhBJpMJly9fFtuePXsmmJqaqtX34sULwdjYWBg+fLja6+Pi4gSlUqnWntt3Ky9ZWVlCzZo1hb59+6q1L168WJDJZMLdu3fzfb2tra3g6+ubo71t27ZC27ZtxeeF+b4FBwfn+EyyvyfHjx8X2xISEgRdXV1h0qRJYlth38e8+Pr6CgCEMWPGiG0qlUrw8vISdHR0hCdPngiCIAh79uwRAAjff/+92ut79+4tyGQy4fbt22KboaFhru9N9s/FkCFD1Np79uwpVKtWLd86VSqVULduXcHDw0NQqVRi+8uXLwV7e3vhk08+Edty+/l9/vy5AEBYsGBBvtvJTW6/g/73v//l+HwKu38RERECAGH06NFq/fr37y8AEGbNmpVvPZs2bRLkcrlw4sQJtfagoCABgHDq1CmxrW3btjm+f+PGjRMUCoXw+vXrfLdTkLw+5/y+z6X9e6+y4iEzidHV1cXgwYNztL89ovHixQs8ffoUrVu3xsuXL3Hjxg21vlWrVlWb56Kjo4OmTZvi7t27YpuxsTEePnyI8+fPF6m+2NhYREREYNCgQTA1NRXbGzRogE8++QT79+/P8Zovv/wy13U5OjrCzc1NfN6sWTMAQIcOHdTOAsluz64/MTER4eHh6NOnj/hePH36FM+ePYOHhwdu3bqFR48eAXgzwbh58+Zo2rSpuD4zMzP4+PgUab/z8vbn8vz5cyQnJ6N169a4dOlSjr5t27aFo6OjWltoaCjc3Nzg7OwstpmamuaoLywsDElJSfj888/F/X369Cm0tLTQrFkzHDlypFj1y+Vy+Pj44M8//8SLFy/E9i1btqBFixaFPtOuIMX9vgFvvietW7cWn5uZmaFevXpq3+fCvo8F8ff3F/+dfZgzIyMDhw4dAvDm+6SlpYWxY8eqvW7SpEkQBAEhISGF3ta7PxetW7fGs2fPkJKSkudrIiIicOvWLfTv3x/Pnj0TvwdpaWno2LEjjh8/nu/hYH19fejo6ODo0aN4/vx5oWvNfm22V69e4enTp2jevDkA5Pp9L2j/sn9XvPtejh8/vlD17NixAw4ODvjoo4/UfiY6dOgAAAX+TBgbGyMtLQ1hYWGF2l5JKYvfe5UVA5HE1KhRAzo6OjnaIyMj0bNnTyiVSigUCpiZmYmhJzk5Wa1vzZo1cwxfm5iYqP0CnDZtGqpWrYqmTZuibt268PPzU5uDkJeYmBgAyHVuiYODg/jL+W15/VF999RXpVIJALC2ts61Pbv+27dvQxAEzJw5E2ZmZmqP7EM82RNKY2JiULdu3RzbLmhuTGHt3bsXzZs3h56eHkxNTWFmZoY1a9bk+EyA3N+HmJgY1KlTJ0f7u223bt0C8OaX5rv7fPDgQXF/i2PgwIH477//sHv3bgBAVFQULl68iAEDBhR7ne8q7vcNyPk9AXJ+nwv7PuZHLpejVq1aam0ffvghAIjzQGJiYmBlZQUjIyO1ftlnh2b/fBTGu/tlYmICAPkGlezvga+vb47vwS+//IL09PRcv3vZdHV1MW/ePISEhMDCwgJt2rTB/PnzERcXV2C9iYmJGDduHCwsLKCvrw8zMzPxO53bNgvav5iYGMjl8hyHmAr7s3nr1i1ERkbmeB+yP7OCfiZGjx6NDz/8EJ07d0bNmjUxZMgQcV5faSqL33uVFecQSUxuc1uSkpLQtm1bKBQKzJkzB7Vr14aenh4uXbqEadOm5fg/wrzObBDemr/j4OCAqKgo7N27F6Ghofj999+xevVqBAQE4Ntvvy31fcqvzoLqz97fyZMnw8PDI9e+RflDWFwnTpzAp59+ijZt2mD16tWoXr06qlSpguDgYLXJwtneZ95S9j5v2rQJlpaWOZZraxf/V4WjoyNcXFywefNmDBw4EJs3b4aOjg769OlT4GvzmrCdlZWl9jm+z/etMN/niqg4+5X9PViwYIHaaNjbCrrA5Pjx49GtWzfs2bMHBw4cwMyZMxEYGIjw8HA0atQoz9f16dMHf//9N6ZMmQJnZ2dUrVoVKpUKnp6euY5KlfbnplKp4OTkhMWLF+e6/N2A8S5zc3NERETgwIEDCAkJQUhICIKDgzFw4EBs2LChRGrMTUX/vadJDESEo0eP4tmzZ9i1axfatGkjtt+7d++91mtoaIi+ffuib9++yMjIQK9evfDDDz9gxowZeZ4Onn1ht6ioqBzLbty4gQ8++KDUT6vP/r/4KlWqwN3dPd++tra24v9Vvy23+ovq999/h56eHg4cOKA2eTc4OLjQ67C1tcXt27dztL/blv1/0ebm5gXuc2EvAvq2gQMHYuLEiYiNjcXWrVvh5eUl/h99fkxMTJCUlJSjPSYmJsdoS3G+b4VV2PcxPyqVCnfv3hVHGADg5s2bACBONra1tcWhQ4fw4sULtVGi7MPWb1/4sDifQ0GyvwcKhaLA70FB65k0aRImTZqEW7duwdnZGYsWLcLmzZtz7f/8+XMcPnwY3377LQICAsT23H62CsvW1hYqlQp37txRGxUq7M9m7dq1ceXKFXTs2LHY77WOjg66deuGbt26QaVSYfTo0Vi7di1mzpxZ6HBRGp9zborye6+y4iEzEv/P4e3/s8rIyMDq1auLvc53T5XV0dGBo6MjBEHI9RTsbNWrV4ezszM2bNig9ofw33//xcGDB9GlS5di11RY5ubmaNeuHdauXYvY2Ngcy588eSL+u0uXLjhz5gzOnTuntnzLli3vXYeWlhZkMpnapQ+io6OxZ8+eQq/Dw8MDp0+fVrulQmJiYo76PDw8oFAoMHfu3Fw/n7f3OTuQ5hZU8vL5559DJpNh3LhxuHv3bqGvtVS7dm2cOXMGGRkZYtvevXvx4MEDtX7F/b4VVmHfx4KsXLlS/LcgCFi5ciWqVKmCjh07AnjzfcrKylLrBwBLliyBTCYTz+IE3nwORfkMCsPFxQW1a9fGwoULkZqammP529+D3Lx8+RKvXr1Sa6tduzaMjIzEU9Vzk9vvIABYunRpISvPKfu9Wr58ebHW2adPHzx69Ag///xzjmX//fdfjkP373r3OymXy9GgQQMAyPe9eFdpfM65KcrvvczMTNy4cSPXfhUZR4gILVq0gImJCXx9fTF27FjIZDJs2rTpvYaeO3XqBEtLS7Rs2RIWFha4fv06Vq5cCS8vrxzzI961YMECdO7cGW5ubhg6dKh42r1SqSyzqyOvWrUKrVq1gpOTE4YPH45atWohPj4ep0+fxsOHD3HlyhUAwNSpU7Fp0yZ4enpi3Lhx4mn3tra2uHr16nvV4OXlhcWLF8PT0xP9+/dHQkICVq1ahTp16hR63VOnTsXmzZvxySefYMyYMeLp4jY2NkhMTBT/71OhUGDNmjUYMGAAGjdujH79+sHMzAz379/Hvn370LJlS/GPtIuLC4A3k1U9PDygpaWFfv365VuHmZkZPD09sWPHDhgbGxf61Odhw4Zh586d8PT0RJ8+fXDnzh1s3rw5x7yQ9/m+FUZh38f86OnpITQ0FL6+vmjWrBlCQkKwb98+fPXVV+LVyLt164b27dvj66+/RnR0NBo2bIiDBw/ijz/+wPjx49X228XFBYcOHcLixYthZWUFe3t7caJsccnlcvzyyy/o3Lkz6tevj8GDB6NGjRp49OgRjhw5AoVCgb/++ivP19+8eRMdO3ZEnz594OjoCG1tbezevRvx8fH5fkcUCoU43ygzMxM1atTAwYMH32uU2tnZGZ9//jlWr16N5ORktGjRAocPHy70qN6AAQPw22+/4csvv8SRI0fQsmVLZGVl4caNG/jtt99w4MABuLq65vn6YcOGITExER06dEDNmjURExODFStWwNnZuUh3DCiNzzkvhf299+jRIzg4OMDX1xfr168vlVo0QgNntlEZyOu0+7xOTT516pTQvHlzQV9fX7CyshKmTp0qnq759mmZea3D19dX7ZTxtWvXCm3atBGqVasm6OrqCrVr1xamTJkiJCcni33yOu1eEATh0KFDQsuWLQV9fX1BoVAI3bp1E65du6bWJ/v02+xTlt9ma2sreHl55WgHIPj5+am13bt3L9dThe/cuSMMHDhQsLS0FKpUqSLUqFFD6Nq1q7Bz5061flevXhXatm0r6OnpCTVq1BC+++474ddffy2R0+5//fVXoW7duoKurq7w0UcfCcHBweJ+F7Rf2S5fviy0bt1a0NXVFWrWrCkEBgYKy5cvFwAIcXFxan2PHDkieHh4CEqlUtDT0xNq164tDBo0SLhw4YLY5/Xr18KYMWMEMzMzQSaTFfoU/N9++00AIIwYMaJQ/bMtWrRIqFGjhqCrqyu0bNlSuHDhQo7T7gvzfcvrNOXcvifvrl8QivY+vsvX11cwNDQU7ty5I3Tq1EkwMDAQLCwshFmzZglZWVlqfV+8eCFMmDBBsLKyEqpUqSLUrVtXWLBggdpp8IIgCDdu3BDatGkj6OvrCwDEU7Pz+rnIbf/zcvnyZaFXr17i+2lrayv06dNHOHz4sNgnt5/fp0+fCn5+fsJHH30kGBoaCkqlUmjWrJnw22+/FbjNhw8fCj179hSMjY0FpVIpfPbZZ8Ljx49znCJflP3777//hLFjxwrVqlUTDA0NhW7dugkPHjwo1Gn3gvDmMhfz5s0T6tevL+jq6gomJiaCi4uL8O2336p9t3L7vbhz506hU6dOgrm5uaCjoyPY2NgII0eOFGJjYwvc7tvy+pyL8n0u6d972a/N7XIAFZlMECr4zEEiKrLx48dj7dq1SE1NLbPL///xxx/o0aMHjh8/rnaae0WmifeRiEoHAxFRJffff/+pnYH27NkzfPjhh2jcuHGZXiOla9euuH79Om7fvl1mE0VLUnl5H4modHAOEVEl5+bmhnbt2sHBwQHx8fH49ddfkZKSgpkzZ5bJ9rdt24arV69i3759WLZsWYUMQ4Dm30eqXAq6NpO+vr54rSAqGxwhIqrkvvrqK+zcuRMPHz6ETCZD48aNMWvWrDI7tVYmk6Fq1aro27cvgoKC3uuaRpqk6feRKpeC/seg0k1YrgAYiIiIiMpY9u1a8mJlZZXjVjxUuhiIiIiISPJ4YUYiIiKSvIp5ML+MqVQqPH78GEZGRhV2QigREZHUCIKAFy9ewMrKCnJ5/mNADESF8Pjx4wJv5EdERETl04MHD1CzZs18+zAQFUL2pf8fPHgAhUKh4WqIiIioMFJSUmBtbV2oW/gwEBXC2/d7YiAiIiKqWAoz3YWTqomIiEjyGIiIiIhI8hiIiIiISPI4h4iIqBLKyspCZmampssgKnU6OjoFnlJfGAxERESViCAIiIuLQ1JSkqZLISoTcrkc9vb20NHRea/1MBAREVUi2WHI3NwcBgYGvJgsVWrZF06OjY2FjY3Ne33fGYiIiCqJrKwsMQxVq1ZN0+UQlQkzMzM8fvwYr1+/RpUqVYq9Hk6qJiKqJLLnDBkYGGi4EqKyk32oLCsr673Ww0BERFTJ8DAZSUlJfd8ZiIiIiEjyGIiIiKhCOHXqFJycnFClShX06NFD0+VQJcNJ1UREEmA3fV+Zbi/6R68SX+fEiRPh7OyMkJAQVK1atUTXHRsbi0mTJuHChQu4ffs2xo4di6VLlxb69enp6WjcuDFatmyJn376SW3Z1KlTsWPHDly9erVQNxktjKNHj6J9+/a5LouNjYWlpWWJbKcg0dHRsLe3x+XLl+Hs7Fwm2ywtHCEiIqIK4c6dO+jQoQNq1qwJY2PjEl13eno6zMzM8M0336Bhw4ZFfr2uri42btyI9evX48CBA2L7mTNnsGTJEqxfv77EwtDboqKiEBsbq/YwNzcv8e1IAQMRERFpnEqlQmBgIOzt7aGvr4+GDRti586dAN6MQshkMjx79gxDhgyBTCbD+vXr8fz5c/j4+MDMzAz6+vqoW7cugoODi7V9Ozs7LFu2DAMHDoRSqSzWOlxcXPD1119j6NChSEpKwqtXrzB48GCMGTMGbdu2xcmTJ9G6dWvo6+vD2toaY8eORVpamvj61atXo27dutDT04OFhQV69+5d4DbNzc1haWmp9pDL5Th48CD09PRyXKBz3Lhx6NChg/i8oJrs7Owwd+5cDBkyBEZGRrCxsVEbAbO3twcANGrUCDKZDO3atSvWe1ceMBAREZHGBQYGYuPGjQgKCkJkZCQmTJiAL774AseOHYO1tTViY2OhUCiwdOlSxMbGom/fvpg5cyauXbuGkJAQXL9+HWvWrMEHH3wgrrN+/fqoWrVqno/OnTuX+H58/fXXsLS0xNixY/HNN99AJpNh7ty5uHPnDjw9PeHt7Y2rV69i+/btOHnyJPz9/QEAFy5cwNixYzFnzhxERUUhNDQUbdq0KXYdHTt2hLGxMX7//XexLSsrC9u3b4ePjw8AFFhTtkWLFsHV1RWXL1/G6NGjMWrUKERFRQEAzp07BwA4dOgQYmNjsWvXrmLXrGmcQ0REVAS5zcUpjfkyUpKeno65c+fi0KFDcHNzAwDUqlULJ0+exNq1a9G2bVtYWlpCJpNBqVSK82Pu37+PRo0awdXVFcCb0Yy37d+/P9/7uenr65f4vmhra2Pjxo1wcXGBSqXCqVOnoKenh8DAQPj4+GD8+PEAgLp162L58uVo27Yt1qxZg/v378PQ0BBdu3aFkZERbG1t0ahRowK3V7NmTbXntra2iIyMhJaWFvr164etW7di6NChAIDDhw8jKSkJ3t7eAFBgTXp6egCALl26YPTo0QCAadOmYcmSJThy5Ajq1asHMzMzAEC1atXKbN5SaWEgIiIijbp9+zZevnyJTz75RK09IyMj31AwatQoeHt749KlS+jUqRN69OiBFi1aiMttbW1Lreb8ODo6wtvbG0lJSWJYu3LlCq5evYotW7aI/QRBgEqlwr179/DJJ5/A1tYWtWrVgqenJzw9PdGzZ88CL7J54sQJtblJb1+p2cfHB82bN8fjx49hZWWFLVu2wMvLS5x/VVBNDg4OAIAGDRqIy2UyGSwtLZGQkFD8N6icYiAiIiKNSk1NBQDs27cPNWrUUFumq6ub5+s6d+6MmJgY7N+/H2FhYejYsSP8/PywcOFCAG8OmcXExOT5+tatWyMkJKQE9iAnbW1taGv/35/Y1NRUjBw5EmPHjs3R18bGBjo6Orh06RKOHj2KgwcPIiAgALNnz8b58+fznUBub2+f5/ImTZqgdu3a2LZtG0aNGoXdu3dj/fr1ha4p27u3w5DJZFCpVHnWVFFpdA7R8ePH0a1bN1hZWUEmk2HPnj159v3yyy8hk8lynAaZmJgIHx8fKBQKGBsbY+jQoeIPV7arV6+idevW0NPTg7W1NebPn18Ke0NERMXh6OgIXV1d3L9/H3Xq1FF7WFtb5/taMzMz+Pr6YvPmzVi6dKnahN/9+/cjIiIiz8cvv/xS2rsmaty4Ma5du5Zj/+rUqSPeekJbWxvu7u6YP38+rl69iujoaISHh7/Xdn18fLBlyxb89ddfkMvl8PL6v8O7hampICV124zyQKMjRGlpaWjYsCGGDBmCXr165dlv9+7dOHPmDKysrHIs8/HxQWxsLMLCwpCZmYnBgwdjxIgR2Lp1KwAgJSUFnTp1gru7O4KCgvDPP/9gyJAhMDY2xogRI0pt34iIqHCMjIwwefJkTJgwASqVCq1atUJycjJOnToFhUIBX1/fXF8XEBAAFxcX1K9fH+np6di7d694mAco+iGziIgIAG9GTp48eYKIiAjo6OjA0dGx2PuWbdq0aWjevDn8/f0xbNgwGBoa4tq1awgLC8PKlSuxd+9e3L17F23atIGJiQn2798PlUqFevXq5bvehIQEvHr1Sq2tWrVq4qiOj48PZs+ejR9++AG9e/dWG3ErqKbCMDc3h76+PkJDQ1GzZk3o6ekV+yw9TdNoIOrcuXOBs/wfPXqEMWPG4MCBA2rJFgCuX7+O0NBQnD9/XjxOu2LFCnTp0gULFy4Uj5lmZGRg3bp10NHRQf369REREYHFixczEBERlRPfffcdzMzMEBgYiLt378LY2BiNGzfGV199ledrdHR0MGPGDERHR0NfXx+tW7fGtm3bil3D2/OVLl68iK1bt8LW1hbR0dEA/u9iiPfu3csxgbsgDRo0wLFjx/D111+jdevWEAQBtWvXRt++fQEAxsbG2LVrF2bPno1Xr16hbt26+N///of69evnu97cAtPp06fRvHlzAECdOnXQtGlTnDt3LscRloJqKgxtbW0sX74cc+bMQUBAAFq3bo2jR48W+vXliUwQBEHTRQBvjknu3r1b7XLsKpUK7u7u6N69O8aNGwc7OzuMHz9enBG/bt06TJo0Cc+fPxdf8/r1a+jp6WHHjh3o2bMnBg4ciJSUFLXDcUeOHEGHDh2QmJgIExOTHLWkp6cjPT1dfJ6SkgJra2skJydDoVCU+L4TUcVRns8ye/XqFe7duwd7e3vxDCEqOcHBwZg7dy6uXbuWY14NaU5+3/uUlBQolcpC/f0u19chmjdvHrS1tXOd8AUAcXFxOa7Iqa2tDVNTU8TFxYl9LCws1PpkP8/u867AwEAolUrxUdAxbCIiqvz279+PuXPnMgxVUuX2LLOLFy9i2bJluHTpEmQyWZlue8aMGZg4caL4PHuEiIiIpGvHjh2aLoFKUbkdITpx4gQSEhJgY2Mjnr4YExODSZMmicduc7sWwuvXr5GYmCheIMrS0hLx8fFqfbKf53URKV1dXSgUCrUHERERVV7lNhANGDAAV69eVTtF0srKClOmTBFvnOfm5oakpCRcvHhRfF14eDhUKhWaNWsm9jl+/Lja1UrDwsJQr169XOcPERERkfRo9JBZamoqbt++LT6/d+8eIiIiYGpqChsbG1SrVk2tf5UqVWBpaSnOqndwcICnpyeGDx+OoKAgZGZmwt/fH/369RNP0e/fvz++/fZbDB06FNOmTcO///6LZcuWYcmSJWW3o0RERFSuaTQQXbhwAe3btxefZ8/b8fX1VbuaZn62bNkCf39/dOzYEXK5HN7e3li+fLm4XKlU4uDBg/Dz84OLiws++OADBAQE8JR7IiIiEmk0ELVr1w5FOes/+1oQbzM1NRUvwpiXBg0a4MSJE0Utj4iIiCSi3M4hIiIiIiorDEREREQkeQxERERUIZw6dQpOTk6oUqWK2l0NpCw6OhoymUy8DxsVX7m9MCMREZWg2WV8w83ZySW+yokTJ8LZ2RkhISGoWrVqia//6NGjmDhxIiIjI2FtbY1vvvkGgwYNKtI67OzsEBMTg//973/o16+f2rL69evj2rVrCA4OLvJ6y1r2fdtyExsbm+d1/EpadHQ07O3tcfnyZTg7O5fqtjhCREREFcKdO3fQoUMH1KxZE8bGxiW67nv37sHLywvt27dHREQExo8fj2HDhonXvSsKa2trBAcHq7WdOXMGcXFxMDQ0LKmSy0RUVBRiY2PVHu/eMquyYCAiIiKNU6lUCAwMhL29PfT19dGwYUPs3LkTwP8dFnr27BmGDBkCmUyG9evX4/nz5/Dx8YGZmRn09fVRt27dHEGksIKCgmBvb49FixbBwcEB/v7+6N27d7GuWefj44Njx47hwYMHYtu6devg4+MDbW31AzOLFy+Gk5MTDA0NYW1tjdGjRyM1NRUAkJaWBoVCIb4P2fbs2QNDQ0O8ePFCbLtx4wZatGgBPT09fPzxxzh27Jjaa/7991907twZVatWhYWFBQYMGICnT58WuC/m5uawtLRUe8jlchw8eBB6enpISkpS6z9u3Dh06NBBfH7y5Em0bt0a+vr6sLa2xtixY5GWliYut7Ozw9y5czFkyBAYGRnBxsYGP/30k7jc3t4eANCoUSPIZDK0a9euwJqLi4GIiIg0LjAwEBs3bkRQUBAiIyMxYcIEfPHFFzh27Bisra0RGxsLhUKBpUuXIjY2Fn379sXMmTNx7do1hISE4Pr161izZg0++OADcZ3169dH1apV83x07txZ7Hv69Gm4u7ur1eTh4YHTp08XeV8sLCzg4eGBDRs2AABevnyJ7du3Y8iQITn6yuVyLF++HJGRkdiwYQPCw8MxdepUAIChoSH69euXI+QFBwejd+/eMDIyEtumTJmCSZMm4fLly3Bzc0O3bt3w7NkzAEBSUhI6dOiARo0a4cKFCwgNDUV8fDz69OlT5H3L1rFjRxgbG+P3338X27KysrB9+3b4+PgAeDOi5+npCW9vb1y9ehXbt2/HyZMn4e/vr7auRYsWwdXVFZcvX8bo0aMxatQoREVFAQDOnTsHADh06BBiY2Oxa9euYtdcEM4hIiIijUpPT8fcuXNx6NAhuLm5AQBq1aqFkydPYu3atWjbti0sLS0hk8mgVCrF+Sv3799Ho0aN4OrqCgDifS6z7d+/X+22Te/S19cX/x0XFwcLCwu15RYWFkhJScF///2n1rcwhgwZgkmTJuHrr7/Gzp07Ubt27VznwIwfP178t52dHb7//nt8+eWXWL16NQBg2LBhaNGiBWJjY1G9enUkJCRg//79OHTokNp6/P394e3tDQBYs2YNQkND8euvv2Lq1KlYuXIlGjVqhLlz54r9161bB2tra9y8eRMffvhhnvtRs2ZNtee2traIjIyElpYW+vXrh61bt2Lo0KEAgMOHDyMpKUmsIzAwED4+PuI+1q1bF8uXL0fbtm2xZs0a6OnpAQC6dOmC0aNHAwCmTZuGJUuW4MiRI6hXrx7MzMwAANWqVSv1eUsMREREpFG3b9/Gy5cv8cknn6i1Z2RkoFGjRnm+btSoUfD29salS5fQqVMn9OjRAy1atBCX29rallrNBfHy8sLIkSNx/PhxrFu3LtfRIeDNyEdgYCBu3LiBlJQUvH79Gq9evcLLly9hYGCApk2bon79+tiwYQOmT5+OzZs3w9bWFm3atFFbT3aQBABtbW24urri+vXrAIArV67gyJEjuU5Ev3PnTr6B6MSJE2ojUVWqVBH/7ePjg+bNm+Px48ewsrLCli1b4OXlJc7vunLlCq5evYotW7aIrxEEASqVCvfu3YODgwOANxdPziaTyXK9cXtZYCAiIiKNyp4zs2/fPtSoUUNtma6ubp6v69y5M2JiYrB//36EhYWhY8eO8PPzw8KFCwG8OWQWExOT5+tbt26NkJAQAIClpSXi4+PVlsfHx0OhUBR5dAh4E0oGDBiAWbNm4ezZs9i9e3eOPtHR0ejatStGjRqFH374Aaampjh58iSGDh2KjIwMGBgYAHgzSrRq1SpMnz4dwcHBGDx4MGQyWaFrSU1NRbdu3TBv3rwcy6pXr57va+3t7fOcwN6kSRPUrl0b27Ztw6hRo7B79261226lpqZi5MiRGDt2bI7X2tjYiP9+O2QBb0KRSqXKt67SwEBEREQa5ejoCF1dXdy/fx9t27Yt0mvNzMzg6+sLX19ftG7dGlOmTBEDUVEOmbm5uWH//v1qy8PCwtRGXopqyJAhWLhwIfr27QsTE5Mcyy9evAiVSoVFixZBLn8zpfe3337L0e+LL77A1KlTsXz5cly7dg2+vr45+pw5c0YcNXr9+jUuXrwoztVp3Lgxfv/9d9jZ2eWY1P2+fHx8sGXLFtSsWRNyuRxeXl7issaNG+PatWuoU6dOsdevo6MD4M38pNLGQERERBplZGSEyZMnY8KECVCpVGjVqhWSk5Nx6tQpKBSKXAMAAAQEBMDFxQX169dHeno69u7dKx6GAYp2yOzLL7/EypUrMXXqVAwZMgTh4eH47bffsG/fvmLvl4ODA54+fSqO9LyrTp06yMzMxIoVK9CtWzecOnUKQUFBOfqZmJigV69emDJlCjp16pRjXg8ArFq1CnXr1oWDgwOWLFmC58+fi4fp/Pz88PPPP+Pzzz/H1KlTYWpqitu3b2Pbtm345ZdfoKWllec+JCQk4NWrV2pt1apVE0d1fHx8MHv2bPzwww/o3bu32ojetGnT0Lx5c/j7+2PYsGEwNDTEtWvXEBYWhpUrVxb8BuLNWW76+voIDQ1FzZo1oaenB6WydK6pxbPMiIhI47777jvMnDkTgYGBcHBwgKenJ/bt2yeedp0bHR0dzJgxAw0aNECbNm2gpaWFbdu2FWv79vb22LdvH8LCwtCwYUMsWrQIv/zyCzw8PMQ+69evL9KhKuBNeMjrkFvDhg2xePFizJs3Dx9//DG2bNmCwMDAXPtmH0bLay7Sjz/+iB9//BENGzbEyZMn8eeff4pn3FlZWeHUqVPIyspCp06d4OTkhPHjx8PY2FgcmcpLvXr1UL16dbXHxYsXxeV16tRB06ZNcfXqVfHssmwNGjTAsWPHcPPmTbRu3RqNGjVCQEAArKys8t3m27S1tbF8+XKsXbsWVlZW6N69e6FfW1QyoSi3m5eolJQUKJVKJCcnQ6FQaLocItIgu+k5Rwyif/TKpWfZe/XqFe7duwd7e3vxDB4qObNmzcKxY8dw9OjRMt/2pk2bMGHCBDx+/Fg8jERv5Pe9L8rfbx4yIyIiKoSQkJBCH+opKS9fvkRsbCx+/PFHjBw5kmGoFPGQGRERUSGcO3cOTZs2LdNtzp8/Hx999BEsLS0xY8aMMt221DAQERERlVOzZ89GZmYmDh8+XCo3tKX/w0BEREREksdARERUyfBcGZKSkvq+c1I1EdF7Ki9nnmVfG+bly5fFuroyUUWUkZEBAPleT6kwGIiIiCoJLS0tGBsbi/eBMjAwKPJ1c4gqEpVKhSdPnsDAwOC9r8LNQEREVIlk3xFcEzfHJNIEuVwOGxub9w7/DERERJWITCZD9erVYW5unu99vIgqCx0dnQKvuF0YDERERJWQlpbWe8+pIJISnmVGREREksdARERERJLHQERERESSx0BEREREksdARERERJLHQERERESSx0BEREREksdARERERJLHQERERESSx0BEREREksdARERERJLHQERERESSx0BEREREksdARERERJKn0UB0/PhxdOvWDVZWVpDJZNizZ4+4LDMzE9OmTYOTkxMMDQ1hZWWFgQMH4vHjx2rrSExMhI+PDxQKBYyNjTF06FCkpqaq9bl69Spat24NPT09WFtbY/78+WWxe0RERFRBaDQQpaWloWHDhli1alWOZS9fvsSlS5cwc+ZMXLp0Cbt27UJUVBQ+/fRTtX4+Pj6IjIxEWFgY9u7di+PHj2PEiBHi8pSUFHTq1Am2tra4ePEiFixYgNmzZ+Onn34q9f0jIiKiikEmCIKg6SIAQCaTYffu3ejRo0eefc6fP4+mTZsiJiYGNjY2uH79OhwdHXH+/Hm4uroCAEJDQ9GlSxc8fPgQVlZWWLNmDb7++mvExcVBR0cHADB9+nTs2bMHN27cKFRtKSkpUCqVSE5OhkKheO99JaKKy276vkL1i/7Rq5QrIaKCFOXvd4WaQ5ScnAyZTAZjY2MAwOnTp2FsbCyGIQBwd3eHXC7H2bNnxT5t2rQRwxAAeHh4ICoqCs+fP891O+np6UhJSVF7EBERUeVVYQLRq1evMG3aNHz++ediyouLi4O5ublaP21tbZiamiIuLk7sY2FhodYn+3l2n3cFBgZCqVSKD2tr65LeHSIiIipHKkQgyszMRJ8+fSAIAtasWVPq25sxYwaSk5PFx4MHD0p9m0RERKQ52pouoCDZYSgmJgbh4eFqxwAtLS2RkJCg1v/169dITEyEpaWl2Cc+Pl6tT/bz7D7v0tXVha6ubknuBhEREZVj5XqEKDsM3bp1C4cOHUK1atXUlru5uSEpKQkXL14U28LDw6FSqdCsWTOxz/Hjx5GZmSn2CQsLQ7169WBiYlI2O0JERETlmkYDUWpqKiIiIhAREQEAuHfvHiIiInD//n1kZmaid+/euHDhArZs2YKsrCzExcUhLi4OGRkZAAAHBwd4enpi+PDhOHfuHE6dOgV/f3/069cPVlZWAID+/ftDR0cHQ4cORWRkJLZv345ly5Zh4sSJmtptIiIiKmc0etr90aNH0b59+xztvr6+mD17Nuzt7XN93ZEjR9CuXTsAby7M6O/vj7/++gtyuRze3t5Yvnw5qlatKva/evUq/Pz8cP78eXzwwQcYM2YMpk2bVug6edo9EWUr7Gn37+Jp+ERlryh/v8vNdYjKMwYiIsrGQERUcVTa6xARERERlQYGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjxtTRdARCQFdtP35WiL/tFLA5UQUW44QkRERESSx0BEREREksdARERERJLHQERERESSx0BEREREksdARERERJLHQERERESSx0BEREREksdARERERJLHQERERESSx0BEREREksdARERERJKn0UB0/PhxdOvWDVZWVpDJZNizZ4/ackEQEBAQgOrVq0NfXx/u7u64deuWWp/ExET4+PhAoVDA2NgYQ4cORWpqqlqfq1evonXr1tDT04O1tTXmz59f2rtGREREFYhGA1FaWhoaNmyIVatW5bp8/vz5WL58OYKCgnD27FkYGhrCw8MDr169Evv4+PggMjISYWFh2Lt3L44fP44RI0aIy1NSUtCpUyfY2tri4sWLWLBgAWbPno2ffvqp1PePiIiIKgaZIAiCposAAJlMht27d6NHjx4A3owOWVlZYdKkSZg8eTIAIDk5GRYWFli/fj369euH69evw9HREefPn4erqysAIDQ0FF26dMHDhw9hZWWFNWvW4Ouvv0ZcXBx0dHQAANOnT8eePXtw48aNQtWWkpICpVKJ5ORkKBSKkt95Iqow7KbvK7F1Rf/oVWLrIqKcivL3u9zOIbp37x7i4uLg7u4utimVSjRr1gynT58GAJw+fRrGxsZiGAIAd3d3yOVynD17VuzTpk0bMQwBgIeHB6KiovD8+fNct52eno6UlBS1BxEREVVe5TYQxcXFAQAsLCzU2i0sLMRlcXFxMDc3V1uura0NU1NTtT65rePtbbwrMDAQSqVSfFhbW7//DhEREVG5VW4DkSbNmDEDycnJ4uPBgweaLomIiIhKUbkNRJaWlgCA+Ph4tfb4+HhxmaWlJRISEtSWv379GomJiWp9clvH29t4l66uLhQKhdqDiIiIKq9yG4js7e1haWmJw4cPi20pKSk4e/Ys3NzcAABubm5ISkrCxYsXxT7h4eFQqVRo1qyZ2Of48ePIzMwU+4SFhaFevXowMTEpo70hIiKi8kyjgSg1NRURERGIiIgA8GYidUREBO7fvw+ZTIbx48fj+++/x59//ol//vkHAwcOhJWVlXgmmoODAzw9PTF8+HCcO3cOp06dgr+/P/r16wcrKysAQP/+/aGjo4OhQ4ciMjIS27dvx7JlyzBx4kQN7TURERGVN9qa3PiFCxfQvn178Xl2SPH19cX69esxdepUpKWlYcSIEUhKSkKrVq0QGhoKPT098TVbtmyBv78/OnbsCLlcDm9vbyxfvlxcrlQqcfDgQfj5+cHFxQUffPABAgIC1K5VRERERNJWbq5DVJ7xOkRElI3XISKqOIry91ujI0RERFKWW7hiSCLSjHI7qZqIiIiorDAQERERkeQxEBEREZHkMRARERGR5DEQERERkeQxEBEREZHkMRARERGR5DEQERERkeQxEBEREZHkMRARERGR5DEQERERkeQxEBEREZHkMRARERGR5DEQERERkeQxEBEREZHkMRARERGR5DEQERERkeQxEBEREZHkMRARERGR5DEQERERkeQxEBEREZHkMRARERGR5GlrugAiIvo/dtP35WiL/tFLA5UQSQtHiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8hiIiIiISPIYiIiIiEjyGIiIiIhI8ooViO7evVvSdRARERFpTLECUZ06ddC+fXts3rwZr169KumaiIiIiMpUsQLRpUuX0KBBA0ycOBGWlpYYOXIkzp07V9K1EREREZWJYgUiZ2dnLFu2DI8fP8a6desQGxuLVq1a4eOPP8bixYvx5MmTkq6TiIiIqNS816RqbW1t9OrVCzt27MC8efNw+/ZtTJ48GdbW1hg4cCBiY2NLqk4iIiKiUvNegejChQsYPXo0qlevjsWLF2Py5Mm4c+cOwsLC8PjxY3Tv3r2k6iQiIiIqNcUKRIsXL4aTkxNatGiBx48fY+PGjYiJicH3338Pe3t7tG7dGuvXr8elS5feq7isrCzMnDkT9vb20NfXR+3atfHdd99BEASxjyAICAgIQPXq1aGvrw93d3fcunVLbT2JiYnw8fGBQqGAsbExhg4ditTU1PeqjYiIiCqPYgWiNWvWoH///oiJicGePXvQtWtXyOXqqzI3N8evv/76XsXNmzcPa9aswcqVK3H9+nXMmzcP8+fPx4oVK8Q+8+fPx/LlyxEUFISzZ8/C0NAQHh4eame/+fj4IDIyEmFhYdi7dy+OHz+OESNGvFdtREREVHnIhLeHWwopOjoaNjY2OUKQIAh48OABbGxsSqS4rl27wsLCQi1YeXt7Q19fH5s3b4YgCLCyssKkSZMwefJkAEBycjIsLCywfv169OvXD9evX4ejoyPOnz8PV1dXAEBoaCi6dOmChw8fwsrKqsA6UlJSoFQqkZycDIVCUSL7RkQVk930fWW+zegfvcp8m0SVQVH+fhdrhKh27dp4+vRpjvbExETY29sXZ5W5atGiBQ4fPoybN28CAK5cuYKTJ0+ic+fOAIB79+4hLi4O7u7u4muUSiWaNWuG06dPAwBOnz4NY2NjMQwBgLu7O+RyOc6ePVtitRIREVHFpV2cF+U1qJSamgo9Pb33Kuht06dPR0pKCj766CNoaWkhKysLP/zwA3x8fAAAcXFxAAALCwu111lYWIjL4uLiYG5urrZcW1sbpqamYp93paenIz09XXyekpJSYvtERFRUuY1KcdSIqGQVKRBNnDgRACCTyRAQEAADAwNxWVZWFs6ePQtnZ+cSK+63337Dli1bsHXrVtSvXx8REREYP348rKys4OvrW2LbeVdgYCC+/fbbUls/ERERlS9FCkSXL18G8GaE6J9//oGOjo64TEdHBw0bNhTn8pSEKVOmYPr06ejXrx8AwMnJCTExMQgMDISvry8sLS0BAPHx8ahevbr4uvj4eDGYWVpaIiEhQW29r1+/RmJiovj6d82YMUMMf8CbESJra+sS2y8iIiIqX4oUiI4cOQIAGDx4MJYtW1bqE4xfvnyZY+K2lpYWVCoVAMDe3h6WlpY4fPiwGIBSUlJw9uxZjBo1CgDg5uaGpKQkXLx4ES4uLgCA8PBwqFQqNGvWLNft6urqQldXt5T2ioiIiMqbYs0hCg4OLuk6ctWtWzf88MMPsLGxQf369XH58mUsXrwYQ4YMAfDm0N348ePx/fffo27durC3t8fMmTNhZWWFHj16AAAcHBzg6emJ4cOHIygoCJmZmfD390e/fv0KdYYZERERVX6FDkS9evXC+vXroVAo0KtXr3z77tq1670LA4AVK1Zg5syZGD16NBISEmBlZYWRI0ciICBA7DN16lSkpaVhxIgRSEpKQqtWrRAaGqo2uXvLli3w9/dHx44dIZfL4e3tjeXLl5dIjURERFTxFToQKZVKyGQy8d9lwcjICEuXLsXSpUvz7COTyTBnzhzMmTMnzz6mpqbYunVrKVRIRERElUGhA9Hbh8nK6pAZERERUVko1oUZ//vvP7x8+VJ8HhMTg6VLl+LgwYMlVhgRERFRWSlWIOrevTs2btwIAEhKSkLTpk2xaNEidO/eHWvWrCnRAomIiIhKW7EC0aVLl9C6dWsAwM6dO2FpaYmYmBhs3LiRk5WJiIiowilWIHr58iWMjIwAAAcPHkSvXr0gl8vRvHlzxMTElGiBRERERKWtWIGoTp062LNnDx48eIADBw6gU6dOAICEhATeDZ6IiIgqnGIFooCAAEyePBl2dnZo1qwZ3NzcALwZLWrUqFGJFkhERERU2op1perevXujVatWiI2NRcOGDcX2jh07omfPniVWHBEREVFZKFYgAt7cNPXdm6M2bdr0vQsiIiIiKmvFCkRpaWn48ccfcfjwYSQkJIg3W8129+7dEimOiIiIqCwUKxANGzYMx44dw4ABA1C9enXxlh5EREREFVGxAlFISAj27duHli1blnQ9RERUCHbT96k9j/7RS0OVEFUOxTrLzMTEBKampiVdCxEREZFGFCsQfffddwgICFC7nxkRERFRRVWsQ2aLFi3CnTt3YGFhATs7O1SpUkVt+aVLl0qkOCIiIqKyUKxA1KNHjxIug4iIiEhzihWIZs2aVdJ1EBEREWlMseYQAUBSUhJ++eUXzJgxA4mJiQDeHCp79OhRiRVHREREVBaKNUJ09epVuLu7Q6lUIjo6GsOHD4epqSl27dqF+/fvY+PGjSVdJxEREVGpKdYI0cSJEzFo0CDcunULenp6YnuXLl1w/PjxEiuOiIiIqCwUKxCdP38eI0eOzNFeo0YNxMXFvXdRRERERGWpWIFIV1cXKSkpOdpv3rwJMzOz9y6KiIiIqCwVKxB9+umnmDNnDjIzMwEAMpkM9+/fx7Rp0+Dt7V2iBRIRERGVtmIFokWLFiE1NRVmZmb477//0LZtW9SpUwdGRkb44YcfSrpGIiIiolJVrLPMlEolwsLCcOrUKVy5cgWpqalo3Lgx3N3dS7o+IiIiolJX5ECkUqmwfv167Nq1C9HR0ZDJZLC3t4elpSUEQYBMJiuNOomIiIhKTZEOmQmCgE8//RTDhg3Do0eP4OTkhPr16yMmJgaDBg1Cz549S6tOIiIiolJTpBGi9evX4/jx4zh8+DDat2+vtiw8PBw9evTAxo0bMXDgwBItkoiIiKg0yQRBEArbuVOnTujQoQOmT5+e6/K5c+fi2LFjOHDgQIkVWB6kpKRAqVQiOTkZCoVC0+UQkQbZTd+n6RIKLfpHL02XQKRRRfn7XaRDZlevXoWnp2eeyzt37owrV64UZZVEREREGlekQJSYmAgLC4s8l1tYWOD58+fvXRQRERFRWSpSIMrKyoK2dt7TjrS0tPD69ev3LoqIiIioLBVpUrUgCBg0aBB0dXVzXZ6enl4iRRERERGVpSIFIl9f3wL78AwzIiIiqmiKFIiCg4NLqw4iIiIijSnWvcyIiIiIKhMGIiIiIpI8BiIiIiKSPAYiIiIikrxyH4gePXqEL774AtWqVYO+vj6cnJxw4cIFcbkgCAgICED16tWhr68Pd3d33Lp1S20diYmJ8PHxgUKhgLGxMYYOHYrU1NSy3hUiIiIqp8p1IHr+/DlatmyJKlWqICQkBNeuXcOiRYtgYmIi9pk/fz6WL1+OoKAgnD17FoaGhvDw8MCrV6/EPj4+PoiMjERYWBj27t2L48ePY8SIEZrYJSIiIiqHinRz17I2ffp0nDp1CidOnMh1uSAIsLKywqRJkzB58mQAQHJyMiwsLLB+/Xr069cP169fh6OjI86fPw9XV1cAQGhoKLp06YKHDx/CysqqwDp4c1ciysabuxJVHKV2c9ey9ueff8LV1RWfffYZzM3N0ahRI/z888/i8nv37iEuLg7u7u5im1KpRLNmzXD69GkAwOnTp2FsbCyGIQBwd3eHXC7H2bNnc91ueno6UlJS1B5ERERUeZXrQHT37l2sWbMGdevWxYEDBzBq1CiMHTsWGzZsAADExcUBQI4bzlpYWIjL4uLiYG5urrZcW1sbpqamYp93BQYGQqlUig9ra+uS3jUiIiIqR4p0peqyplKp4Orqirlz5wIAGjVqhH///RdBQUGFuo1Icc2YMQMTJ04Un6ekpDAUEVGFk9vhPR5GI8pduR4hql69OhwdHdXaHBwccP/+fQCApaUlACA+Pl6tT3x8vLjM0tISCQkJastfv36NxMREsc+7dHV1oVAo1B5ERERUeZXrQNSyZUtERUWptd28eRO2trYAAHt7e1haWuLw4cPi8pSUFJw9exZubm4AADc3NyQlJeHixYtin/DwcKhUKjRr1qwM9oKIiIjKu3J9yGzChAlo0aIF5s6diz59+uDcuXP46aef8NNPPwEAZDIZxo8fj++//x5169aFvb09Zs6cCSsrK/To0QPAmxElT09PDB8+HEFBQcjMzIS/vz/69etXqDPMiIiIqPIr14GoSZMm2L17N2bMmIE5c+bA3t4eS5cuhY+Pj9hn6tSpSEtLw4gRI5CUlIRWrVohNDQUenp6Yp8tW7bA398fHTt2hFwuh7e3N5YvX66JXSIiIqJyqFxfh6i84HWIiChbRboOUW44qZqkpNJch4iIiIioLDAQERERkeQxEBEREZHkMRARERGR5DEQERERkeQxEBEREZHklevrEBERUcni/c2IcscRIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjze3JWISOLeveErb/ZKUsQRIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjxeqZqIiNS8e+VqgFevpsqPI0REREQkeQxEREREJHkMRERERCR5DEREREQkeQxEREREJHkMRERERCR5DEREREQkeQxEREREJHkVKhD9+OOPkMlkGD9+vNj26tUr+Pn5oVq1aqhatSq8vb0RHx+v9rr79+/Dy8sLBgYGMDc3x5QpU/D69esyrp6IiIjKqwoTiM6fP4+1a9eiQYMGau0TJkzAX3/9hR07duDYsWN4/PgxevXqJS7PysqCl5cXMjIy8Pfff2PDhg1Yv349AgICynoXiIiIqJyqEIEoNTUVPj4++Pnnn2FiYiK2Jycn49dff8XixYvRoUMHuLi4IDg4GH///TfOnDkDADh48CCuXbuGzZs3w9nZGZ07d8Z3332HVatWISMjQ1O7REREROVIhQhEfn5+8PLygru7u1r7xYsXkZmZqdb+0UcfwcbGBqdPnwYAnD59Gk5OTrCwsBD7eHh4ICUlBZGRkbluLz09HSkpKWoPIiIps5u+L8eDqDIp9zd33bZtGy5duoTz58/nWBYXFwcdHR0YGxurtVtYWCAuLk7s83YYyl6evSw3gYGB+Pbbb0ugeiIiIqoIyvUI0YMHDzBu3Dhs2bIFenp6ZbbdGTNmIDk5WXw8ePCgzLZNREREZa9cB6KLFy8iISEBjRs3hra2NrS1tXHs2DEsX74c2trasLCwQEZGBpKSktReFx8fD0tLSwCApaVljrPOsp9n93mXrq4uFAqF2oOIiIgqr3IdiDp27Ih//vkHERER4sPV1RU+Pj7iv6tUqYLDhw+Lr4mKisL9+/fh5uYGAHBzc8M///yDhIQEsU9YWBgUCgUcHR3LfJ+IiIio/CnXc4iMjIzw8ccfq7UZGhqiWrVqYvvQoUMxceJEmJqaQqFQYMyYMXBzc0Pz5s0BAJ06dYKjoyMGDBiA+fPnIy4uDt988w38/Pygq6tb5vtERERE5U+5DkSFsWTJEsjlcnh7eyM9PR0eHh5YvXq1uFxLSwt79+7FqFGj4ObmBkNDQ/j6+mLOnDkarJqIiIjKE5kgCIKmiyjvUlJSoFQqkZyczPlERBLH083/T/SPXpougShfRfn7XeFHiIiISDNyC4cMSVRRletJ1URERERlgYGIiIiIJI+BiIiIiCSPgYiIiIgkj4GIiIiIJI+BiIiIiCSPgYiIiIgkj4GIiIiIJI8XZiQiohLDizVSRcURIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8BiIiIiKSPAYiIiIikjwGIiIiIpI8XoeIiIhK1bvXJuJ1iag84ggRERERSR4DEREREUkeAxERERFJHgMRERERSR4DEREREUkezzIjIqIy9e5ZZwDPPCPN4wgRERERSR4DEREREUkeAxERERFJHgMRERERSR4DEREREUkezzIjIiKN45lnpGkcISIiIiLJYyAiIiIiyWMgIiIiIsljICIiIiLJYyAiIiIiyeNZZkREVC7xzDMqSxwhIiIiIsljICIiIiLJK9eBKDAwEE2aNIGRkRHMzc3Ro0cPREVFqfV59eoV/Pz8UK1aNVStWhXe3t6Ij49X63P//n14eXnBwMAA5ubmmDJlCl6/fl2Wu0JERETlWLkORMeOHYOfnx/OnDmDsLAwZGZmolOnTkhLSxP7TJgwAX/99Rd27NiBY8eO4fHjx+jVq5e4PCsrC15eXsjIyMDff/+NDRs2YP369QgICNDELhEREVE5JBMEQdB0EYX15MkTmJub49ixY2jTpg2Sk5NhZmaGrVu3onfv3gCAGzduwMHBAadPn0bz5s0REhKCrl274vHjx7CwsAAABAUFYdq0aXjy5Al0dHQK3G5KSgqUSiWSk5OhUChKdR+JqHzLbaIvlR1OqqaiKMrf7wp1lllycjIAwNTUFABw8eJFZGZmwt3dXezz0UcfwcbGRgxEp0+fhpOTkxiGAMDDwwOjRo1CZGQkGjVqlGM76enpSE9PF5+npKSU1i4REVER8MwzKi3l+pDZ21QqFcaPH4+WLVvi448/BgDExcVBR0cHxsbGan0tLCwQFxcn9nk7DGUvz16Wm8DAQCiVSvFhbW1dwntDRERE5UmFCUR+fn74999/sW3btlLf1owZM5CcnCw+Hjx4UOrbJCIiIs2pEIfM/P39sXfvXhw/fhw1a9YU2y0tLZGRkYGkpCS1UaL4+HhYWlqKfc6dO6e2vuyz0LL7vEtXVxe6urolvBdERERUXpXrESJBEODv74/du3cjPDwc9vb2astdXFxQpUoVHD58WGyLiorC/fv34ebmBgBwc3PDP//8g4SEBLFPWFgYFAoFHB0dy2ZHiIio1NhN36f2ICqOcj1C5Ofnh61bt+KPP/6AkZGROOdHqVRCX18fSqUSQ4cOxcSJE2FqagqFQoExY8bAzc0NzZs3BwB06tQJjo6OGDBgAObPn4+4uDh888038PPz4ygQERERASjngWjNmjUAgHbt2qm1BwcHY9CgQQCAJUuWQC6Xw9vbG+np6fDw8MDq1avFvlpaWti7dy9GjRoFNzc3GBoawtfXF3PmzCmr3SAiIqJyrkJdh0hTeB0iIsrGQzLlH0/Dp2xF+ftdrucQEREREZWFcn3IjIiIqKh48UYqDo4QERERkeQxEBEREZHk8ZAZERFVejyMRgXhCBERERFJHgMRERERSR4DEREREUke5xARVWazlf//v8marYOoHOK8InobR4iIpCA7GBERUa4YiIikYraSwYiIKA88ZEYkNbOVPIRGlId3D6PxEJp0cISISIo4WkREpIaBiIiIiCSPh8yIiIjywDPRpIMjRERERCR5DEREREQkeTxkRkREVAQ8jFY5cYSIiIiIJI8jRERERO+Jo0YVH0eIiIiISPI4QkRERFQKOGpUsXCEiIiIiCSPI0RERERlhPdKK784QkRERESSxxEiIiIiDeE8o/KDI0REREQkeRwhIiIiKkc4aqQZDERERETlHENS6WMgIiIiqoAYkkoW5xARERGR5HGEiIiIqJLgdY6Kj4GIiIiokuJhtcJjICIiIpIQhqTcMRARERFJHEMSAxERERHlIreQ9K7KFJoYiIiIiKhYKtPIEgMRERERlZjCjCwB5S84SSoQrVq1CgsWLEBcXBwaNmyIFStWoGnTppoui4iISHLK2yE5yQSi7du3Y+LEiQgKCkKzZs2wdOlSeHh4ICoqCubm5pouj4iIiN5R2NGm3BQ1TEnmStWLFy/G8OHDMXjwYDg6OiIoKAgGBgZYt26dpksjIiKiEmY3fR8+nnWg0P0lEYgyMjJw8eJFuLu7i21yuRzu7u44ffq0BisjIiKi8kASh8yePn2KrKwsWFhYqLVbWFjgxo0bOfqnp6cjPT1dfJ6cnAwASElJKd1CiUpaupD/cn6ni0yV/lLTJRBRIWX/vApCAb8LIZFAVFSBgYH49ttvc7RbW1troBqiUvSjUtMVEBGVuhcvXkCpzP/3nSQC0QcffAAtLS3Ex8ertcfHx8PS0jJH/xkzZmDixInic5VKhcTERFSrVg0ymazU6y0tKSkpsLa2xoMHD6BQKDRdjqTxsyg/+FmUL/w8yo/K8FkIgoAXL17AysqqwL6SCEQ6OjpwcXHB4cOH0aNHDwBvQs7hw4fh7++fo7+uri50dXXV2oyNjcug0rKhUCgq7Je7suFnUX7wsyhf+HmUHxX9syhoZCibJAIRAEycOBG+vr5wdXVF06ZNsXTpUqSlpWHw4MGaLo2IiIg0TDKBqG/fvnjy5AkCAgIQFxcHZ2dnhIaG5phoTURERNIjmUAEAP7+/rkeIpMKXV1dzJo1K8fhQCp7/CzKD34W5Qs/j/JDap+FTCjMuWhERERElZgkLsxIRERElB8GIiIiIpI8BiIiIiKSPAYiIiIikjwGIolLT0+Hs7MzZDIZIiIiNF2O5ERHR2Po0KGwt7eHvr4+ateujVmzZiEjI0PTpUnGqlWrYGdnBz09PTRr1gznzp3TdEmSExgYiCZNmsDIyAjm5ubo0aMHoqKiNF0WAfjxxx8hk8kwfvx4TZdS6hiIJG7q1KmFuqQ5lY4bN25ApVJh7dq1iIyMxJIlSxAUFISvvvpK06VJwvbt2zFx4kTMmjULly5dQsOGDeHh4YGEhARNlyYpx44dg5+fH86cOYOwsDBkZmaiU6dOSEtL03Rpknb+/HmsXbsWDRo00HQpZYKn3UtYSEgIJk6ciN9//x3169fH5cuX4ezsrOmyJG/BggVYs2YN7t69q+lSKr1mzZqhSZMmWLlyJYA3t/SxtrbGmDFjMH36dA1XJ11PnjyBubk5jh07hjZt2mi6HElKTU1F48aNsXr1anz//fdwdnbG0qVLNV1WqeIIkUTFx8dj+PDh2LRpEwwMDDRdDr0lOTkZpqammi6j0svIyMDFixfh7u4utsnlcri7u+P06dMarIySk5MBgD8HGuTn5wcvLy+1n4/KTlJXqqY3BEHAoEGD8OWXX8LV1RXR0dGaLon+v9u3b2PFihVYuHChpkup9J4+fYqsrKwct++xsLDAjRs3NFQVqVQqjB8/Hi1btsTHH3+s6XIkadu2bbh06RLOnz+v6VLKFEeIKpHp06dDJpPl+7hx4wZWrFiBFy9eYMaMGZouudIq7GfxtkePHsHT0xOfffYZhg8frqHKiTTLz88P//77L7Zt26bpUiTpwYMHGDduHLZs2QI9PT1Nl1OmOIeoEnny5AmePXuWb59atWqhT58++OuvvyCTycT2rKwsaGlpwcfHBxs2bCjtUiu9wn4WOjo6AIDHjx+jXbt2aN68OdavXw+5nP+vUtoyMjJgYGCAnTt3okePHmK7r68vkpKS8Mcff2iuOIny9/fHH3/8gePHj8Pe3l7T5UjSnj170LNnT2hpaYltWVlZkMlkkMvlSE9PV1tWmTAQSdD9+/eRkpIiPn/8+DE8PDywc+dONGvWDDVr1tRgddLz6NEjtG/fHi4uLti8eXOl/WVTHjVr1gxNmzbFihUrALw5XGNjYwN/f39Oqi5DgiBgzJgx2L17N44ePYq6detquiTJevHiBWJiYtTaBg8ejI8++gjTpk2r1IcxOYdIgmxsbNSeV61aFQBQu3ZthqEy9ujRI7Rr1w62trZYuHAhnjx5Ii6ztLTUYGXSMHHiRPj6+sLV1RVNmzbF0qVLkZaWhsGDB2u6NEnx8/PD1q1b8ccff8DIyAhxcXEAAKVSCX19fQ1XJy1GRkY5Qo+hoSGqVatWqcMQwEBEpFFhYWG4ffs2bt++nSOMcvC29PXt2xdPnjxBQEAA4uLi4OzsjNDQ0BwTral0rVmzBgDQrl07tfbg4GAMGjSo7AsiSeIhMyIiIpI8ztwkIiIiyWMgIiIiIsljICIiIiLJYyAiIiIiyWMgIiIiIsljICIiIiLJYyAiIiIiyWMgIiKNiIuLwyeffAJDQ0MYGxtrupxCk8lk2LNnj6bLIKISxkBEVMnJZLJ8H7Nnz9ZIXUuWLEFsbCwiIiJw8+ZNjdRQ0srre51dG4McUd546w6iSi42Nlb89/bt2xEQEICoqCixLftedsCb24VkZWVBW7v0fzXcuXMHLi4u73Ujz4yMDOjo6JRgVe+nKO91YZS3/SOqzDhCRFTJWVpaig+lUgmZTCY+v3HjBoyMjBASEgIXFxfo6uri5MmTuHPnDrp37w4LCwtUrVoVTZo0waFDh9TWa2dnh7lz52LIkCEwMjKCjY0NfvrpJ3F5RkYG/P39Ub16dejp6cHW1haBgYHia3///Xds3LgRMplMvF/V/fv30b17d1StWhUKhQJ9+vRBfHy8uM7Zs2fD2dkZv/zyC+zt7aGnpwfgzejH2rVr0bVrVxgYGMDBwQGnT5/G7du30a5dOxgaGqJFixa4c+eO2j788ccfaNy4MfT09FCrVi18++23eP36tbj81q1baNOmDfT09ODo6IiwsLBiv9dpaWnw8fEp8D397rvvMHDgQCgUCowYMQIA8PPPP8Pa2hoGBgbo2bMnFi9enOMwY377YmdnBwDo2bMnZDKZ+PxdHTp0gL+/v1rbkydPoKOjg8OHD+e770QVnkBEkhEcHCwolUrx+ZEjRwQAQoMGDYSDBw8Kt2/fFp49eyZEREQIQUFBwj///CPcvHlT+OabbwQ9PT0hJiZGfK2tra1gamoqrFq1Srh165YQGBgoyOVy4caNG4IgCMKCBQsEa2tr4fjx40J0dLRw4sQJYevWrYIgCEJCQoLg6ekp9OnTR4iNjRWSkpKErKwswdnZWWjVqpVw4cIF4cyZM4KLi4vQtm1bcZuzZs0SDA0NBU9PT+HSpUvClStXBEEQBABCjRo1hO3btwtRUVFCjx49BDs7O6FDhw5CaGiocO3aNaF58+aCp6enuK7jx48LCoVCWL9+vXDnzh3h4MGDgp2dnTB79mxBEAQhKytL+Pjjj4WOHTsKERERwrFjx4RGjRoJAITdu3cX+b0u7HuqUCiEhQsXCrdv3xZu374tnDx5UpDL5cKCBQuEqKgoYdWqVYKpqanaugval4SEBAGAEBwcLMTGxgoJCQm51rxlyxbBxMREePXqldi2ePFiwc7OTlCpVAXuM1FFxkBEJCF5BaI9e/YU+Nr69esLK1asEJ/b2toKX3zxhfhcpVIJ5ubmwpo1awRBEIQxY8YIHTp0yPMPaffu3QVfX1/x+cGDBwUtLS3h/v37YltkZKQAQDh37pwgCG8CUZUqVXL8QQcgfPPNN+Lz06dPCwCEX3/9VWz73//+J+jp6YnPO3bsKMydO1dtPZs2bRKqV68uCIIgHDhwQNDW1hYePXokLg8JCSl2IMpNbu9pjx491Pr07dtX8PLyUmvz8fFRW3dB+yIIQqHq/u+//wQTExNh+/btYluDBg3EYEVUmfGQGRHB1dVV7XlqaiomT54MBwcHGBsbo2rVqrh+/Tru37+v1q9Bgwbiv7MPDyUkJAAABg0ahIiICNSrVw9jx47FwYMH863h+vXrsLa2hrW1tdjm6OgIY2NjXL9+XWyztbWFmZlZjte/XYuFhQUAwMnJSa3t1atXSElJAQBcuXIFc+bMQdWqVcXH8OHDERsbi5cvX4r1WFlZietwc3PLdx/yU9j39N3PIioqCk2bNlVre/d5QftSWHp6ehgwYADWrVsHALh06RL+/fdf8ZAmUWXGSdVEBENDQ7XnkydPRlhYGBYuXIg6depAX18fvXv3RkZGhlq/KlWqqD2XyWRQqVQAgMaNG+PevXsICQnBoUOH0KdPH7i7u2Pnzp0lWmtutchksjzbsutLTU3Ft99+i169euVYV/bcpJJU2Pc0r/3LT0nuy7Bhw+Ds7IyHDx8iODgYHTp0gK2tbZFrIqpoGIiIKIdTp05h0KBB6NmzJ4A3f3Cjo6OLvB6FQoG+ffuib9++6N27Nzw9PZGYmAhTU9McfR0cHPDgwQM8ePBAHCW6du0akpKS4Ojo+F77k5vGjRsjKioKderUyXV5dj2xsbGoXr06AODMmTPF3l5x39N69erh/Pnzam3vPi9oX4A34TArK6vA7Tk5OcHV1RU///wztm7dipUrVxb4GqLKgIGIiHKoW7cudu3ahW7dukEmk2HmzJniyEphLV68GNWrV0ejRo0gl8uxY8cOWFpa5nkRRnd3dzg5OcHHxwdLly7F69evMXr0aLRt2zbHYaSSEBAQgK5du8LGxga9e/eGXC7HlStX8O+//+L777+Hu7s7PvzwQ/j6+mLBggVISUnB119/XeztFfc9HTNmDNq0aYPFixejW7duCA8PR0hIiDjiVZh9Ad6caXb48GG0bNkSurq6MDExyXObw4YNg7+/PwwNDcUAR1TZcQ4REeWwePFimJiYoEWLFujWrRs8PDzQuHHjIq3DyMgI8+fPh6urK5o0aYLo6Gjs378fcnnuv3ZkMhn++OMPmJiYoE2bNnB3d0etWrWwffv2ktilHDw8PLB3714cPHgQTZo0QfPmzbFkyRLx8JBcLsfu3bvx33//oWnTphg2bBh++OGHYm+vuO9py5YtERQUhMWLF6Nhw4YIDQ3FhAkT1A6FFbQvALBo0SKEhYXB2toajRo1ynebn3/+ObS1tfH555+XyuFDovJIJgiCoOkiiIio8IYPH44bN27gxIkTpbL+6Oho1K5dG+fPny9yECaqqHjIjIionFu4cKF437eQkBBs2LABq1evLvHtZGZm4tmzZ/jmm2/QvHlzhiGSFAYiIqJy7ty5c5g/fz5evHiBWrVqYfny5Rg2bFiJb+fUqVNo3749Pvzww/c+G5CoouEhMyIiIpI8TqomIiIiyWMgIiIiIsljICIiIiLJYyAiIiIiyWMgIiIiIsljICIiIiLJYyAiIiIiyWMgIiIiIsljICIiIiLJ+38BbAEznFdXNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"y\"] = train_df.efs_time.values\n",
    "mx = train_df.loc[train_df.efs==1,\"efs_time\"].max()\n",
    "mn = train_df.loc[train_df.efs==0,\"efs_time\"].min()\n",
    "train_df.loc[train_df.efs==0,\"y\"] = train_df.loc[train_df.efs==0,\"y\"] + mx - mn\n",
    "train_df.y = train_df.y.rank()\n",
    "train_df.loc[train_df.efs==0,\"y\"] += 2*len(train_df)\n",
    "train_df.y = train_df.y / train_df.y.max()\n",
    "train_df.y = np.log( train_df.y )\n",
    "train_df.y -= train_df.y.mean()\n",
    "train_df.y *= -1.0\n",
    "\n",
    "plt.hist(train_df.loc[train_df.efs==1,\"y\"],bins=100,label=\"efs=1, Yes Event\")\n",
    "plt.hist(train_df.loc[train_df.efs==0,\"y\"],bins=100,label=\"efs=0, Maybe Event\")\n",
    "plt.xlim((-5,5))\n",
    "plt.xlabel(\"Transformed Target y\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Transformed Target y using both efs and efs_time.\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a86d5a0",
   "metadata": {
    "_cell_guid": "2d39ee67-9ffb-40bf-8fcd-c3e1153016f6",
    "_uuid": "3cc22a12-1155-4d62-b15e-5c4024979be4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T17:22:10.913107Z",
     "iopub.status.busy": "2025-01-12T17:22:10.912680Z",
     "iopub.status.idle": "2025-01-12T17:22:10.917077Z",
     "shell.execute_reply": "2025-01-12T17:22:10.915924Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018748,
     "end_time": "2025-01-12T17:22:10.919017",
     "exception": false,
     "start_time": "2025-01-12T17:22:10.900269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from lifelines import NelsonAalenFitter\n",
    "\n",
    "# FOLDS = 10\n",
    "# oof_preds = np.zeros(len(train))\n",
    "# for fold in range(FOLDS):\n",
    "#     train_data = train[train[\"fold\"]!=fold].copy()\n",
    "#     valid_data = train[train[\"fold\"]==fold].copy()\n",
    "    \n",
    "#     naf = NelsonAalenFitter()\n",
    "#     naf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n",
    "    \n",
    "#     oof_preds[valid_data.index] = -naf.cumulative_hazard_at_times(valid_data['efs_time']).values\n",
    "\n",
    "# train['target3'] = oof_preds  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0454c251",
   "metadata": {
    "_cell_guid": "16a6bd99-3af0-4a85-8bf5-7cf445665282",
    "_uuid": "59794a24-b402-4a12-97a6-2c76becf9469",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011022,
     "end_time": "2025-01-12T17:22:10.941653",
     "exception": false,
     "start_time": "2025-01-12T17:22:10.930631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5: Features preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c2ed07",
   "metadata": {
    "_cell_guid": "6c02561f-bcf6-4f70-90d7-d896d00ea765",
    "_uuid": "7bf2caf4-3ad8-42b1-bf13-5e412a2a8127",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T17:22:10.966096Z",
     "iopub.status.busy": "2025-01-12T17:22:10.965613Z",
     "iopub.status.idle": "2025-01-12T17:22:10.971764Z",
     "shell.execute_reply": "2025-01-12T17:22:10.970568Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020658,
     "end_time": "2025-01-12T17:22:10.973431",
     "exception": false,
     "start_time": "2025-01-12T17:22:10.952773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57 FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'hla_match_c_high', 'hla_high_res_8', 'tbi_status', 'arrhythmia', 'hla_low_res_6', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'hla_high_res_6', 'cmv_status', 'hla_high_res_10', 'hla_match_dqb1_high', 'tce_imm_match', 'hla_nmdp_6', 'hla_match_c_low', 'rituximab', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'year_hct', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hla_match_a_high', 'hepatic_severe', 'donor_age', 'prior_tumor', 'hla_match_b_low', 'peptic_ulcer', 'age_at_hct', 'hla_match_a_low', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hla_match_b_high', 'race_group', 'comorbidity_score', 'karnofsky_score', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'hla_low_res_8', 'cardiac', 'hla_match_drb1_high', 'pulm_moderate', 'hla_low_res_10']\n"
     ]
    }
   ],
   "source": [
    "RMV = [\"ID\",\"efs\",\"efs_time\",\"y\",\"fold\"]\n",
    "FEATURES = [c for c in train_df.columns if not c in RMV]\n",
    "print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "698426eb",
   "metadata": {
    "_cell_guid": "fac9793e-41fd-4156-8707-2dadddae8635",
    "_uuid": "62c1bdf1-7676-43e1-86fc-5b5237f4bf4f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T17:22:10.997687Z",
     "iopub.status.busy": "2025-01-12T17:22:10.997319Z",
     "iopub.status.idle": "2025-01-12T17:22:11.112907Z",
     "shell.execute_reply": "2025-01-12T17:22:11.111465Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.129849,
     "end_time": "2025-01-12T17:22:11.114769",
     "exception": false,
     "start_time": "2025-01-12T17:22:10.984920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In these features, there are 35 CATEGORICAL FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status', 'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe', 'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match', 'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'cardiac', 'pulm_moderate']\n"
     ]
    }
   ],
   "source": [
    "CATS = []\n",
    "for c in FEATURES:\n",
    "    if train_df[c].dtype==\"object\":\n",
    "        CATS.append(c)\n",
    "        train_df[c] = train_df[c].fillna(\"NAN\")\n",
    "        test_df[c]  = test_df[c].fillna(\"NAN\")\n",
    "    else:\n",
    "        train_df[c] = train_df[c].fillna(-1)\n",
    "        test_df[c]  = test_df[c].fillna(-1)\n",
    "print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f774e408",
   "metadata": {
    "_cell_guid": "77422609-d68b-48e6-b0d3-1e5705cf548a",
    "_uuid": "e723d9c8-24fe-4101-8323-f39db250b23f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T17:22:11.139858Z",
     "iopub.status.busy": "2025-01-12T17:22:11.139447Z",
     "iopub.status.idle": "2025-01-12T17:22:11.315078Z",
     "shell.execute_reply": "2025-01-12T17:22:11.314056Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.190812,
     "end_time": "2025-01-12T17:22:11.317186",
     "exception": false,
     "start_time": "2025-01-12T17:22:11.126374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We LABEL ENCODE the CATEGORICAL FEATURES: dri_score, psych_disturb, cyto_score, diabetes, tbi_status, arrhythmia, graft_type, vent_hist, renal_issue, pulm_severe, prim_disease_hct, cmv_status, tce_imm_match, rituximab, prod_type, cyto_score_detail, conditioning_intensity, ethnicity, obesity, mrd_hct, in_vivo_tcd, tce_match, hepatic_severe, prior_tumor, peptic_ulcer, gvhd_proph, rheum_issue, sex_match, race_group, hepatic_mild, tce_div_match, donor_related, melphalan_dose, cardiac, pulm_moderate, "
     ]
    }
   ],
   "source": [
    "combined = pd.concat([train_df,test_df],axis=0,ignore_index=True)\n",
    "#print(\"Combined data shape:\", combined.shape )\n",
    "\n",
    "# LABEL ENCODE CATEGORICAL FEATURES\n",
    "print(\"We LABEL ENCODE the CATEGORICAL FEATURES: \",end=\"\")\n",
    "for c in FEATURES:\n",
    "\n",
    "    # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "    if c in CATS:\n",
    "        print(f\"{c}, \",end=\"\")\n",
    "        combined[c],_ = combined[c].factorize()\n",
    "        combined[c] -= combined[c].min()\n",
    "        combined[c] = combined[c].astype(\"int32\")\n",
    "        combined[c] = combined[c].astype(\"category\")\n",
    "\n",
    "    # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n",
    "    else:\n",
    "        if combined[c].dtype==\"float64\":\n",
    "            combined[c] = combined[c].astype(\"float32\")\n",
    "        if combined[c].dtype==\"int64\":\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "    \n",
    "train_df = combined.iloc[:len(train_df)].copy()\n",
    "test_df = combined.iloc[len(train_df):].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22828727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:22:11.341855Z",
     "iopub.status.busy": "2025-01-12T17:22:11.341456Z",
     "iopub.status.idle": "2025-01-12T17:22:11.346112Z",
     "shell.execute_reply": "2025-01-12T17:22:11.345029Z"
    },
    "papermill": {
     "duration": 0.019342,
     "end_time": "2025-01-12T17:22:11.348125",
     "exception": false,
     "start_time": "2025-01-12T17:22:11.328783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUMS = [c for c in FEATURES if not c in CATS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab17fd8",
   "metadata": {
    "_cell_guid": "c741ac55-cb90-47b5-ae30-af924cd94e92",
    "_uuid": "2e460c73-3bfd-4e7b-8438-9214f48cdf10",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01119,
     "end_time": "2025-01-12T17:22:11.371079",
     "exception": false,
     "start_time": "2025-01-12T17:22:11.359889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 6: Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74b29194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:22:11.395604Z",
     "iopub.status.busy": "2025-01-12T17:22:11.395247Z",
     "iopub.status.idle": "2025-01-12T17:22:11.399060Z",
     "shell.execute_reply": "2025-01-12T17:22:11.398093Z"
    },
    "papermill": {
     "duration": 0.018098,
     "end_time": "2025-01-12T17:22:11.400678",
     "exception": false,
     "start_time": "2025-01-12T17:22:11.382580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow.python.framework.ops import disable_eager_execution\n",
    "# disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd396d10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:22:11.424953Z",
     "iopub.status.busy": "2025-01-12T17:22:11.424552Z",
     "iopub.status.idle": "2025-01-12T17:22:11.429290Z",
     "shell.execute_reply": "2025-01-12T17:22:11.428164Z"
    },
    "papermill": {
     "duration": 0.018938,
     "end_time": "2025-01-12T17:22:11.431065",
     "exception": false,
     "start_time": "2025-01-12T17:22:11.412127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75a5cea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:22:11.455701Z",
     "iopub.status.busy": "2025-01-12T17:22:11.455247Z",
     "iopub.status.idle": "2025-01-12T17:22:11.459937Z",
     "shell.execute_reply": "2025-01-12T17:22:11.458824Z"
    },
    "papermill": {
     "duration": 0.019311,
     "end_time": "2025-01-12T17:22:11.461892",
     "exception": false,
     "start_time": "2025-01-12T17:22:11.442581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3c99fbd",
   "metadata": {
    "_cell_guid": "f7785561-27f1-4b1f-8476-491cdde0d065",
    "_uuid": "42fe86fc-ecd4-47ce-93d8-b137aff3855a",
    "execution": {
     "iopub.execute_input": "2025-01-12T17:22:11.486511Z",
     "iopub.status.busy": "2025-01-12T17:22:11.486163Z",
     "iopub.status.idle": "2025-01-12T17:37:21.195384Z",
     "shell.execute_reply": "2025-01-12T17:37:21.194009Z"
    },
    "papermill": {
     "duration": 909.723689,
     "end_time": "2025-01-12T17:37:21.197428",
     "exception": false,
     "start_time": "2025-01-12T17:22:11.473739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "#########################\n",
      "01-12 17:22:11 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7da2b94c2e00>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 17:22:11 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 17:22:11 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:22:11 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 17:22:11 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 17:22:11 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.023367643356323242s\n",
      "01-12 17:22:11 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 17:22:11 I deeptables.m.preprocessor.py 383 - Imputation taken 0.07860040664672852s\n",
      "01-12 17:22:11 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 17:22:11 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.05524897575378418s\n",
      "01-12 17:22:11 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.20468711853027344s\n",
      "01-12 17:22:11 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 17:22:11 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 17:22:11 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:22:11 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:22:11 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:22:12 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 17:22:12 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 7s 7ms/step - loss: 65881.5469 - root_mean_squared_error: 256.6740 - val_loss: 14674.1855 - val_root_mean_squared_error: 121.1371\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4711.1597 - root_mean_squared_error: 68.6379 - val_loss: 546.5446 - val_root_mean_squared_error: 23.3783\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 161.9319 - root_mean_squared_error: 12.7252 - val_loss: 40.6770 - val_root_mean_squared_error: 6.3779\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 40.4272 - root_mean_squared_error: 6.3582 - val_loss: 35.7981 - val_root_mean_squared_error: 5.9832\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 38.4560 - root_mean_squared_error: 6.2013 - val_loss: 34.6076 - val_root_mean_squared_error: 5.8828\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 37.1374 - root_mean_squared_error: 6.0940 - val_loss: 33.2381 - val_root_mean_squared_error: 5.7653\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 35.6279 - root_mean_squared_error: 5.9689 - val_loss: 32.1406 - val_root_mean_squared_error: 5.6693\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 34.2705 - root_mean_squared_error: 5.8541 - val_loss: 30.6162 - val_root_mean_squared_error: 5.5332\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 32.5557 - root_mean_squared_error: 5.7058 - val_loss: 29.0290 - val_root_mean_squared_error: 5.3879\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 31.0340 - root_mean_squared_error: 5.5708 - val_loss: 27.4909 - val_root_mean_squared_error: 5.2432\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 29.2473 - root_mean_squared_error: 5.4081 - val_loss: 26.0378 - val_root_mean_squared_error: 5.1027\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 27.5289 - root_mean_squared_error: 5.2468 - val_loss: 24.3278 - val_root_mean_squared_error: 4.9323\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 25.9544 - root_mean_squared_error: 5.0946 - val_loss: 22.7933 - val_root_mean_squared_error: 4.7742\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 24.2271 - root_mean_squared_error: 4.9221 - val_loss: 21.2745 - val_root_mean_squared_error: 4.6124\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 22.5414 - root_mean_squared_error: 4.7478 - val_loss: 19.6776 - val_root_mean_squared_error: 4.4359\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 21.1383 - root_mean_squared_error: 4.5976 - val_loss: 18.4076 - val_root_mean_squared_error: 4.2904\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 19.4465 - root_mean_squared_error: 4.4098 - val_loss: 17.0151 - val_root_mean_squared_error: 4.1249\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 18.0122 - root_mean_squared_error: 4.2441 - val_loss: 15.6424 - val_root_mean_squared_error: 3.9551\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 16.7334 - root_mean_squared_error: 4.0906 - val_loss: 14.3440 - val_root_mean_squared_error: 3.7874\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 15.4456 - root_mean_squared_error: 3.9301 - val_loss: 13.2044 - val_root_mean_squared_error: 3.6338\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 14.2144 - root_mean_squared_error: 3.7702 - val_loss: 12.1320 - val_root_mean_squared_error: 3.4831\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 13.0150 - root_mean_squared_error: 3.6076 - val_loss: 11.1863 - val_root_mean_squared_error: 3.3446\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 12.0463 - root_mean_squared_error: 3.4708 - val_loss: 10.2690 - val_root_mean_squared_error: 3.2045\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 11.0112 - root_mean_squared_error: 3.3183 - val_loss: 9.3250 - val_root_mean_squared_error: 3.0537\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 10.0821 - root_mean_squared_error: 3.1752 - val_loss: 8.5245 - val_root_mean_squared_error: 2.9197\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 9.2934 - root_mean_squared_error: 3.0485 - val_loss: 7.7430 - val_root_mean_squared_error: 2.7826\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 8.4381 - root_mean_squared_error: 2.9048 - val_loss: 6.9870 - val_root_mean_squared_error: 2.6433\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 7.6629 - root_mean_squared_error: 2.7682 - val_loss: 6.3243 - val_root_mean_squared_error: 2.5148\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 7.0712 - root_mean_squared_error: 2.6592 - val_loss: 5.7300 - val_root_mean_squared_error: 2.3937\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.4198 - root_mean_squared_error: 2.5337 - val_loss: 5.2115 - val_root_mean_squared_error: 2.2829\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.8524 - root_mean_squared_error: 2.4192 - val_loss: 4.7004 - val_root_mean_squared_error: 2.1680\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.3609 - root_mean_squared_error: 2.3154 - val_loss: 4.2825 - val_root_mean_squared_error: 2.0694\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.8646 - root_mean_squared_error: 2.2056 - val_loss: 3.8837 - val_root_mean_squared_error: 1.9707\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 4.5111 - root_mean_squared_error: 2.1239 - val_loss: 3.5470 - val_root_mean_squared_error: 1.8834\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 4.1630 - root_mean_squared_error: 2.0403 - val_loss: 3.2647 - val_root_mean_squared_error: 1.8069\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.8693 - root_mean_squared_error: 1.9670 - val_loss: 3.0588 - val_root_mean_squared_error: 1.7489\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.5753 - root_mean_squared_error: 1.8908 - val_loss: 2.8396 - val_root_mean_squared_error: 1.6851\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.4154 - root_mean_squared_error: 1.8481 - val_loss: 2.6862 - val_root_mean_squared_error: 1.6390\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.2106 - root_mean_squared_error: 1.7918 - val_loss: 2.5659 - val_root_mean_squared_error: 1.6018\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.0830 - root_mean_squared_error: 1.7558 - val_loss: 2.4981 - val_root_mean_squared_error: 1.5805\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.9897 - root_mean_squared_error: 1.7291 - val_loss: 2.4064 - val_root_mean_squared_error: 1.5513\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.8736 - root_mean_squared_error: 1.6952 - val_loss: 2.3231 - val_root_mean_squared_error: 1.5242\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.7696 - root_mean_squared_error: 1.6642 - val_loss: 2.2679 - val_root_mean_squared_error: 1.5060\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.7277 - root_mean_squared_error: 1.6516 - val_loss: 2.2196 - val_root_mean_squared_error: 1.4898\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.6637 - root_mean_squared_error: 1.6321 - val_loss: 2.2063 - val_root_mean_squared_error: 1.4854\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.6261 - root_mean_squared_error: 1.6205 - val_loss: 2.1959 - val_root_mean_squared_error: 1.4818\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5628 - root_mean_squared_error: 1.6009 - val_loss: 2.1710 - val_root_mean_squared_error: 1.4734\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5209 - root_mean_squared_error: 1.5877 - val_loss: 2.1672 - val_root_mean_squared_error: 1.4721\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4894 - root_mean_squared_error: 1.5778 - val_loss: 2.1255 - val_root_mean_squared_error: 1.4579\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4777 - root_mean_squared_error: 1.5741 - val_loss: 2.1195 - val_root_mean_squared_error: 1.4559\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4363 - root_mean_squared_error: 1.5609 - val_loss: 2.1084 - val_root_mean_squared_error: 1.4520\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3895 - root_mean_squared_error: 1.5458 - val_loss: 2.1121 - val_root_mean_squared_error: 1.4533\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3372 - root_mean_squared_error: 1.5288 - val_loss: 2.0910 - val_root_mean_squared_error: 1.4460\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3266 - root_mean_squared_error: 1.5253 - val_loss: 2.1001 - val_root_mean_squared_error: 1.4492\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3220 - root_mean_squared_error: 1.5238 - val_loss: 2.0595 - val_root_mean_squared_error: 1.4351\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2772 - root_mean_squared_error: 1.5090 - val_loss: 2.0855 - val_root_mean_squared_error: 1.4441\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2566 - root_mean_squared_error: 1.5022 - val_loss: 2.1096 - val_root_mean_squared_error: 1.4524\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2316 - root_mean_squared_error: 1.4939 - val_loss: 2.0450 - val_root_mean_squared_error: 1.4300\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2131 - root_mean_squared_error: 1.4877 - val_loss: 2.0414 - val_root_mean_squared_error: 1.4288\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1835 - root_mean_squared_error: 1.4777 - val_loss: 2.0249 - val_root_mean_squared_error: 1.4230\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1764 - root_mean_squared_error: 1.4753 - val_loss: 2.0320 - val_root_mean_squared_error: 1.4255\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1323 - root_mean_squared_error: 1.4602 - val_loss: 2.0020 - val_root_mean_squared_error: 1.4149\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1334 - root_mean_squared_error: 1.4606 - val_loss: 1.9949 - val_root_mean_squared_error: 1.4124\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1071 - root_mean_squared_error: 1.4516 - val_loss: 2.0057 - val_root_mean_squared_error: 1.4162\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0944 - root_mean_squared_error: 1.4472 - val_loss: 2.0106 - val_root_mean_squared_error: 1.4179\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0740 - root_mean_squared_error: 1.4401 - val_loss: 1.9849 - val_root_mean_squared_error: 1.4088\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0614 - root_mean_squared_error: 1.4358 - val_loss: 2.0026 - val_root_mean_squared_error: 1.4151\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0665 - root_mean_squared_error: 1.4375 - val_loss: 1.9877 - val_root_mean_squared_error: 1.4099\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0366 - root_mean_squared_error: 1.4271 - val_loss: 2.0326 - val_root_mean_squared_error: 1.4257\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0363 - root_mean_squared_error: 1.4270 - val_loss: 1.9720 - val_root_mean_squared_error: 1.4043\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0133 - root_mean_squared_error: 1.4189 - val_loss: 2.0238 - val_root_mean_squared_error: 1.4226\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0332 - root_mean_squared_error: 1.4259 - val_loss: 1.9524 - val_root_mean_squared_error: 1.3973\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9999 - root_mean_squared_error: 1.4142 - val_loss: 1.9589 - val_root_mean_squared_error: 1.3996\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0088 - root_mean_squared_error: 1.4173 - val_loss: 1.9505 - val_root_mean_squared_error: 1.3966\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0158 - root_mean_squared_error: 1.4198 - val_loss: 1.9458 - val_root_mean_squared_error: 1.3949\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9995 - root_mean_squared_error: 1.4140 - val_loss: 1.9620 - val_root_mean_squared_error: 1.4007\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0202 - root_mean_squared_error: 1.4213 - val_loss: 2.0713 - val_root_mean_squared_error: 1.4392\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9965 - root_mean_squared_error: 1.4130 - val_loss: 1.9963 - val_root_mean_squared_error: 1.4129\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0093 - root_mean_squared_error: 1.4175 - val_loss: 2.0076 - val_root_mean_squared_error: 1.4169\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9579 - root_mean_squared_error: 1.3993 - val_loss: 1.9882 - val_root_mean_squared_error: 1.4100\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9671 - root_mean_squared_error: 1.4025 - val_loss: 1.9619 - val_root_mean_squared_error: 1.4007\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9656 - root_mean_squared_error: 1.4020 - val_loss: 1.9221 - val_root_mean_squared_error: 1.3864\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9690 - root_mean_squared_error: 1.4032 - val_loss: 1.9967 - val_root_mean_squared_error: 1.4131\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9634 - root_mean_squared_error: 1.4012 - val_loss: 2.0885 - val_root_mean_squared_error: 1.4452\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9934 - root_mean_squared_error: 1.4119 - val_loss: 1.9675 - val_root_mean_squared_error: 1.4027\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9959 - root_mean_squared_error: 1.4128 - val_loss: 1.9518 - val_root_mean_squared_error: 1.3971\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9457 - root_mean_squared_error: 1.3949 - val_loss: 2.1048 - val_root_mean_squared_error: 1.4508\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9785 - root_mean_squared_error: 1.4066 - val_loss: 1.9774 - val_root_mean_squared_error: 1.4062\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9536 - root_mean_squared_error: 1.3977 - val_loss: 1.9317 - val_root_mean_squared_error: 1.3898\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9666 - root_mean_squared_error: 1.4023 - val_loss: 1.9642 - val_root_mean_squared_error: 1.4015\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9645 - root_mean_squared_error: 1.4016 - val_loss: 1.9639 - val_root_mean_squared_error: 1.4014\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9686 - root_mean_squared_error: 1.4031 - val_loss: 1.9615 - val_root_mean_squared_error: 1.4005\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9454 - root_mean_squared_error: 1.3948 - val_loss: 1.9732 - val_root_mean_squared_error: 1.4047\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9651 - root_mean_squared_error: 1.4018 - val_loss: 1.9200 - val_root_mean_squared_error: 1.3856\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9783 - root_mean_squared_error: 1.4065 - val_loss: 1.9374 - val_root_mean_squared_error: 1.3919\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0023 - root_mean_squared_error: 1.4150 - val_loss: 1.9587 - val_root_mean_squared_error: 1.3995\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9858 - root_mean_squared_error: 1.4092 - val_loss: 1.9648 - val_root_mean_squared_error: 1.4017\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9376 - root_mean_squared_error: 1.3920 - val_loss: 1.9483 - val_root_mean_squared_error: 1.3958\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9483 - root_mean_squared_error: 1.3958 - val_loss: 2.1772 - val_root_mean_squared_error: 1.4755\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9481 - root_mean_squared_error: 1.3957 - val_loss: 1.9448 - val_root_mean_squared_error: 1.3945\n",
      "01-12 17:23:43 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 17:23:43 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 17:23:43 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112172211_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 17:23:43 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:23:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 17:23:44 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:23:44 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 2\n",
      "#########################\n",
      "01-12 17:23:44 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7da2b43718a0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 17:23:44 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 17:23:44 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:23:44 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 17:23:44 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 17:23:44 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.023946762084960938s\n",
      "01-12 17:23:44 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 17:23:44 I deeptables.m.preprocessor.py 383 - Imputation taken 0.07692766189575195s\n",
      "01-12 17:23:44 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 17:23:44 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.05671429634094238s\n",
      "01-12 17:23:44 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.20613527297973633s\n",
      "01-12 17:23:44 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 17:23:44 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 17:23:44 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:23:44 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:23:44 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:23:44 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 17:23:44 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 6s 7ms/step - loss: 5083.1626 - root_mean_squared_error: 71.2963 - val_loss: 140.0943 - val_root_mean_squared_error: 11.8361\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 133.8185 - root_mean_squared_error: 11.5680 - val_loss: 129.5245 - val_root_mean_squared_error: 11.3809\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 121.8590 - root_mean_squared_error: 11.0390 - val_loss: 116.3479 - val_root_mean_squared_error: 10.7865\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 107.8422 - root_mean_squared_error: 10.3847 - val_loss: 101.9790 - val_root_mean_squared_error: 10.0985\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 92.9182 - root_mean_squared_error: 9.6394 - val_loss: 85.6990 - val_root_mean_squared_error: 9.2574\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 78.9754 - root_mean_squared_error: 8.8868 - val_loss: 71.9255 - val_root_mean_squared_error: 8.4809\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 67.1029 - root_mean_squared_error: 8.1916 - val_loss: 61.2148 - val_root_mean_squared_error: 7.8240\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 57.6091 - root_mean_squared_error: 7.5901 - val_loss: 53.6115 - val_root_mean_squared_error: 7.3220\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 50.2761 - root_mean_squared_error: 7.0906 - val_loss: 46.8138 - val_root_mean_squared_error: 6.8421\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 44.1703 - root_mean_squared_error: 6.6461 - val_loss: 41.1862 - val_root_mean_squared_error: 6.4176\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 38.8630 - root_mean_squared_error: 6.2340 - val_loss: 36.3404 - val_root_mean_squared_error: 6.0283\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 34.1953 - root_mean_squared_error: 5.8477 - val_loss: 31.9086 - val_root_mean_squared_error: 5.6488\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 29.9415 - root_mean_squared_error: 5.4719 - val_loss: 28.0735 - val_root_mean_squared_error: 5.2984\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 26.0182 - root_mean_squared_error: 5.1008 - val_loss: 24.3981 - val_root_mean_squared_error: 4.9394\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 22.6275 - root_mean_squared_error: 4.7568 - val_loss: 21.1562 - val_root_mean_squared_error: 4.5996\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 19.5734 - root_mean_squared_error: 4.4242 - val_loss: 18.2910 - val_root_mean_squared_error: 4.2768\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 16.7598 - root_mean_squared_error: 4.0939 - val_loss: 15.6317 - val_root_mean_squared_error: 3.9537\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 14.2917 - root_mean_squared_error: 3.7804 - val_loss: 13.4679 - val_root_mean_squared_error: 3.6699\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 12.2461 - root_mean_squared_error: 3.4994 - val_loss: 11.2780 - val_root_mean_squared_error: 3.3583\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 10.3632 - root_mean_squared_error: 3.2192 - val_loss: 9.6038 - val_root_mean_squared_error: 3.0990\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 8.7438 - root_mean_squared_error: 2.9570 - val_loss: 8.0911 - val_root_mean_squared_error: 2.8445\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 7.3914 - root_mean_squared_error: 2.7187 - val_loss: 6.8968 - val_root_mean_squared_error: 2.6262\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.2731 - root_mean_squared_error: 2.5046 - val_loss: 5.8386 - val_root_mean_squared_error: 2.4163\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.3161 - root_mean_squared_error: 2.3057 - val_loss: 4.9950 - val_root_mean_squared_error: 2.2350\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.5931 - root_mean_squared_error: 2.1431 - val_loss: 4.2766 - val_root_mean_squared_error: 2.0680\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.9757 - root_mean_squared_error: 1.9939 - val_loss: 3.7535 - val_root_mean_squared_error: 1.9374\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.5152 - root_mean_squared_error: 1.8749 - val_loss: 3.3066 - val_root_mean_squared_error: 1.8184\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.1291 - root_mean_squared_error: 1.7689 - val_loss: 2.9953 - val_root_mean_squared_error: 1.7307\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.8549 - root_mean_squared_error: 1.6897 - val_loss: 2.6945 - val_root_mean_squared_error: 1.6415\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.6336 - root_mean_squared_error: 1.6228 - val_loss: 2.5057 - val_root_mean_squared_error: 1.5829\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4716 - root_mean_squared_error: 1.5721 - val_loss: 2.3927 - val_root_mean_squared_error: 1.5468\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3759 - root_mean_squared_error: 1.5414 - val_loss: 2.3169 - val_root_mean_squared_error: 1.5221\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2978 - root_mean_squared_error: 1.5159 - val_loss: 2.2085 - val_root_mean_squared_error: 1.4861\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2360 - root_mean_squared_error: 1.4953 - val_loss: 2.1715 - val_root_mean_squared_error: 1.4736\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1917 - root_mean_squared_error: 1.4804 - val_loss: 2.1263 - val_root_mean_squared_error: 1.4582\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1737 - root_mean_squared_error: 1.4743 - val_loss: 2.1351 - val_root_mean_squared_error: 1.4612\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1481 - root_mean_squared_error: 1.4656 - val_loss: 2.0715 - val_root_mean_squared_error: 1.4393\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1312 - root_mean_squared_error: 1.4599 - val_loss: 2.0800 - val_root_mean_squared_error: 1.4422\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1235 - root_mean_squared_error: 1.4572 - val_loss: 2.0578 - val_root_mean_squared_error: 1.4345\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0966 - root_mean_squared_error: 1.4480 - val_loss: 2.0612 - val_root_mean_squared_error: 1.4357\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0862 - root_mean_squared_error: 1.4444 - val_loss: 2.0587 - val_root_mean_squared_error: 1.4348\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0855 - root_mean_squared_error: 1.4441 - val_loss: 2.0439 - val_root_mean_squared_error: 1.4297\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0700 - root_mean_squared_error: 1.4387 - val_loss: 2.1764 - val_root_mean_squared_error: 1.4753\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0439 - root_mean_squared_error: 1.4297 - val_loss: 2.0344 - val_root_mean_squared_error: 1.4263\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0631 - root_mean_squared_error: 1.4363 - val_loss: 2.0177 - val_root_mean_squared_error: 1.4205\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0266 - root_mean_squared_error: 1.4236 - val_loss: 1.9968 - val_root_mean_squared_error: 1.4131\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0290 - root_mean_squared_error: 1.4244 - val_loss: 2.0237 - val_root_mean_squared_error: 1.4226\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0310 - root_mean_squared_error: 1.4251 - val_loss: 1.9932 - val_root_mean_squared_error: 1.4118\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9913 - root_mean_squared_error: 1.4111 - val_loss: 1.9770 - val_root_mean_squared_error: 1.4061\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0565 - root_mean_squared_error: 1.4341 - val_loss: 1.9902 - val_root_mean_squared_error: 1.4107\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9935 - root_mean_squared_error: 1.4119 - val_loss: 1.9654 - val_root_mean_squared_error: 1.4019\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9834 - root_mean_squared_error: 1.4083 - val_loss: 1.9551 - val_root_mean_squared_error: 1.3982\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9765 - root_mean_squared_error: 1.4059 - val_loss: 2.0630 - val_root_mean_squared_error: 1.4363\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9944 - root_mean_squared_error: 1.4122 - val_loss: 2.0107 - val_root_mean_squared_error: 1.4180\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9933 - root_mean_squared_error: 1.4118 - val_loss: 2.0624 - val_root_mean_squared_error: 1.4361\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0126 - root_mean_squared_error: 1.4187 - val_loss: 2.0137 - val_root_mean_squared_error: 1.4190\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9748 - root_mean_squared_error: 1.4053 - val_loss: 1.9438 - val_root_mean_squared_error: 1.3942\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9717 - root_mean_squared_error: 1.4042 - val_loss: 1.9440 - val_root_mean_squared_error: 1.3943\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9513 - root_mean_squared_error: 1.3969 - val_loss: 1.9494 - val_root_mean_squared_error: 1.3962\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0021 - root_mean_squared_error: 1.4150 - val_loss: 1.9497 - val_root_mean_squared_error: 1.3963\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9487 - root_mean_squared_error: 1.3960 - val_loss: 1.9348 - val_root_mean_squared_error: 1.3910\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9906 - root_mean_squared_error: 1.4109 - val_loss: 1.9674 - val_root_mean_squared_error: 1.4026\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9707 - root_mean_squared_error: 1.4038 - val_loss: 2.4485 - val_root_mean_squared_error: 1.5648\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0448 - root_mean_squared_error: 1.4300 - val_loss: 1.9699 - val_root_mean_squared_error: 1.4035\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9554 - root_mean_squared_error: 1.3984 - val_loss: 1.9503 - val_root_mean_squared_error: 1.3965\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9607 - root_mean_squared_error: 1.4003 - val_loss: 2.1072 - val_root_mean_squared_error: 1.4516\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9771 - root_mean_squared_error: 1.4061 - val_loss: 1.9599 - val_root_mean_squared_error: 1.4000\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9670 - root_mean_squared_error: 1.4025 - val_loss: 1.9312 - val_root_mean_squared_error: 1.3897\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9847 - root_mean_squared_error: 1.4088 - val_loss: 1.9710 - val_root_mean_squared_error: 1.4039\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9749 - root_mean_squared_error: 1.4053 - val_loss: 1.9880 - val_root_mean_squared_error: 1.4100\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9524 - root_mean_squared_error: 1.3973 - val_loss: 1.9778 - val_root_mean_squared_error: 1.4063\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9928 - root_mean_squared_error: 1.4117 - val_loss: 1.9175 - val_root_mean_squared_error: 1.3847\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9853 - root_mean_squared_error: 1.4090 - val_loss: 1.9245 - val_root_mean_squared_error: 1.3873\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9298 - root_mean_squared_error: 1.3892 - val_loss: 1.9691 - val_root_mean_squared_error: 1.4032\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9812 - root_mean_squared_error: 1.4075 - val_loss: 2.0120 - val_root_mean_squared_error: 1.4185\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0027 - root_mean_squared_error: 1.4152 - val_loss: 2.0570 - val_root_mean_squared_error: 1.4342\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9600 - root_mean_squared_error: 1.4000 - val_loss: 1.9246 - val_root_mean_squared_error: 1.3873\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9226 - root_mean_squared_error: 1.3866 - val_loss: 1.9255 - val_root_mean_squared_error: 1.3876\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9971 - root_mean_squared_error: 1.4132 - val_loss: 2.0895 - val_root_mean_squared_error: 1.4455\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9316 - root_mean_squared_error: 1.3898 - val_loss: 1.9609 - val_root_mean_squared_error: 1.4003\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9601 - root_mean_squared_error: 1.4000 - val_loss: 1.9018 - val_root_mean_squared_error: 1.3791\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9551 - root_mean_squared_error: 1.3982 - val_loss: 1.9186 - val_root_mean_squared_error: 1.3851\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9729 - root_mean_squared_error: 1.4046 - val_loss: 1.9289 - val_root_mean_squared_error: 1.3888\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9581 - root_mean_squared_error: 1.3993 - val_loss: 1.9319 - val_root_mean_squared_error: 1.3899\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9571 - root_mean_squared_error: 1.3990 - val_loss: 1.9315 - val_root_mean_squared_error: 1.3898\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9334 - root_mean_squared_error: 1.3905 - val_loss: 2.3856 - val_root_mean_squared_error: 1.5445\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0151 - root_mean_squared_error: 1.4195 - val_loss: 2.0335 - val_root_mean_squared_error: 1.4260\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9475 - root_mean_squared_error: 1.3955 - val_loss: 2.0178 - val_root_mean_squared_error: 1.4205\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0225 - root_mean_squared_error: 1.4221 - val_loss: 2.3524 - val_root_mean_squared_error: 1.5337\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9639 - root_mean_squared_error: 1.4014 - val_loss: 2.0581 - val_root_mean_squared_error: 1.4346\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9495 - root_mean_squared_error: 1.3963 - val_loss: 1.9397 - val_root_mean_squared_error: 1.3927\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9694 - root_mean_squared_error: 1.4034 - val_loss: 1.9275 - val_root_mean_squared_error: 1.3884\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9432 - root_mean_squared_error: 1.3940 - val_loss: 2.1067 - val_root_mean_squared_error: 1.4515\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9643 - root_mean_squared_error: 1.4016 - val_loss: 2.0056 - val_root_mean_squared_error: 1.4162\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9824 - root_mean_squared_error: 1.4080 - val_loss: 1.9275 - val_root_mean_squared_error: 1.3884\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9323 - root_mean_squared_error: 1.3901 - val_loss: 1.9287 - val_root_mean_squared_error: 1.3888\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9424 - root_mean_squared_error: 1.3937 - val_loss: 1.9293 - val_root_mean_squared_error: 1.3890\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9416 - root_mean_squared_error: 1.3934 - val_loss: 2.0200 - val_root_mean_squared_error: 1.4213\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9513 - root_mean_squared_error: 1.3969 - val_loss: 1.9217 - val_root_mean_squared_error: 1.3862\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9433 - root_mean_squared_error: 1.3940 - val_loss: 2.0103 - val_root_mean_squared_error: 1.4178\n",
      "01-12 17:25:12 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 17:25:12 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 17:25:12 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112172344_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 17:25:12 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:25:12 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 17:25:13 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:25:13 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 3\n",
      "#########################\n",
      "01-12 17:25:13 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7da2b8bfba90>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 17:25:13 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 17:25:13 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:25:13 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 17:25:13 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 17:25:13 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.021411657333374023s\n",
      "01-12 17:25:13 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 17:25:13 I deeptables.m.preprocessor.py 383 - Imputation taken 0.07651185989379883s\n",
      "01-12 17:25:13 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 17:25:13 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.055524587631225586s\n",
      "01-12 17:25:13 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.2028968334197998s\n",
      "01-12 17:25:13 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 17:25:13 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 17:25:13 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:25:13 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:25:13 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:25:13 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 17:25:13 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 6s 7ms/step - loss: 100960.9453 - root_mean_squared_error: 317.7435 - val_loss: 20403.5000 - val_root_mean_squared_error: 142.8408\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6071.3091 - root_mean_squared_error: 77.9186 - val_loss: 634.1979 - val_root_mean_squared_error: 25.1833\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 258.2077 - root_mean_squared_error: 16.0688 - val_loss: 162.5825 - val_root_mean_squared_error: 12.7508\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 156.5620 - root_mean_squared_error: 12.5125 - val_loss: 154.7624 - val_root_mean_squared_error: 12.4404\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 152.8414 - root_mean_squared_error: 12.3629 - val_loss: 153.1568 - val_root_mean_squared_error: 12.3757\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 149.7581 - root_mean_squared_error: 12.2376 - val_loss: 146.6276 - val_root_mean_squared_error: 12.1090\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 145.7723 - root_mean_squared_error: 12.0736 - val_loss: 147.0973 - val_root_mean_squared_error: 12.1284\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 141.7076 - root_mean_squared_error: 11.9041 - val_loss: 141.0563 - val_root_mean_squared_error: 11.8767\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 137.7527 - root_mean_squared_error: 11.7368 - val_loss: 138.4871 - val_root_mean_squared_error: 11.7681\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 132.8729 - root_mean_squared_error: 11.5271 - val_loss: 131.2919 - val_root_mean_squared_error: 11.4583\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 128.3311 - root_mean_squared_error: 11.3283 - val_loss: 124.7388 - val_root_mean_squared_error: 11.1687\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 123.5750 - root_mean_squared_error: 11.1164 - val_loss: 122.7070 - val_root_mean_squared_error: 11.0773\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 118.7669 - root_mean_squared_error: 10.8980 - val_loss: 117.7877 - val_root_mean_squared_error: 10.8530\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 113.8619 - root_mean_squared_error: 10.6706 - val_loss: 113.4112 - val_root_mean_squared_error: 10.6495\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 108.9274 - root_mean_squared_error: 10.4368 - val_loss: 106.9796 - val_root_mean_squared_error: 10.3431\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 103.7106 - root_mean_squared_error: 10.1838 - val_loss: 103.4046 - val_root_mean_squared_error: 10.1688\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 98.6876 - root_mean_squared_error: 9.9342 - val_loss: 98.1618 - val_root_mean_squared_error: 9.9077\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 93.5793 - root_mean_squared_error: 9.6736 - val_loss: 92.0442 - val_root_mean_squared_error: 9.5940\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 88.8117 - root_mean_squared_error: 9.4240 - val_loss: 86.5559 - val_root_mean_squared_error: 9.3035\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 83.9574 - root_mean_squared_error: 9.1628 - val_loss: 83.7940 - val_root_mean_squared_error: 9.1539\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 79.0542 - root_mean_squared_error: 8.8912 - val_loss: 77.8386 - val_root_mean_squared_error: 8.8226\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 74.1008 - root_mean_squared_error: 8.6082 - val_loss: 72.8783 - val_root_mean_squared_error: 8.5369\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 69.4514 - root_mean_squared_error: 8.3337 - val_loss: 67.4790 - val_root_mean_squared_error: 8.2146\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 64.8514 - root_mean_squared_error: 8.0530 - val_loss: 64.2487 - val_root_mean_squared_error: 8.0155\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 60.4111 - root_mean_squared_error: 7.7725 - val_loss: 58.8542 - val_root_mean_squared_error: 7.6717\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 55.6404 - root_mean_squared_error: 7.4592 - val_loss: 54.2354 - val_root_mean_squared_error: 7.3645\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 51.2098 - root_mean_squared_error: 7.1561 - val_loss: 49.9526 - val_root_mean_squared_error: 7.0677\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 47.2169 - root_mean_squared_error: 6.8715 - val_loss: 46.0317 - val_root_mean_squared_error: 6.7847\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 42.9981 - root_mean_squared_error: 6.5573 - val_loss: 40.9584 - val_root_mean_squared_error: 6.3999\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 39.0922 - root_mean_squared_error: 6.2524 - val_loss: 37.0434 - val_root_mean_squared_error: 6.0863\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 35.2244 - root_mean_squared_error: 5.9350 - val_loss: 33.7258 - val_root_mean_squared_error: 5.8074\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 31.6752 - root_mean_squared_error: 5.6281 - val_loss: 29.9903 - val_root_mean_squared_error: 5.4763\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 28.1176 - root_mean_squared_error: 5.3026 - val_loss: 26.5018 - val_root_mean_squared_error: 5.1480\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 24.7373 - root_mean_squared_error: 4.9737 - val_loss: 23.4076 - val_root_mean_squared_error: 4.8381\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 21.9899 - root_mean_squared_error: 4.6893 - val_loss: 20.3491 - val_root_mean_squared_error: 4.5110\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 19.3056 - root_mean_squared_error: 4.3938 - val_loss: 17.3133 - val_root_mean_squared_error: 4.1609\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 16.7020 - root_mean_squared_error: 4.0868 - val_loss: 15.1917 - val_root_mean_squared_error: 3.8977\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 14.3997 - root_mean_squared_error: 3.7947 - val_loss: 12.9053 - val_root_mean_squared_error: 3.5924\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 12.5609 - root_mean_squared_error: 3.5441 - val_loss: 10.8710 - val_root_mean_squared_error: 3.2971\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 10.6759 - root_mean_squared_error: 3.2674 - val_loss: 9.4127 - val_root_mean_squared_error: 3.0680\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 9.1851 - root_mean_squared_error: 3.0307 - val_loss: 7.8150 - val_root_mean_squared_error: 2.7955\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 7.8873 - root_mean_squared_error: 2.8084 - val_loss: 6.6081 - val_root_mean_squared_error: 2.5706\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.7752 - root_mean_squared_error: 2.6029 - val_loss: 5.5830 - val_root_mean_squared_error: 2.3628\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.8824 - root_mean_squared_error: 2.4254 - val_loss: 4.7495 - val_root_mean_squared_error: 2.1793\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.1140 - root_mean_squared_error: 2.2614 - val_loss: 4.0594 - val_root_mean_squared_error: 2.0148\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.5268 - root_mean_squared_error: 2.1276 - val_loss: 3.5888 - val_root_mean_squared_error: 1.8944\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.0746 - root_mean_squared_error: 2.0186 - val_loss: 3.1655 - val_root_mean_squared_error: 1.7792\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.6977 - root_mean_squared_error: 1.9229 - val_loss: 2.8407 - val_root_mean_squared_error: 1.6854\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.3821 - root_mean_squared_error: 1.8391 - val_loss: 2.6231 - val_root_mean_squared_error: 1.6196\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.1787 - root_mean_squared_error: 1.7829 - val_loss: 2.4628 - val_root_mean_squared_error: 1.5693\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.0063 - root_mean_squared_error: 1.7339 - val_loss: 2.3743 - val_root_mean_squared_error: 1.5409\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.8803 - root_mean_squared_error: 1.6971 - val_loss: 2.2763 - val_root_mean_squared_error: 1.5087\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.7483 - root_mean_squared_error: 1.6578 - val_loss: 2.2161 - val_root_mean_squared_error: 1.4887\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.6800 - root_mean_squared_error: 1.6371 - val_loss: 2.1914 - val_root_mean_squared_error: 1.4803\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5818 - root_mean_squared_error: 1.6068 - val_loss: 2.1403 - val_root_mean_squared_error: 1.4630\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5371 - root_mean_squared_error: 1.5928 - val_loss: 2.1728 - val_root_mean_squared_error: 1.4741\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.4671 - root_mean_squared_error: 1.5707 - val_loss: 2.1105 - val_root_mean_squared_error: 1.4528\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4491 - root_mean_squared_error: 1.5650 - val_loss: 2.0996 - val_root_mean_squared_error: 1.4490\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3931 - root_mean_squared_error: 1.5470 - val_loss: 2.1166 - val_root_mean_squared_error: 1.4549\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3355 - root_mean_squared_error: 1.5282 - val_loss: 2.1070 - val_root_mean_squared_error: 1.4515\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.3061 - root_mean_squared_error: 1.5186 - val_loss: 2.0766 - val_root_mean_squared_error: 1.4411\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.2570 - root_mean_squared_error: 1.5023 - val_loss: 2.0841 - val_root_mean_squared_error: 1.4436\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2687 - root_mean_squared_error: 1.5062 - val_loss: 2.1801 - val_root_mean_squared_error: 1.4765\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1980 - root_mean_squared_error: 1.4826 - val_loss: 2.0476 - val_root_mean_squared_error: 1.4309\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1661 - root_mean_squared_error: 1.4718 - val_loss: 2.0549 - val_root_mean_squared_error: 1.4335\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1370 - root_mean_squared_error: 1.4619 - val_loss: 2.0723 - val_root_mean_squared_error: 1.4396\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1306 - root_mean_squared_error: 1.4596 - val_loss: 2.0888 - val_root_mean_squared_error: 1.4453\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1113 - root_mean_squared_error: 1.4530 - val_loss: 2.1188 - val_root_mean_squared_error: 1.4556\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0972 - root_mean_squared_error: 1.4482 - val_loss: 2.0525 - val_root_mean_squared_error: 1.4327\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0832 - root_mean_squared_error: 1.4433 - val_loss: 2.1406 - val_root_mean_squared_error: 1.4631\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0626 - root_mean_squared_error: 1.4362 - val_loss: 2.2368 - val_root_mean_squared_error: 1.4956\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0472 - root_mean_squared_error: 1.4308 - val_loss: 2.0514 - val_root_mean_squared_error: 1.4323\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0533 - root_mean_squared_error: 1.4329 - val_loss: 2.0089 - val_root_mean_squared_error: 1.4174\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0545 - root_mean_squared_error: 1.4334 - val_loss: 2.0263 - val_root_mean_squared_error: 1.4235\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0191 - root_mean_squared_error: 1.4209 - val_loss: 2.1038 - val_root_mean_squared_error: 1.4504\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0012 - root_mean_squared_error: 1.4146 - val_loss: 2.0063 - val_root_mean_squared_error: 1.4164\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0372 - root_mean_squared_error: 1.4273 - val_loss: 2.0829 - val_root_mean_squared_error: 1.4432\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9944 - root_mean_squared_error: 1.4122 - val_loss: 2.0142 - val_root_mean_squared_error: 1.4192\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9849 - root_mean_squared_error: 1.4089 - val_loss: 1.9909 - val_root_mean_squared_error: 1.4110\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9999 - root_mean_squared_error: 1.4142 - val_loss: 2.0593 - val_root_mean_squared_error: 1.4350\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0073 - root_mean_squared_error: 1.4168 - val_loss: 2.0546 - val_root_mean_squared_error: 1.4334\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0133 - root_mean_squared_error: 1.4189 - val_loss: 2.0850 - val_root_mean_squared_error: 1.4440\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0087 - root_mean_squared_error: 1.4173 - val_loss: 1.9848 - val_root_mean_squared_error: 1.4088\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9720 - root_mean_squared_error: 1.4043 - val_loss: 2.0554 - val_root_mean_squared_error: 1.4337\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9675 - root_mean_squared_error: 1.4027 - val_loss: 1.9993 - val_root_mean_squared_error: 1.4140\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0291 - root_mean_squared_error: 1.4245 - val_loss: 2.9273 - val_root_mean_squared_error: 1.7109\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9755 - root_mean_squared_error: 1.4055 - val_loss: 1.9776 - val_root_mean_squared_error: 1.4063\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0072 - root_mean_squared_error: 1.4168 - val_loss: 2.0897 - val_root_mean_squared_error: 1.4456\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0411 - root_mean_squared_error: 1.4287 - val_loss: 1.9983 - val_root_mean_squared_error: 1.4136\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0245 - root_mean_squared_error: 1.4228 - val_loss: 1.9930 - val_root_mean_squared_error: 1.4118\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9998 - root_mean_squared_error: 1.4142 - val_loss: 2.1739 - val_root_mean_squared_error: 1.4744\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9332 - root_mean_squared_error: 1.3904 - val_loss: 1.9854 - val_root_mean_squared_error: 1.4090\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0148 - root_mean_squared_error: 1.4194 - val_loss: 2.0094 - val_root_mean_squared_error: 1.4175\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9811 - root_mean_squared_error: 1.4075 - val_loss: 1.9948 - val_root_mean_squared_error: 1.4124\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9541 - root_mean_squared_error: 1.3979 - val_loss: 2.0263 - val_root_mean_squared_error: 1.4235\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9376 - root_mean_squared_error: 1.3920 - val_loss: 1.9942 - val_root_mean_squared_error: 1.4122\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9420 - root_mean_squared_error: 1.3936 - val_loss: 2.6927 - val_root_mean_squared_error: 1.6409\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9885 - root_mean_squared_error: 1.4101 - val_loss: 2.0402 - val_root_mean_squared_error: 1.4284\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9552 - root_mean_squared_error: 1.3983 - val_loss: 1.9836 - val_root_mean_squared_error: 1.4084\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 1.9548 - root_mean_squared_error: 1.3982 - val_loss: 1.9729 - val_root_mean_squared_error: 1.4046\n",
      "01-12 17:26:44 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 17:26:44 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 17:26:44 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112172513_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 17:26:44 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:26:44 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 17:26:44 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:26:44 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 4\n",
      "#########################\n",
      "01-12 17:26:45 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7da2b6694fd0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 17:26:45 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 17:26:45 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:26:45 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 17:26:45 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 17:26:45 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.022769927978515625s\n",
      "01-12 17:26:45 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 17:26:45 I deeptables.m.preprocessor.py 383 - Imputation taken 0.08098483085632324s\n",
      "01-12 17:26:45 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 17:26:45 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.057097434997558594s\n",
      "01-12 17:26:45 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.21411442756652832s\n",
      "01-12 17:26:45 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 17:26:45 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 17:26:45 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:26:45 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:26:45 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:26:45 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 19, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 17:26:45 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 6s 7ms/step - loss: 7625.7876 - root_mean_squared_error: 87.3258 - val_loss: 159.5461 - val_root_mean_squared_error: 12.6312\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 159.9293 - root_mean_squared_error: 12.6463 - val_loss: 150.4873 - val_root_mean_squared_error: 12.2673\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 148.1424 - root_mean_squared_error: 12.1714 - val_loss: 137.7148 - val_root_mean_squared_error: 11.7352\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 134.6740 - root_mean_squared_error: 11.6049 - val_loss: 123.7364 - val_root_mean_squared_error: 11.1237\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 120.5626 - root_mean_squared_error: 10.9801 - val_loss: 111.2327 - val_root_mean_squared_error: 10.5467\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 106.7045 - root_mean_squared_error: 10.3298 - val_loss: 95.9132 - val_root_mean_squared_error: 9.7935\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 93.7983 - root_mean_squared_error: 9.6850 - val_loss: 85.1747 - val_root_mean_squared_error: 9.2290\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 82.2524 - root_mean_squared_error: 9.0693 - val_loss: 75.6482 - val_root_mean_squared_error: 8.6976\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 72.4024 - root_mean_squared_error: 8.5090 - val_loss: 65.1545 - val_root_mean_squared_error: 8.0718\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 64.0478 - root_mean_squared_error: 8.0030 - val_loss: 58.0051 - val_root_mean_squared_error: 7.6161\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 56.4568 - root_mean_squared_error: 7.5138 - val_loss: 50.8843 - val_root_mean_squared_error: 7.1333\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 49.4226 - root_mean_squared_error: 7.0301 - val_loss: 44.2511 - val_root_mean_squared_error: 6.6521\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 43.1530 - root_mean_squared_error: 6.5691 - val_loss: 38.9019 - val_root_mean_squared_error: 6.2371\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 37.1200 - root_mean_squared_error: 6.0926 - val_loss: 33.1993 - val_root_mean_squared_error: 5.7619\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 31.6437 - root_mean_squared_error: 5.6253 - val_loss: 28.3055 - val_root_mean_squared_error: 5.3203\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 26.8050 - root_mean_squared_error: 5.1774 - val_loss: 23.2516 - val_root_mean_squared_error: 4.8220\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 22.4719 - root_mean_squared_error: 4.7405 - val_loss: 20.0266 - val_root_mean_squared_error: 4.4751\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 18.6756 - root_mean_squared_error: 4.3215 - val_loss: 16.2270 - val_root_mean_squared_error: 4.0283\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 15.3328 - root_mean_squared_error: 3.9157 - val_loss: 13.3353 - val_root_mean_squared_error: 3.6517\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 12.5188 - root_mean_squared_error: 3.5382 - val_loss: 10.7536 - val_root_mean_squared_error: 3.2793\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 10.2143 - root_mean_squared_error: 3.1960 - val_loss: 8.8528 - val_root_mean_squared_error: 2.9754\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 8.2855 - root_mean_squared_error: 2.8784 - val_loss: 7.1552 - val_root_mean_squared_error: 2.6749\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.7634 - root_mean_squared_error: 2.6007 - val_loss: 5.7534 - val_root_mean_squared_error: 2.3986\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.5498 - root_mean_squared_error: 2.3558 - val_loss: 4.7513 - val_root_mean_squared_error: 2.1798\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.6105 - root_mean_squared_error: 2.1472 - val_loss: 3.9893 - val_root_mean_squared_error: 1.9973\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.9111 - root_mean_squared_error: 1.9777 - val_loss: 3.3736 - val_root_mean_squared_error: 1.8367\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 3.3887 - root_mean_squared_error: 1.8409 - val_loss: 2.9434 - val_root_mean_squared_error: 1.7156\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 3.0350 - root_mean_squared_error: 1.7421 - val_loss: 2.7027 - val_root_mean_squared_error: 1.6440\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.7837 - root_mean_squared_error: 1.6684 - val_loss: 2.4755 - val_root_mean_squared_error: 1.5734\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5980 - root_mean_squared_error: 1.6118 - val_loss: 2.3379 - val_root_mean_squared_error: 1.5290\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4694 - root_mean_squared_error: 1.5714 - val_loss: 2.2298 - val_root_mean_squared_error: 1.4933\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3898 - root_mean_squared_error: 1.5459 - val_loss: 2.1627 - val_root_mean_squared_error: 1.4706\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3273 - root_mean_squared_error: 1.5256 - val_loss: 2.1220 - val_root_mean_squared_error: 1.4567\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2873 - root_mean_squared_error: 1.5124 - val_loss: 2.0748 - val_root_mean_squared_error: 1.4404\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2518 - root_mean_squared_error: 1.5006 - val_loss: 2.0815 - val_root_mean_squared_error: 1.4428\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1995 - root_mean_squared_error: 1.4831 - val_loss: 2.0519 - val_root_mean_squared_error: 1.4324\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2094 - root_mean_squared_error: 1.4864 - val_loss: 2.0952 - val_root_mean_squared_error: 1.4475\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1784 - root_mean_squared_error: 1.4759 - val_loss: 2.0529 - val_root_mean_squared_error: 1.4328\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1603 - root_mean_squared_error: 1.4698 - val_loss: 2.0653 - val_root_mean_squared_error: 1.4371\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1479 - root_mean_squared_error: 1.4656 - val_loss: 1.9757 - val_root_mean_squared_error: 1.4056\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1128 - root_mean_squared_error: 1.4536 - val_loss: 1.9474 - val_root_mean_squared_error: 1.3955\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1041 - root_mean_squared_error: 1.4506 - val_loss: 1.9632 - val_root_mean_squared_error: 1.4012\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1186 - root_mean_squared_error: 1.4556 - val_loss: 1.9731 - val_root_mean_squared_error: 1.4047\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0773 - root_mean_squared_error: 1.4413 - val_loss: 1.9507 - val_root_mean_squared_error: 1.3967\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0846 - root_mean_squared_error: 1.4438 - val_loss: 1.9162 - val_root_mean_squared_error: 1.3843\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0664 - root_mean_squared_error: 1.4375 - val_loss: 1.9035 - val_root_mean_squared_error: 1.3797\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0441 - root_mean_squared_error: 1.4297 - val_loss: 1.8939 - val_root_mean_squared_error: 1.3762\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0553 - root_mean_squared_error: 1.4336 - val_loss: 1.9326 - val_root_mean_squared_error: 1.3902\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0406 - root_mean_squared_error: 1.4285 - val_loss: 1.9672 - val_root_mean_squared_error: 1.4026\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0224 - root_mean_squared_error: 1.4221 - val_loss: 1.9065 - val_root_mean_squared_error: 1.3808\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0276 - root_mean_squared_error: 1.4239 - val_loss: 1.8925 - val_root_mean_squared_error: 1.3757\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0190 - root_mean_squared_error: 1.4209 - val_loss: 1.8577 - val_root_mean_squared_error: 1.3630\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0307 - root_mean_squared_error: 1.4250 - val_loss: 1.8471 - val_root_mean_squared_error: 1.3591\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0191 - root_mean_squared_error: 1.4209 - val_loss: 1.8552 - val_root_mean_squared_error: 1.3621\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0119 - root_mean_squared_error: 1.4184 - val_loss: 1.8648 - val_root_mean_squared_error: 1.3656\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0156 - root_mean_squared_error: 1.4197 - val_loss: 1.8559 - val_root_mean_squared_error: 1.3623\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0366 - root_mean_squared_error: 1.4271 - val_loss: 1.8388 - val_root_mean_squared_error: 1.3560\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0107 - root_mean_squared_error: 1.4180 - val_loss: 1.9603 - val_root_mean_squared_error: 1.4001\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0335 - root_mean_squared_error: 1.4260 - val_loss: 1.8311 - val_root_mean_squared_error: 1.3532\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0372 - root_mean_squared_error: 1.4273 - val_loss: 2.0267 - val_root_mean_squared_error: 1.4236\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0084 - root_mean_squared_error: 1.4172 - val_loss: 1.8253 - val_root_mean_squared_error: 1.3510\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9945 - root_mean_squared_error: 1.4123 - val_loss: 1.8627 - val_root_mean_squared_error: 1.3648\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0080 - root_mean_squared_error: 1.4171 - val_loss: 1.8385 - val_root_mean_squared_error: 1.3559\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0199 - root_mean_squared_error: 1.4212 - val_loss: 1.8600 - val_root_mean_squared_error: 1.3638\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0432 - root_mean_squared_error: 1.4294 - val_loss: 1.8254 - val_root_mean_squared_error: 1.3511\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9945 - root_mean_squared_error: 1.4123 - val_loss: 1.8855 - val_root_mean_squared_error: 1.3731\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0116 - root_mean_squared_error: 1.4183 - val_loss: 1.8932 - val_root_mean_squared_error: 1.3759\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0743 - root_mean_squared_error: 1.4403 - val_loss: 2.1923 - val_root_mean_squared_error: 1.4806\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0325 - root_mean_squared_error: 1.4257 - val_loss: 2.0195 - val_root_mean_squared_error: 1.4211\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0358 - root_mean_squared_error: 1.4268 - val_loss: 1.8361 - val_root_mean_squared_error: 1.3550\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9960 - root_mean_squared_error: 1.4128 - val_loss: 2.0993 - val_root_mean_squared_error: 1.4489\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9826 - root_mean_squared_error: 1.4080 - val_loss: 1.8568 - val_root_mean_squared_error: 1.3626\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0995 - root_mean_squared_error: 1.4490 - val_loss: 1.8668 - val_root_mean_squared_error: 1.3663\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0235 - root_mean_squared_error: 1.4225 - val_loss: 1.9996 - val_root_mean_squared_error: 1.4141\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0064 - root_mean_squared_error: 1.4165 - val_loss: 1.8276 - val_root_mean_squared_error: 1.3519\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0447 - root_mean_squared_error: 1.4299 - val_loss: 1.8296 - val_root_mean_squared_error: 1.3526\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0493 - root_mean_squared_error: 1.4315 - val_loss: 1.9828 - val_root_mean_squared_error: 1.4081\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9830 - root_mean_squared_error: 1.4082 - val_loss: 1.8376 - val_root_mean_squared_error: 1.3556\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0656 - root_mean_squared_error: 1.4372 - val_loss: 1.9163 - val_root_mean_squared_error: 1.3843\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9854 - root_mean_squared_error: 1.4091 - val_loss: 1.8093 - val_root_mean_squared_error: 1.3451\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0883 - root_mean_squared_error: 1.4451 - val_loss: 2.0038 - val_root_mean_squared_error: 1.4156\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9768 - root_mean_squared_error: 1.4060 - val_loss: 1.8902 - val_root_mean_squared_error: 1.3748\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0088 - root_mean_squared_error: 1.4173 - val_loss: 2.0000 - val_root_mean_squared_error: 1.4142\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0266 - root_mean_squared_error: 1.4236 - val_loss: 1.8527 - val_root_mean_squared_error: 1.3611\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9956 - root_mean_squared_error: 1.4127 - val_loss: 1.9768 - val_root_mean_squared_error: 1.4060\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0533 - root_mean_squared_error: 1.4329 - val_loss: 1.8616 - val_root_mean_squared_error: 1.3644\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0350 - root_mean_squared_error: 1.4265 - val_loss: 1.8282 - val_root_mean_squared_error: 1.3521\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9871 - root_mean_squared_error: 1.4096 - val_loss: 1.8348 - val_root_mean_squared_error: 1.3546\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0504 - root_mean_squared_error: 1.4319 - val_loss: 1.8592 - val_root_mean_squared_error: 1.3635\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0533 - root_mean_squared_error: 1.4329 - val_loss: 1.8876 - val_root_mean_squared_error: 1.3739\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0483 - root_mean_squared_error: 1.4312 - val_loss: 1.8269 - val_root_mean_squared_error: 1.3516\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0203 - root_mean_squared_error: 1.4214 - val_loss: 1.8580 - val_root_mean_squared_error: 1.3631\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0927 - root_mean_squared_error: 1.4466 - val_loss: 1.8456 - val_root_mean_squared_error: 1.3585\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9786 - root_mean_squared_error: 1.4066 - val_loss: 2.0626 - val_root_mean_squared_error: 1.4362\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0303 - root_mean_squared_error: 1.4249 - val_loss: 1.9229 - val_root_mean_squared_error: 1.3867\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9759 - root_mean_squared_error: 1.4057 - val_loss: 2.0926 - val_root_mean_squared_error: 1.4466\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0020 - root_mean_squared_error: 1.4149 - val_loss: 1.8532 - val_root_mean_squared_error: 1.3613\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9953 - root_mean_squared_error: 1.4126 - val_loss: 2.1626 - val_root_mean_squared_error: 1.4706\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0287 - root_mean_squared_error: 1.4243 - val_loss: 1.8248 - val_root_mean_squared_error: 1.3509\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0498 - root_mean_squared_error: 1.4317 - val_loss: 1.9772 - val_root_mean_squared_error: 1.4061\n",
      "01-12 17:28:16 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 17:28:16 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 17:28:16 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112172645_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 17:28:16 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:28:16 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 17:28:17 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:28:17 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 5\n",
      "#########################\n",
      "01-12 17:28:17 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7da2b48dae00>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 17:28:17 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 17:28:17 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:28:17 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 17:28:17 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 17:28:17 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.0218963623046875s\n",
      "01-12 17:28:17 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 17:28:17 I deeptables.m.preprocessor.py 383 - Imputation taken 0.07726216316223145s\n",
      "01-12 17:28:17 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 17:28:17 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.05402803421020508s\n",
      "01-12 17:28:17 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.20523691177368164s\n",
      "01-12 17:28:17 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 17:28:17 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 17:28:17 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:28:17 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:28:17 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:28:17 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 17:28:17 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 6s 7ms/step - loss: 318772.1875 - root_mean_squared_error: 564.5991 - val_loss: 99292.8750 - val_root_mean_squared_error: 315.1077\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 37729.9375 - root_mean_squared_error: 194.2420 - val_loss: 7177.9287 - val_root_mean_squared_error: 84.7227\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2264.1475 - root_mean_squared_error: 47.5831 - val_loss: 416.3120 - val_root_mean_squared_error: 20.4037\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 294.1579 - root_mean_squared_error: 17.1510 - val_loss: 241.0159 - val_root_mean_squared_error: 15.5247\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 249.1821 - root_mean_squared_error: 15.7855 - val_loss: 237.2734 - val_root_mean_squared_error: 15.4037\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 241.9043 - root_mean_squared_error: 15.5533 - val_loss: 229.2496 - val_root_mean_squared_error: 15.1410\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 235.2110 - root_mean_squared_error: 15.3366 - val_loss: 219.7111 - val_root_mean_squared_error: 14.8227\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 227.6146 - root_mean_squared_error: 15.0869 - val_loss: 214.3826 - val_root_mean_squared_error: 14.6418\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 219.9703 - root_mean_squared_error: 14.8314 - val_loss: 205.7449 - val_root_mean_squared_error: 14.3438\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 212.1555 - root_mean_squared_error: 14.5656 - val_loss: 196.5840 - val_root_mean_squared_error: 14.0208\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 202.7424 - root_mean_squared_error: 14.2388 - val_loss: 186.8613 - val_root_mean_squared_error: 13.6697\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 193.6645 - root_mean_squared_error: 13.9163 - val_loss: 175.5359 - val_root_mean_squared_error: 13.2490\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 185.7957 - root_mean_squared_error: 13.6307 - val_loss: 169.9204 - val_root_mean_squared_error: 13.0354\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 176.3302 - root_mean_squared_error: 13.2789 - val_loss: 157.1690 - val_root_mean_squared_error: 12.5367\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 168.1844 - root_mean_squared_error: 12.9686 - val_loss: 152.1707 - val_root_mean_squared_error: 12.3357\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 159.9518 - root_mean_squared_error: 12.6472 - val_loss: 142.4390 - val_root_mean_squared_error: 11.9348\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 151.5039 - root_mean_squared_error: 12.3087 - val_loss: 136.2967 - val_root_mean_squared_error: 11.6746\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 144.2671 - root_mean_squared_error: 12.0111 - val_loss: 128.8326 - val_root_mean_squared_error: 11.3504\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 137.0441 - root_mean_squared_error: 11.7066 - val_loss: 120.7363 - val_root_mean_squared_error: 10.9880\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 129.7843 - root_mean_squared_error: 11.3923 - val_loss: 113.4953 - val_root_mean_squared_error: 10.6534\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 123.7881 - root_mean_squared_error: 11.1260 - val_loss: 107.1668 - val_root_mean_squared_error: 10.3521\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 116.4888 - root_mean_squared_error: 10.7930 - val_loss: 100.2011 - val_root_mean_squared_error: 10.0101\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 110.8546 - root_mean_squared_error: 10.5287 - val_loss: 95.8295 - val_root_mean_squared_error: 9.7893\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 104.8221 - root_mean_squared_error: 10.2383 - val_loss: 90.1871 - val_root_mean_squared_error: 9.4967\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 99.3801 - root_mean_squared_error: 9.9690 - val_loss: 84.0023 - val_root_mean_squared_error: 9.1653\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 93.5215 - root_mean_squared_error: 9.6706 - val_loss: 80.0452 - val_root_mean_squared_error: 8.9468\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 87.6308 - root_mean_squared_error: 9.3611 - val_loss: 73.9232 - val_root_mean_squared_error: 8.5979\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 82.7067 - root_mean_squared_error: 9.0943 - val_loss: 69.0061 - val_root_mean_squared_error: 8.3070\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 76.7803 - root_mean_squared_error: 8.7624 - val_loss: 64.6085 - val_root_mean_squared_error: 8.0379\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 71.6440 - root_mean_squared_error: 8.4643 - val_loss: 59.1368 - val_root_mean_squared_error: 7.6900\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 66.6537 - root_mean_squared_error: 8.1642 - val_loss: 54.6266 - val_root_mean_squared_error: 7.3910\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 61.2452 - root_mean_squared_error: 7.8259 - val_loss: 49.5325 - val_root_mean_squared_error: 7.0379\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 56.1689 - root_mean_squared_error: 7.4946 - val_loss: 45.1743 - val_root_mean_squared_error: 6.7212\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 51.4060 - root_mean_squared_error: 7.1698 - val_loss: 40.6336 - val_root_mean_squared_error: 6.3744\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 46.5316 - root_mean_squared_error: 6.8214 - val_loss: 37.0769 - val_root_mean_squared_error: 6.0891\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 42.1483 - root_mean_squared_error: 6.4922 - val_loss: 32.9336 - val_root_mean_squared_error: 5.7388\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 37.8469 - root_mean_squared_error: 6.1520 - val_loss: 28.9015 - val_root_mean_squared_error: 5.3760\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 33.5892 - root_mean_squared_error: 5.7956 - val_loss: 25.6345 - val_root_mean_squared_error: 5.0631\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 29.9636 - root_mean_squared_error: 5.4739 - val_loss: 22.5650 - val_root_mean_squared_error: 4.7503\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 26.5492 - root_mean_squared_error: 5.1526 - val_loss: 19.4236 - val_root_mean_squared_error: 4.4072\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 23.3705 - root_mean_squared_error: 4.8343 - val_loss: 16.8441 - val_root_mean_squared_error: 4.1042\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 20.6419 - root_mean_squared_error: 4.5433 - val_loss: 14.5270 - val_root_mean_squared_error: 3.8114\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 17.9054 - root_mean_squared_error: 4.2315 - val_loss: 12.3542 - val_root_mean_squared_error: 3.5149\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 15.7569 - root_mean_squared_error: 3.9695 - val_loss: 10.5476 - val_root_mean_squared_error: 3.2477\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 13.7286 - root_mean_squared_error: 3.7052 - val_loss: 8.9148 - val_root_mean_squared_error: 2.9858\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 12.0549 - root_mean_squared_error: 3.4720 - val_loss: 7.6549 - val_root_mean_squared_error: 2.7667\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 10.5157 - root_mean_squared_error: 3.2428 - val_loss: 6.4686 - val_root_mean_squared_error: 2.5433\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 9.2438 - root_mean_squared_error: 3.0404 - val_loss: 5.6152 - val_root_mean_squared_error: 2.3696\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 8.1684 - root_mean_squared_error: 2.8580 - val_loss: 4.9678 - val_root_mean_squared_error: 2.2289\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 7.4156 - root_mean_squared_error: 2.7232 - val_loss: 4.2723 - val_root_mean_squared_error: 2.0670\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.7313 - root_mean_squared_error: 2.5945 - val_loss: 3.8251 - val_root_mean_squared_error: 1.9558\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.1871 - root_mean_squared_error: 2.4874 - val_loss: 3.4822 - val_root_mean_squared_error: 1.8661\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.5731 - root_mean_squared_error: 2.3607 - val_loss: 3.2273 - val_root_mean_squared_error: 1.7965\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.2681 - root_mean_squared_error: 2.2952 - val_loss: 2.9934 - val_root_mean_squared_error: 1.7301\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.8864 - root_mean_squared_error: 2.2105 - val_loss: 2.8352 - val_root_mean_squared_error: 1.6838\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.5330 - root_mean_squared_error: 2.1291 - val_loss: 2.7376 - val_root_mean_squared_error: 1.6546\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.2578 - root_mean_squared_error: 2.0634 - val_loss: 2.6514 - val_root_mean_squared_error: 1.6283\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.1213 - root_mean_squared_error: 2.0301 - val_loss: 2.5362 - val_root_mean_squared_error: 1.5925\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.8796 - root_mean_squared_error: 1.9697 - val_loss: 2.4746 - val_root_mean_squared_error: 1.5731\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.6912 - root_mean_squared_error: 1.9213 - val_loss: 2.4204 - val_root_mean_squared_error: 1.5558\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.5679 - root_mean_squared_error: 1.8889 - val_loss: 2.3707 - val_root_mean_squared_error: 1.5397\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.4060 - root_mean_squared_error: 1.8455 - val_loss: 2.4819 - val_root_mean_squared_error: 1.5754\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.2480 - root_mean_squared_error: 1.8022 - val_loss: 2.2849 - val_root_mean_squared_error: 1.5116\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 3.1459 - root_mean_squared_error: 1.7737 - val_loss: 2.2728 - val_root_mean_squared_error: 1.5076\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.9879 - root_mean_squared_error: 1.7285 - val_loss: 2.2218 - val_root_mean_squared_error: 1.4906\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.9092 - root_mean_squared_error: 1.7056 - val_loss: 2.1977 - val_root_mean_squared_error: 1.4825\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.7854 - root_mean_squared_error: 1.6689 - val_loss: 2.1681 - val_root_mean_squared_error: 1.4725\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.7227 - root_mean_squared_error: 1.6500 - val_loss: 2.1320 - val_root_mean_squared_error: 1.4601\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.6108 - root_mean_squared_error: 1.6158 - val_loss: 2.1608 - val_root_mean_squared_error: 1.4700\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5433 - root_mean_squared_error: 1.5948 - val_loss: 2.1373 - val_root_mean_squared_error: 1.4619\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4513 - root_mean_squared_error: 1.5657 - val_loss: 2.0883 - val_root_mean_squared_error: 1.4451\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4153 - root_mean_squared_error: 1.5541 - val_loss: 2.1193 - val_root_mean_squared_error: 1.4558\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.3493 - root_mean_squared_error: 1.5327 - val_loss: 2.1740 - val_root_mean_squared_error: 1.4745\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.3120 - root_mean_squared_error: 1.5205 - val_loss: 2.0370 - val_root_mean_squared_error: 1.4272\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2464 - root_mean_squared_error: 1.4988 - val_loss: 1.9858 - val_root_mean_squared_error: 1.4092\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2053 - root_mean_squared_error: 1.4850 - val_loss: 2.0684 - val_root_mean_squared_error: 1.4382\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1889 - root_mean_squared_error: 1.4795 - val_loss: 2.0432 - val_root_mean_squared_error: 1.4294\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1901 - root_mean_squared_error: 1.4799 - val_loss: 2.1058 - val_root_mean_squared_error: 1.4511\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1369 - root_mean_squared_error: 1.4618 - val_loss: 1.9532 - val_root_mean_squared_error: 1.3976\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1113 - root_mean_squared_error: 1.4530 - val_loss: 1.9673 - val_root_mean_squared_error: 1.4026\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1384 - root_mean_squared_error: 1.4623 - val_loss: 1.9275 - val_root_mean_squared_error: 1.3884\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0677 - root_mean_squared_error: 1.4380 - val_loss: 1.9291 - val_root_mean_squared_error: 1.3889\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0838 - root_mean_squared_error: 1.4435 - val_loss: 2.0699 - val_root_mean_squared_error: 1.4387\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0811 - root_mean_squared_error: 1.4426 - val_loss: 1.9705 - val_root_mean_squared_error: 1.4038\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0808 - root_mean_squared_error: 1.4425 - val_loss: 2.1950 - val_root_mean_squared_error: 1.4816\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0880 - root_mean_squared_error: 1.4450 - val_loss: 2.0543 - val_root_mean_squared_error: 1.4333\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0152 - root_mean_squared_error: 1.4196 - val_loss: 1.9330 - val_root_mean_squared_error: 1.3903\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0293 - root_mean_squared_error: 1.4245 - val_loss: 1.9074 - val_root_mean_squared_error: 1.3811\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0470 - root_mean_squared_error: 1.4307 - val_loss: 1.9441 - val_root_mean_squared_error: 1.3943\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0203 - root_mean_squared_error: 1.4214 - val_loss: 1.9533 - val_root_mean_squared_error: 1.3976\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0217 - root_mean_squared_error: 1.4219 - val_loss: 2.1846 - val_root_mean_squared_error: 1.4780\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9912 - root_mean_squared_error: 1.4111 - val_loss: 1.9642 - val_root_mean_squared_error: 1.4015\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9873 - root_mean_squared_error: 1.4097 - val_loss: 1.9643 - val_root_mean_squared_error: 1.4015\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9932 - root_mean_squared_error: 1.4118 - val_loss: 2.0411 - val_root_mean_squared_error: 1.4287\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0160 - root_mean_squared_error: 1.4199 - val_loss: 1.9198 - val_root_mean_squared_error: 1.3856\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9775 - root_mean_squared_error: 1.4062 - val_loss: 1.9115 - val_root_mean_squared_error: 1.3826\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9838 - root_mean_squared_error: 1.4085 - val_loss: 1.9076 - val_root_mean_squared_error: 1.3812\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0152 - root_mean_squared_error: 1.4196 - val_loss: 1.9165 - val_root_mean_squared_error: 1.3844\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0114 - root_mean_squared_error: 1.4182 - val_loss: 1.9081 - val_root_mean_squared_error: 1.3813\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9976 - root_mean_squared_error: 1.4134 - val_loss: 2.0574 - val_root_mean_squared_error: 1.4344\n",
      "01-12 17:29:47 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 17:29:47 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 17:29:47 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112172817_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 17:29:47 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:29:47 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 17:29:48 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:29:48 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 6\n",
      "#########################\n",
      "01-12 17:29:48 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7da2b7dee290>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 17:29:48 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 17:29:48 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:29:48 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 17:29:48 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 17:29:48 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.022442340850830078s\n",
      "01-12 17:29:48 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 17:29:48 I deeptables.m.preprocessor.py 383 - Imputation taken 0.08619332313537598s\n",
      "01-12 17:29:48 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 17:29:48 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.05751991271972656s\n",
      "01-12 17:29:48 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.2179107666015625s\n",
      "01-12 17:29:48 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 17:29:48 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 17:29:48 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:29:48 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:29:48 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:29:48 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 17:29:48 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 6s 7ms/step - loss: 14358.4102 - root_mean_squared_error: 119.8266 - val_loss: 2853.9067 - val_root_mean_squared_error: 53.4220\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 941.9583 - root_mean_squared_error: 30.6913 - val_loss: 143.6193 - val_root_mean_squared_error: 11.9841\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 50.7388 - root_mean_squared_error: 7.1231 - val_loss: 23.4021 - val_root_mean_squared_error: 4.8376\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 20.7971 - root_mean_squared_error: 4.5604 - val_loss: 20.6965 - val_root_mean_squared_error: 4.5493\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 19.8438 - root_mean_squared_error: 4.4546 - val_loss: 19.8288 - val_root_mean_squared_error: 4.4530\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 19.0695 - root_mean_squared_error: 4.3669 - val_loss: 19.1292 - val_root_mean_squared_error: 4.3737\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 18.2296 - root_mean_squared_error: 4.2696 - val_loss: 18.2590 - val_root_mean_squared_error: 4.2730\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 17.3897 - root_mean_squared_error: 4.1701 - val_loss: 17.3541 - val_root_mean_squared_error: 4.1658\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 16.5230 - root_mean_squared_error: 4.0649 - val_loss: 16.1953 - val_root_mean_squared_error: 4.0243\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 15.5057 - root_mean_squared_error: 3.9377 - val_loss: 15.3507 - val_root_mean_squared_error: 3.9180\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 14.5791 - root_mean_squared_error: 3.8183 - val_loss: 14.2773 - val_root_mean_squared_error: 3.7785\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 13.6823 - root_mean_squared_error: 3.6990 - val_loss: 13.3803 - val_root_mean_squared_error: 3.6579\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 12.7414 - root_mean_squared_error: 3.5695 - val_loss: 12.3338 - val_root_mean_squared_error: 3.5119\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 11.8177 - root_mean_squared_error: 3.4377 - val_loss: 11.4723 - val_root_mean_squared_error: 3.3871\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 10.9756 - root_mean_squared_error: 3.3129 - val_loss: 10.6032 - val_root_mean_squared_error: 3.2563\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 10.1858 - root_mean_squared_error: 3.1915 - val_loss: 9.7413 - val_root_mean_squared_error: 3.1211\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 9.4961 - root_mean_squared_error: 3.0816 - val_loss: 9.0491 - val_root_mean_squared_error: 3.0082\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 8.8149 - root_mean_squared_error: 2.9690 - val_loss: 8.4628 - val_root_mean_squared_error: 2.9091\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 8.2074 - root_mean_squared_error: 2.8649 - val_loss: 7.8194 - val_root_mean_squared_error: 2.7963\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 7.6646 - root_mean_squared_error: 2.7685 - val_loss: 7.2562 - val_root_mean_squared_error: 2.6937\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 7.2073 - root_mean_squared_error: 2.6846 - val_loss: 6.7409 - val_root_mean_squared_error: 2.5963\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 6.7550 - root_mean_squared_error: 2.5990 - val_loss: 6.3824 - val_root_mean_squared_error: 2.5263\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 6.3929 - root_mean_squared_error: 2.5284 - val_loss: 5.9893 - val_root_mean_squared_error: 2.4473\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.0130 - root_mean_squared_error: 2.4521 - val_loss: 5.6223 - val_root_mean_squared_error: 2.3711\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.6128 - root_mean_squared_error: 2.3691 - val_loss: 5.2745 - val_root_mean_squared_error: 2.2966\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.2887 - root_mean_squared_error: 2.2997 - val_loss: 4.9159 - val_root_mean_squared_error: 2.2172\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.9762 - root_mean_squared_error: 2.2307 - val_loss: 4.6047 - val_root_mean_squared_error: 2.1459\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 4.6694 - root_mean_squared_error: 2.1609 - val_loss: 4.3239 - val_root_mean_squared_error: 2.0794\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.4004 - root_mean_squared_error: 2.0977 - val_loss: 4.0653 - val_root_mean_squared_error: 2.0163\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.1010 - root_mean_squared_error: 2.0251 - val_loss: 3.7741 - val_root_mean_squared_error: 1.9427\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.8613 - root_mean_squared_error: 1.9650 - val_loss: 3.5190 - val_root_mean_squared_error: 1.8759\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.6388 - root_mean_squared_error: 1.9076 - val_loss: 3.3180 - val_root_mean_squared_error: 1.8215\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.4200 - root_mean_squared_error: 1.8493 - val_loss: 3.0876 - val_root_mean_squared_error: 1.7572\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.2375 - root_mean_squared_error: 1.7993 - val_loss: 2.9398 - val_root_mean_squared_error: 1.7146\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.0712 - root_mean_squared_error: 1.7525 - val_loss: 2.7710 - val_root_mean_squared_error: 1.6646\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.9181 - root_mean_squared_error: 1.7083 - val_loss: 2.6199 - val_root_mean_squared_error: 1.6186\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.7699 - root_mean_squared_error: 1.6643 - val_loss: 2.4854 - val_root_mean_squared_error: 1.5765\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.6796 - root_mean_squared_error: 1.6370 - val_loss: 2.3692 - val_root_mean_squared_error: 1.5392\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5681 - root_mean_squared_error: 1.6025 - val_loss: 2.3036 - val_root_mean_squared_error: 1.5178\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.4548 - root_mean_squared_error: 1.5668 - val_loss: 2.2384 - val_root_mean_squared_error: 1.4961\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.3794 - root_mean_squared_error: 1.5425 - val_loss: 2.1431 - val_root_mean_squared_error: 1.4639\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3371 - root_mean_squared_error: 1.5288 - val_loss: 2.0953 - val_root_mean_squared_error: 1.4475\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2507 - root_mean_squared_error: 1.5002 - val_loss: 2.0268 - val_root_mean_squared_error: 1.4236\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2188 - root_mean_squared_error: 1.4896 - val_loss: 2.0021 - val_root_mean_squared_error: 1.4149\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2045 - root_mean_squared_error: 1.4848 - val_loss: 1.9542 - val_root_mean_squared_error: 1.3979\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1546 - root_mean_squared_error: 1.4679 - val_loss: 1.9398 - val_root_mean_squared_error: 1.3928\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1246 - root_mean_squared_error: 1.4576 - val_loss: 1.9293 - val_root_mean_squared_error: 1.3890\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1115 - root_mean_squared_error: 1.4531 - val_loss: 1.9087 - val_root_mean_squared_error: 1.3816\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0947 - root_mean_squared_error: 1.4473 - val_loss: 1.8925 - val_root_mean_squared_error: 1.3757\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0766 - root_mean_squared_error: 1.4410 - val_loss: 1.8773 - val_root_mean_squared_error: 1.3702\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0523 - root_mean_squared_error: 1.4326 - val_loss: 1.8842 - val_root_mean_squared_error: 1.3727\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0556 - root_mean_squared_error: 1.4337 - val_loss: 1.8604 - val_root_mean_squared_error: 1.3640\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0356 - root_mean_squared_error: 1.4268 - val_loss: 1.8699 - val_root_mean_squared_error: 1.3674\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0404 - root_mean_squared_error: 1.4284 - val_loss: 1.8557 - val_root_mean_squared_error: 1.3623\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0056 - root_mean_squared_error: 1.4162 - val_loss: 1.8303 - val_root_mean_squared_error: 1.3529\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0215 - root_mean_squared_error: 1.4218 - val_loss: 1.8364 - val_root_mean_squared_error: 1.3551\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0048 - root_mean_squared_error: 1.4159 - val_loss: 1.8351 - val_root_mean_squared_error: 1.3547\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0001 - root_mean_squared_error: 1.4143 - val_loss: 1.8972 - val_root_mean_squared_error: 1.3774\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0039 - root_mean_squared_error: 1.4156 - val_loss: 1.8265 - val_root_mean_squared_error: 1.3515\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9867 - root_mean_squared_error: 1.4095 - val_loss: 1.8439 - val_root_mean_squared_error: 1.3579\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9876 - root_mean_squared_error: 1.4098 - val_loss: 1.8512 - val_root_mean_squared_error: 1.3606\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9761 - root_mean_squared_error: 1.4057 - val_loss: 1.8372 - val_root_mean_squared_error: 1.3554\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9797 - root_mean_squared_error: 1.4070 - val_loss: 1.8529 - val_root_mean_squared_error: 1.3612\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9848 - root_mean_squared_error: 1.4088 - val_loss: 1.8823 - val_root_mean_squared_error: 1.3720\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9845 - root_mean_squared_error: 1.4087 - val_loss: 1.8290 - val_root_mean_squared_error: 1.3524\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9631 - root_mean_squared_error: 1.4011 - val_loss: 1.8818 - val_root_mean_squared_error: 1.3718\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9649 - root_mean_squared_error: 1.4017 - val_loss: 1.8922 - val_root_mean_squared_error: 1.3756\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9612 - root_mean_squared_error: 1.4004 - val_loss: 1.8862 - val_root_mean_squared_error: 1.3734\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9846 - root_mean_squared_error: 1.4088 - val_loss: 1.9953 - val_root_mean_squared_error: 1.4125\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9605 - root_mean_squared_error: 1.4002 - val_loss: 1.8462 - val_root_mean_squared_error: 1.3587\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9524 - root_mean_squared_error: 1.3973 - val_loss: 1.8346 - val_root_mean_squared_error: 1.3545\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9607 - root_mean_squared_error: 1.4003 - val_loss: 1.8374 - val_root_mean_squared_error: 1.3555\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9516 - root_mean_squared_error: 1.3970 - val_loss: 1.8435 - val_root_mean_squared_error: 1.3578\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9497 - root_mean_squared_error: 1.3963 - val_loss: 1.8388 - val_root_mean_squared_error: 1.3560\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9457 - root_mean_squared_error: 1.3949 - val_loss: 1.9185 - val_root_mean_squared_error: 1.3851\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9506 - root_mean_squared_error: 1.3966 - val_loss: 1.8160 - val_root_mean_squared_error: 1.3476\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9708 - root_mean_squared_error: 1.4039 - val_loss: 1.8751 - val_root_mean_squared_error: 1.3693\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9455 - root_mean_squared_error: 1.3948 - val_loss: 1.8911 - val_root_mean_squared_error: 1.3752\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9574 - root_mean_squared_error: 1.3991 - val_loss: 1.8233 - val_root_mean_squared_error: 1.3503\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9385 - root_mean_squared_error: 1.3923 - val_loss: 1.8408 - val_root_mean_squared_error: 1.3568\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9399 - root_mean_squared_error: 1.3928 - val_loss: 1.8334 - val_root_mean_squared_error: 1.3540\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9591 - root_mean_squared_error: 1.3997 - val_loss: 1.8531 - val_root_mean_squared_error: 1.3613\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9401 - root_mean_squared_error: 1.3929 - val_loss: 1.8530 - val_root_mean_squared_error: 1.3613\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9466 - root_mean_squared_error: 1.3952 - val_loss: 1.9037 - val_root_mean_squared_error: 1.3797\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9496 - root_mean_squared_error: 1.3963 - val_loss: 2.0267 - val_root_mean_squared_error: 1.4236\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9607 - root_mean_squared_error: 1.4002 - val_loss: 1.8200 - val_root_mean_squared_error: 1.3491\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9399 - root_mean_squared_error: 1.3928 - val_loss: 1.8199 - val_root_mean_squared_error: 1.3490\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9368 - root_mean_squared_error: 1.3917 - val_loss: 1.9167 - val_root_mean_squared_error: 1.3845\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9558 - root_mean_squared_error: 1.3985 - val_loss: 1.8129 - val_root_mean_squared_error: 1.3464\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9314 - root_mean_squared_error: 1.3898 - val_loss: 1.8104 - val_root_mean_squared_error: 1.3455\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9752 - root_mean_squared_error: 1.4054 - val_loss: 1.8217 - val_root_mean_squared_error: 1.3497\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9418 - root_mean_squared_error: 1.3935 - val_loss: 1.8211 - val_root_mean_squared_error: 1.3495\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9406 - root_mean_squared_error: 1.3930 - val_loss: 1.8512 - val_root_mean_squared_error: 1.3606\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9361 - root_mean_squared_error: 1.3915 - val_loss: 1.8252 - val_root_mean_squared_error: 1.3510\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9564 - root_mean_squared_error: 1.3987 - val_loss: 1.8208 - val_root_mean_squared_error: 1.3494\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9386 - root_mean_squared_error: 1.3923 - val_loss: 1.8371 - val_root_mean_squared_error: 1.3554\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9294 - root_mean_squared_error: 1.3890 - val_loss: 1.8469 - val_root_mean_squared_error: 1.3590\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9378 - root_mean_squared_error: 1.3921 - val_loss: 1.8191 - val_root_mean_squared_error: 1.3487\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9488 - root_mean_squared_error: 1.3960 - val_loss: 1.9291 - val_root_mean_squared_error: 1.3889\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9399 - root_mean_squared_error: 1.3928 - val_loss: 1.8325 - val_root_mean_squared_error: 1.3537\n",
      "01-12 17:31:20 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 17:31:20 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 17:31:20 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112172948_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 17:31:20 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:31:20 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 17:31:21 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:31:21 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 7\n",
      "#########################\n",
      "01-12 17:31:21 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7da2b6274100>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 17:31:21 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 17:31:21 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:31:21 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 17:31:21 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 17:31:21 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.023618459701538086s\n",
      "01-12 17:31:21 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 17:31:21 I deeptables.m.preprocessor.py 383 - Imputation taken 0.07594799995422363s\n",
      "01-12 17:31:21 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 17:31:21 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.05402851104736328s\n",
      "01-12 17:31:21 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.20444631576538086s\n",
      "01-12 17:31:21 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 17:31:21 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 17:31:21 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:31:21 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:31:21 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:31:21 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 17:31:21 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 6s 7ms/step - loss: 83116.4922 - root_mean_squared_error: 288.2993 - val_loss: 31512.6113 - val_root_mean_squared_error: 177.5179\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 15307.7900 - root_mean_squared_error: 123.7247 - val_loss: 5644.7988 - val_root_mean_squared_error: 75.1319\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2536.4390 - root_mean_squared_error: 50.3631 - val_loss: 770.0234 - val_root_mean_squared_error: 27.7493\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 319.6710 - root_mean_squared_error: 17.8793 - val_loss: 95.8992 - val_root_mean_squared_error: 9.7928\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 59.2066 - root_mean_squared_error: 7.6946 - val_loss: 41.7354 - val_root_mean_squared_error: 6.4603\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 40.5729 - root_mean_squared_error: 6.3697 - val_loss: 38.4449 - val_root_mean_squared_error: 6.2004\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 38.7962 - root_mean_squared_error: 6.2287 - val_loss: 37.1349 - val_root_mean_squared_error: 6.0938\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 37.7712 - root_mean_squared_error: 6.1458 - val_loss: 35.9701 - val_root_mean_squared_error: 5.9975\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 36.7823 - root_mean_squared_error: 6.0648 - val_loss: 35.2369 - val_root_mean_squared_error: 5.9361\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 35.4857 - root_mean_squared_error: 5.9570 - val_loss: 33.7754 - val_root_mean_squared_error: 5.8117\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 34.1048 - root_mean_squared_error: 5.8399 - val_loss: 32.0486 - val_root_mean_squared_error: 5.6611\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 32.7848 - root_mean_squared_error: 5.7258 - val_loss: 31.0318 - val_root_mean_squared_error: 5.5706\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 31.4298 - root_mean_squared_error: 5.6062 - val_loss: 29.7419 - val_root_mean_squared_error: 5.4536\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 30.0073 - root_mean_squared_error: 5.4779 - val_loss: 28.2297 - val_root_mean_squared_error: 5.3132\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 28.6970 - root_mean_squared_error: 5.3570 - val_loss: 26.7934 - val_root_mean_squared_error: 5.1762\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 27.2316 - root_mean_squared_error: 5.2184 - val_loss: 25.6776 - val_root_mean_squared_error: 5.0673\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 26.1164 - root_mean_squared_error: 5.1104 - val_loss: 24.3844 - val_root_mean_squared_error: 4.9381\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 24.5219 - root_mean_squared_error: 4.9520 - val_loss: 23.0967 - val_root_mean_squared_error: 4.8059\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 23.3250 - root_mean_squared_error: 4.8296 - val_loss: 21.7200 - val_root_mean_squared_error: 4.6605\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 22.3265 - root_mean_squared_error: 4.7251 - val_loss: 20.3990 - val_root_mean_squared_error: 4.5165\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 20.9664 - root_mean_squared_error: 4.5789 - val_loss: 19.3370 - val_root_mean_squared_error: 4.3974\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 19.7070 - root_mean_squared_error: 4.4393 - val_loss: 18.2789 - val_root_mean_squared_error: 4.2754\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 18.6457 - root_mean_squared_error: 4.3181 - val_loss: 17.2672 - val_root_mean_squared_error: 4.1554\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 17.8984 - root_mean_squared_error: 4.2307 - val_loss: 16.3369 - val_root_mean_squared_error: 4.0419\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 16.9950 - root_mean_squared_error: 4.1225 - val_loss: 15.2509 - val_root_mean_squared_error: 3.9052\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 16.2139 - root_mean_squared_error: 4.0267 - val_loss: 14.6434 - val_root_mean_squared_error: 3.8267\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 15.3666 - root_mean_squared_error: 3.9200 - val_loss: 13.8097 - val_root_mean_squared_error: 3.7161\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 14.6320 - root_mean_squared_error: 3.8252 - val_loss: 13.0448 - val_root_mean_squared_error: 3.6118\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 14.0129 - root_mean_squared_error: 3.7434 - val_loss: 12.4131 - val_root_mean_squared_error: 3.5232\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 13.3244 - root_mean_squared_error: 3.6503 - val_loss: 11.7049 - val_root_mean_squared_error: 3.4212\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 12.7888 - root_mean_squared_error: 3.5761 - val_loss: 11.2235 - val_root_mean_squared_error: 3.3502\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 12.0819 - root_mean_squared_error: 3.4759 - val_loss: 10.5685 - val_root_mean_squared_error: 3.2509\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 11.6787 - root_mean_squared_error: 3.4174 - val_loss: 9.9709 - val_root_mean_squared_error: 3.1577\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 11.0621 - root_mean_squared_error: 3.3260 - val_loss: 9.5119 - val_root_mean_squared_error: 3.0841\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 10.4372 - root_mean_squared_error: 3.2307 - val_loss: 8.8388 - val_root_mean_squared_error: 2.9730\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 10.0061 - root_mean_squared_error: 3.1632 - val_loss: 8.4113 - val_root_mean_squared_error: 2.9002\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 9.3304 - root_mean_squared_error: 3.0546 - val_loss: 7.8941 - val_root_mean_squared_error: 2.8097\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 8.7711 - root_mean_squared_error: 2.9616 - val_loss: 7.3770 - val_root_mean_squared_error: 2.7161\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 8.2897 - root_mean_squared_error: 2.8792 - val_loss: 6.8650 - val_root_mean_squared_error: 2.6201\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 7.8777 - root_mean_squared_error: 2.8067 - val_loss: 6.4589 - val_root_mean_squared_error: 2.5414\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 7.3621 - root_mean_squared_error: 2.7133 - val_loss: 6.0505 - val_root_mean_squared_error: 2.4598\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.8895 - root_mean_squared_error: 2.6248 - val_loss: 5.6134 - val_root_mean_squared_error: 2.3693\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.4447 - root_mean_squared_error: 2.5386 - val_loss: 5.1721 - val_root_mean_squared_error: 2.2742\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.0004 - root_mean_squared_error: 2.4496 - val_loss: 4.7685 - val_root_mean_squared_error: 2.1837\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.6661 - root_mean_squared_error: 2.3804 - val_loss: 4.4612 - val_root_mean_squared_error: 2.1122\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.2955 - root_mean_squared_error: 2.3012 - val_loss: 4.1300 - val_root_mean_squared_error: 2.0322\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 4.9111 - root_mean_squared_error: 2.2161 - val_loss: 3.8226 - val_root_mean_squared_error: 1.9551\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 4.6450 - root_mean_squared_error: 2.1552 - val_loss: 3.5595 - val_root_mean_squared_error: 1.8867\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.2898 - root_mean_squared_error: 2.0712 - val_loss: 3.3189 - val_root_mean_squared_error: 1.8218\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.0457 - root_mean_squared_error: 2.0114 - val_loss: 3.1159 - val_root_mean_squared_error: 1.7652\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.7818 - root_mean_squared_error: 1.9447 - val_loss: 2.9083 - val_root_mean_squared_error: 1.7054\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.6366 - root_mean_squared_error: 1.9070 - val_loss: 2.7198 - val_root_mean_squared_error: 1.6492\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.4205 - root_mean_squared_error: 1.8495 - val_loss: 2.6075 - val_root_mean_squared_error: 1.6148\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.2236 - root_mean_squared_error: 1.7954 - val_loss: 2.4728 - val_root_mean_squared_error: 1.5725\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.0867 - root_mean_squared_error: 1.7569 - val_loss: 2.3641 - val_root_mean_squared_error: 1.5376\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.9479 - root_mean_squared_error: 1.7169 - val_loss: 2.2761 - val_root_mean_squared_error: 1.5087\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.8461 - root_mean_squared_error: 1.6870 - val_loss: 2.2117 - val_root_mean_squared_error: 1.4872\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.7211 - root_mean_squared_error: 1.6496 - val_loss: 2.1475 - val_root_mean_squared_error: 1.4654\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.6412 - root_mean_squared_error: 1.6252 - val_loss: 2.1071 - val_root_mean_squared_error: 1.4516\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5658 - root_mean_squared_error: 1.6018 - val_loss: 2.0963 - val_root_mean_squared_error: 1.4479\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5082 - root_mean_squared_error: 1.5837 - val_loss: 2.0607 - val_root_mean_squared_error: 1.4355\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4494 - root_mean_squared_error: 1.5651 - val_loss: 2.0595 - val_root_mean_squared_error: 1.4351\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3808 - root_mean_squared_error: 1.5430 - val_loss: 2.0261 - val_root_mean_squared_error: 1.4234\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3365 - root_mean_squared_error: 1.5286 - val_loss: 1.9996 - val_root_mean_squared_error: 1.4141\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2810 - root_mean_squared_error: 1.5103 - val_loss: 1.9837 - val_root_mean_squared_error: 1.4084\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2703 - root_mean_squared_error: 1.5068 - val_loss: 1.9907 - val_root_mean_squared_error: 1.4109\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.2107 - root_mean_squared_error: 1.4868 - val_loss: 1.9681 - val_root_mean_squared_error: 1.4029\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1734 - root_mean_squared_error: 1.4742 - val_loss: 1.9955 - val_root_mean_squared_error: 1.4126\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1420 - root_mean_squared_error: 1.4635 - val_loss: 2.0044 - val_root_mean_squared_error: 1.4158\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1360 - root_mean_squared_error: 1.4615 - val_loss: 1.9470 - val_root_mean_squared_error: 1.3954\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1105 - root_mean_squared_error: 1.4528 - val_loss: 1.9347 - val_root_mean_squared_error: 1.3909\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0921 - root_mean_squared_error: 1.4464 - val_loss: 1.9746 - val_root_mean_squared_error: 1.4052\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0814 - root_mean_squared_error: 1.4427 - val_loss: 1.9796 - val_root_mean_squared_error: 1.4070\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0563 - root_mean_squared_error: 1.4340 - val_loss: 2.0029 - val_root_mean_squared_error: 1.4152\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0501 - root_mean_squared_error: 1.4318 - val_loss: 1.9561 - val_root_mean_squared_error: 1.3986\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0233 - root_mean_squared_error: 1.4224 - val_loss: 1.9791 - val_root_mean_squared_error: 1.4068\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0367 - root_mean_squared_error: 1.4271 - val_loss: 1.9456 - val_root_mean_squared_error: 1.3948\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0165 - root_mean_squared_error: 1.4200 - val_loss: 2.0003 - val_root_mean_squared_error: 1.4143\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0001 - root_mean_squared_error: 1.4142 - val_loss: 1.9323 - val_root_mean_squared_error: 1.3901\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9974 - root_mean_squared_error: 1.4133 - val_loss: 1.9615 - val_root_mean_squared_error: 1.4005\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9836 - root_mean_squared_error: 1.4084 - val_loss: 1.9358 - val_root_mean_squared_error: 1.3913\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9970 - root_mean_squared_error: 1.4132 - val_loss: 1.9501 - val_root_mean_squared_error: 1.3965\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9850 - root_mean_squared_error: 1.4089 - val_loss: 1.9459 - val_root_mean_squared_error: 1.3950\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9869 - root_mean_squared_error: 1.4096 - val_loss: 2.0096 - val_root_mean_squared_error: 1.4176\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9679 - root_mean_squared_error: 1.4028 - val_loss: 1.9904 - val_root_mean_squared_error: 1.4108\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9684 - root_mean_squared_error: 1.4030 - val_loss: 2.0278 - val_root_mean_squared_error: 1.4240\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9700 - root_mean_squared_error: 1.4036 - val_loss: 1.9995 - val_root_mean_squared_error: 1.4141\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9594 - root_mean_squared_error: 1.3998 - val_loss: 1.9201 - val_root_mean_squared_error: 1.3857\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9623 - root_mean_squared_error: 1.4008 - val_loss: 1.9423 - val_root_mean_squared_error: 1.3937\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9426 - root_mean_squared_error: 1.3938 - val_loss: 1.9373 - val_root_mean_squared_error: 1.3919\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9598 - root_mean_squared_error: 1.3999 - val_loss: 1.9714 - val_root_mean_squared_error: 1.4041\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9576 - root_mean_squared_error: 1.3991 - val_loss: 1.9839 - val_root_mean_squared_error: 1.4085\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9503 - root_mean_squared_error: 1.3965 - val_loss: 1.9119 - val_root_mean_squared_error: 1.3827\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9523 - root_mean_squared_error: 1.3972 - val_loss: 1.9352 - val_root_mean_squared_error: 1.3911\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9323 - root_mean_squared_error: 1.3901 - val_loss: 1.9662 - val_root_mean_squared_error: 1.4022\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9496 - root_mean_squared_error: 1.3963 - val_loss: 2.0228 - val_root_mean_squared_error: 1.4222\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9429 - root_mean_squared_error: 1.3939 - val_loss: 2.0277 - val_root_mean_squared_error: 1.4240\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9372 - root_mean_squared_error: 1.3918 - val_loss: 1.9126 - val_root_mean_squared_error: 1.3830\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9474 - root_mean_squared_error: 1.3955 - val_loss: 1.9149 - val_root_mean_squared_error: 1.3838\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9348 - root_mean_squared_error: 1.3910 - val_loss: 1.9762 - val_root_mean_squared_error: 1.4058\n",
      "01-12 17:32:50 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 17:32:50 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 17:32:50 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112173121_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 17:32:50 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:32:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 17:32:50 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:32:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 8\n",
      "#########################\n",
      "01-12 17:32:50 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7da2b489ffd0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 17:32:50 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 17:32:50 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:32:50 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 17:32:50 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 17:32:51 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.021335363388061523s\n",
      "01-12 17:32:51 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 17:32:51 I deeptables.m.preprocessor.py 383 - Imputation taken 0.07800555229187012s\n",
      "01-12 17:32:51 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 17:32:51 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.055188894271850586s\n",
      "01-12 17:32:51 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.20482373237609863s\n",
      "01-12 17:32:51 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 17:32:51 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 17:32:51 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:32:51 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:32:51 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:32:51 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 17:32:51 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 7s 8ms/step - loss: 3838.2373 - root_mean_squared_error: 61.9535 - val_loss: 95.4847 - val_root_mean_squared_error: 9.7716\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 103.2588 - root_mean_squared_error: 10.1616 - val_loss: 92.1309 - val_root_mean_squared_error: 9.5985\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 97.7446 - root_mean_squared_error: 9.8866 - val_loss: 86.8611 - val_root_mean_squared_error: 9.3199\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 91.2559 - root_mean_squared_error: 9.5528 - val_loss: 79.7462 - val_root_mean_squared_error: 8.9301\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 84.1660 - root_mean_squared_error: 9.1742 - val_loss: 73.3721 - val_root_mean_squared_error: 8.5658\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 76.6712 - root_mean_squared_error: 8.7562 - val_loss: 66.4653 - val_root_mean_squared_error: 8.1526\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 69.2598 - root_mean_squared_error: 8.3222 - val_loss: 59.5478 - val_root_mean_squared_error: 7.7167\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 61.8747 - root_mean_squared_error: 7.8661 - val_loss: 52.4811 - val_root_mean_squared_error: 7.2444\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 54.9866 - root_mean_squared_error: 7.4153 - val_loss: 46.7071 - val_root_mean_squared_error: 6.8343\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 48.4779 - root_mean_squared_error: 6.9626 - val_loss: 40.2485 - val_root_mean_squared_error: 6.3442\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 42.4739 - root_mean_squared_error: 6.5172 - val_loss: 35.7703 - val_root_mean_squared_error: 5.9808\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 36.9416 - root_mean_squared_error: 6.0780 - val_loss: 31.0044 - val_root_mean_squared_error: 5.5682\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 31.8379 - root_mean_squared_error: 5.6425 - val_loss: 26.7291 - val_root_mean_squared_error: 5.1700\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 27.2781 - root_mean_squared_error: 5.2228 - val_loss: 22.8873 - val_root_mean_squared_error: 4.7841\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 23.1492 - root_mean_squared_error: 4.8114 - val_loss: 19.1126 - val_root_mean_squared_error: 4.3718\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 19.5524 - root_mean_squared_error: 4.4218 - val_loss: 16.2387 - val_root_mean_squared_error: 4.0297\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 16.3846 - root_mean_squared_error: 4.0478 - val_loss: 13.4782 - val_root_mean_squared_error: 3.6713\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 13.6397 - root_mean_squared_error: 3.6932 - val_loss: 11.2412 - val_root_mean_squared_error: 3.3528\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 11.3047 - root_mean_squared_error: 3.3623 - val_loss: 9.1969 - val_root_mean_squared_error: 3.0326\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 9.3404 - root_mean_squared_error: 3.0562 - val_loss: 7.7188 - val_root_mean_squared_error: 2.7783\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 7.7228 - root_mean_squared_error: 2.7790 - val_loss: 6.4111 - val_root_mean_squared_error: 2.5320\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.4158 - root_mean_squared_error: 2.5329 - val_loss: 5.3026 - val_root_mean_squared_error: 2.3027\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.3337 - root_mean_squared_error: 2.3095 - val_loss: 4.5773 - val_root_mean_squared_error: 2.1395\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.5323 - root_mean_squared_error: 2.1289 - val_loss: 3.8310 - val_root_mean_squared_error: 1.9573\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.8925 - root_mean_squared_error: 1.9729 - val_loss: 3.3733 - val_root_mean_squared_error: 1.8367\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.4163 - root_mean_squared_error: 1.8483 - val_loss: 2.9729 - val_root_mean_squared_error: 1.7242\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.0487 - root_mean_squared_error: 1.7461 - val_loss: 2.7655 - val_root_mean_squared_error: 1.6630\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.8038 - root_mean_squared_error: 1.6744 - val_loss: 2.5361 - val_root_mean_squared_error: 1.5925\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.6181 - root_mean_squared_error: 1.6180 - val_loss: 2.4009 - val_root_mean_squared_error: 1.5495\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4829 - root_mean_squared_error: 1.5757 - val_loss: 2.3217 - val_root_mean_squared_error: 1.5237\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3932 - root_mean_squared_error: 1.5470 - val_loss: 2.2724 - val_root_mean_squared_error: 1.5074\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3418 - root_mean_squared_error: 1.5303 - val_loss: 2.2117 - val_root_mean_squared_error: 1.4872\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2720 - root_mean_squared_error: 1.5073 - val_loss: 2.1649 - val_root_mean_squared_error: 1.4714\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2367 - root_mean_squared_error: 1.4956 - val_loss: 2.1288 - val_root_mean_squared_error: 1.4590\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2070 - root_mean_squared_error: 1.4856 - val_loss: 2.0934 - val_root_mean_squared_error: 1.4469\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2076 - root_mean_squared_error: 1.4858 - val_loss: 2.1124 - val_root_mean_squared_error: 1.4534\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1679 - root_mean_squared_error: 1.4724 - val_loss: 2.0709 - val_root_mean_squared_error: 1.4391\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1512 - root_mean_squared_error: 1.4667 - val_loss: 2.0379 - val_root_mean_squared_error: 1.4275\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1280 - root_mean_squared_error: 1.4588 - val_loss: 2.1072 - val_root_mean_squared_error: 1.4516\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1078 - root_mean_squared_error: 1.4518 - val_loss: 2.0271 - val_root_mean_squared_error: 1.4238\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1009 - root_mean_squared_error: 1.4494 - val_loss: 2.0186 - val_root_mean_squared_error: 1.4208\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0793 - root_mean_squared_error: 1.4420 - val_loss: 2.0117 - val_root_mean_squared_error: 1.4183\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0714 - root_mean_squared_error: 1.4392 - val_loss: 2.0094 - val_root_mean_squared_error: 1.4175\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0469 - root_mean_squared_error: 1.4307 - val_loss: 1.9828 - val_root_mean_squared_error: 1.4081\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0351 - root_mean_squared_error: 1.4266 - val_loss: 1.9685 - val_root_mean_squared_error: 1.4030\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0277 - root_mean_squared_error: 1.4240 - val_loss: 1.9815 - val_root_mean_squared_error: 1.4077\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0407 - root_mean_squared_error: 1.4285 - val_loss: 1.9498 - val_root_mean_squared_error: 1.3964\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0352 - root_mean_squared_error: 1.4266 - val_loss: 1.9930 - val_root_mean_squared_error: 1.4117\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0332 - root_mean_squared_error: 1.4259 - val_loss: 1.9500 - val_root_mean_squared_error: 1.3964\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0053 - root_mean_squared_error: 1.4161 - val_loss: 1.9592 - val_root_mean_squared_error: 1.3997\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0092 - root_mean_squared_error: 1.4175 - val_loss: 1.9339 - val_root_mean_squared_error: 1.3906\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9793 - root_mean_squared_error: 1.4069 - val_loss: 1.9181 - val_root_mean_squared_error: 1.3850\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0191 - root_mean_squared_error: 1.4209 - val_loss: 1.9520 - val_root_mean_squared_error: 1.3971\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9831 - root_mean_squared_error: 1.4082 - val_loss: 1.9230 - val_root_mean_squared_error: 1.3867\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0122 - root_mean_squared_error: 1.4185 - val_loss: 1.9283 - val_root_mean_squared_error: 1.3886\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0078 - root_mean_squared_error: 1.4170 - val_loss: 2.0307 - val_root_mean_squared_error: 1.4250\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9872 - root_mean_squared_error: 1.4097 - val_loss: 1.9157 - val_root_mean_squared_error: 1.3841\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0111 - root_mean_squared_error: 1.4181 - val_loss: 2.2284 - val_root_mean_squared_error: 1.4928\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0192 - root_mean_squared_error: 1.4210 - val_loss: 1.9258 - val_root_mean_squared_error: 1.3877\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9645 - root_mean_squared_error: 1.4016 - val_loss: 1.9971 - val_root_mean_squared_error: 1.4132\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9690 - root_mean_squared_error: 1.4032 - val_loss: 1.9083 - val_root_mean_squared_error: 1.3814\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0203 - root_mean_squared_error: 1.4214 - val_loss: 1.9380 - val_root_mean_squared_error: 1.3921\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0077 - root_mean_squared_error: 1.4169 - val_loss: 1.9878 - val_root_mean_squared_error: 1.4099\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9896 - root_mean_squared_error: 1.4105 - val_loss: 1.9079 - val_root_mean_squared_error: 1.3813\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9781 - root_mean_squared_error: 1.4064 - val_loss: 1.9527 - val_root_mean_squared_error: 1.3974\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0134 - root_mean_squared_error: 1.4190 - val_loss: 1.9920 - val_root_mean_squared_error: 1.4114\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9904 - root_mean_squared_error: 1.4108 - val_loss: 1.9400 - val_root_mean_squared_error: 1.3929\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9843 - root_mean_squared_error: 1.4086 - val_loss: 2.0195 - val_root_mean_squared_error: 1.4211\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9996 - root_mean_squared_error: 1.4141 - val_loss: 2.3573 - val_root_mean_squared_error: 1.5354\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9856 - root_mean_squared_error: 1.4091 - val_loss: 2.0889 - val_root_mean_squared_error: 1.4453\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9704 - root_mean_squared_error: 1.4037 - val_loss: 2.0894 - val_root_mean_squared_error: 1.4455\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9990 - root_mean_squared_error: 1.4138 - val_loss: 2.0726 - val_root_mean_squared_error: 1.4396\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9690 - root_mean_squared_error: 1.4032 - val_loss: 1.9142 - val_root_mean_squared_error: 1.3835\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9714 - root_mean_squared_error: 1.4041 - val_loss: 2.0186 - val_root_mean_squared_error: 1.4208\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9723 - root_mean_squared_error: 1.4044 - val_loss: 2.0071 - val_root_mean_squared_error: 1.4167\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0211 - root_mean_squared_error: 1.4216 - val_loss: 1.9065 - val_root_mean_squared_error: 1.3807\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9932 - root_mean_squared_error: 1.4118 - val_loss: 1.9956 - val_root_mean_squared_error: 1.4127\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0067 - root_mean_squared_error: 1.4166 - val_loss: 2.2002 - val_root_mean_squared_error: 1.4833\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0834 - root_mean_squared_error: 1.4434 - val_loss: 2.1794 - val_root_mean_squared_error: 1.4763\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9656 - root_mean_squared_error: 1.4020 - val_loss: 2.0009 - val_root_mean_squared_error: 1.4145\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9677 - root_mean_squared_error: 1.4027 - val_loss: 1.9076 - val_root_mean_squared_error: 1.3811\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9690 - root_mean_squared_error: 1.4032 - val_loss: 1.9180 - val_root_mean_squared_error: 1.3849\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0125 - root_mean_squared_error: 1.4186 - val_loss: 1.9079 - val_root_mean_squared_error: 1.3813\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9728 - root_mean_squared_error: 1.4046 - val_loss: 1.9975 - val_root_mean_squared_error: 1.4133\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9731 - root_mean_squared_error: 1.4047 - val_loss: 2.0049 - val_root_mean_squared_error: 1.4160\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9311 - root_mean_squared_error: 1.3897 - val_loss: 1.9147 - val_root_mean_squared_error: 1.3837\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9790 - root_mean_squared_error: 1.4068 - val_loss: 1.9218 - val_root_mean_squared_error: 1.3863\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9846 - root_mean_squared_error: 1.4088 - val_loss: 1.8896 - val_root_mean_squared_error: 1.3746\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9951 - root_mean_squared_error: 1.4125 - val_loss: 1.9074 - val_root_mean_squared_error: 1.3811\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9759 - root_mean_squared_error: 1.4057 - val_loss: 2.1394 - val_root_mean_squared_error: 1.4627\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9547 - root_mean_squared_error: 1.3981 - val_loss: 1.9557 - val_root_mean_squared_error: 1.3985\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0064 - root_mean_squared_error: 1.4165 - val_loss: 1.9057 - val_root_mean_squared_error: 1.3805\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0139 - root_mean_squared_error: 1.4191 - val_loss: 1.9117 - val_root_mean_squared_error: 1.3827\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9617 - root_mean_squared_error: 1.4006 - val_loss: 1.9830 - val_root_mean_squared_error: 1.4082\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9837 - root_mean_squared_error: 1.4084 - val_loss: 1.9042 - val_root_mean_squared_error: 1.3799\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9624 - root_mean_squared_error: 1.4009 - val_loss: 1.9455 - val_root_mean_squared_error: 1.3948\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9834 - root_mean_squared_error: 1.4083 - val_loss: 2.2126 - val_root_mean_squared_error: 1.4875\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9561 - root_mean_squared_error: 1.3986 - val_loss: 1.9883 - val_root_mean_squared_error: 1.4101\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9428 - root_mean_squared_error: 1.3938 - val_loss: 1.8888 - val_root_mean_squared_error: 1.3743\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9525 - root_mean_squared_error: 1.3973 - val_loss: 1.9459 - val_root_mean_squared_error: 1.3950\n",
      "01-12 17:34:20 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 17:34:20 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 17:34:20 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112173250_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 17:34:20 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:34:20 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 17:34:20 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:34:20 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 9\n",
      "#########################\n",
      "01-12 17:34:20 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7da2b6452260>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 17:34:20 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 17:34:20 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:34:20 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 17:34:20 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 17:34:20 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.021657943725585938s\n",
      "01-12 17:34:20 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 17:34:20 I deeptables.m.preprocessor.py 383 - Imputation taken 0.08274245262145996s\n",
      "01-12 17:34:20 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 17:34:20 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.05347251892089844s\n",
      "01-12 17:34:20 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.2076408863067627s\n",
      "01-12 17:34:20 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 17:34:20 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 17:34:20 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:34:20 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:34:20 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:34:21 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 17:34:21 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 6s 7ms/step - loss: 3814.0486 - root_mean_squared_error: 61.7580 - val_loss: 803.6282 - val_root_mean_squared_error: 28.3483\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 347.4397 - root_mean_squared_error: 18.6397 - val_loss: 118.2327 - val_root_mean_squared_error: 10.8735\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 55.3501 - root_mean_squared_error: 7.4398 - val_loss: 21.5193 - val_root_mean_squared_error: 4.6389\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 12.0445 - root_mean_squared_error: 3.4705 - val_loss: 7.4623 - val_root_mean_squared_error: 2.7317\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.3925 - root_mean_squared_error: 2.5283 - val_loss: 5.9296 - val_root_mean_squared_error: 2.4351\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.7798 - root_mean_squared_error: 2.4041 - val_loss: 5.6857 - val_root_mean_squared_error: 2.3845\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.6254 - root_mean_squared_error: 2.3718 - val_loss: 5.5196 - val_root_mean_squared_error: 2.3494\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.4755 - root_mean_squared_error: 2.3400 - val_loss: 5.4039 - val_root_mean_squared_error: 2.3246\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.3264 - root_mean_squared_error: 2.3079 - val_loss: 5.2198 - val_root_mean_squared_error: 2.2847\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 5.1573 - root_mean_squared_error: 2.2710 - val_loss: 5.0358 - val_root_mean_squared_error: 2.2441\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.9959 - root_mean_squared_error: 2.2352 - val_loss: 4.8785 - val_root_mean_squared_error: 2.2087\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.8171 - root_mean_squared_error: 2.1948 - val_loss: 4.6914 - val_root_mean_squared_error: 2.1660\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.6401 - root_mean_squared_error: 2.1541 - val_loss: 4.5418 - val_root_mean_squared_error: 2.1311\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.4602 - root_mean_squared_error: 2.1119 - val_loss: 4.3130 - val_root_mean_squared_error: 2.0768\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.2844 - root_mean_squared_error: 2.0699 - val_loss: 4.1404 - val_root_mean_squared_error: 2.0348\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.0991 - root_mean_squared_error: 2.0246 - val_loss: 3.9590 - val_root_mean_squared_error: 1.9897\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.9250 - root_mean_squared_error: 1.9812 - val_loss: 3.7942 - val_root_mean_squared_error: 1.9479\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.7428 - root_mean_squared_error: 1.9346 - val_loss: 3.5873 - val_root_mean_squared_error: 1.8940\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.5854 - root_mean_squared_error: 1.8935 - val_loss: 3.4546 - val_root_mean_squared_error: 1.8587\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 3.4301 - root_mean_squared_error: 1.8521 - val_loss: 3.3185 - val_root_mean_squared_error: 1.8217\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.2900 - root_mean_squared_error: 1.8138 - val_loss: 3.1629 - val_root_mean_squared_error: 1.7785\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.1660 - root_mean_squared_error: 1.7793 - val_loss: 3.0462 - val_root_mean_squared_error: 1.7453\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.0589 - root_mean_squared_error: 1.7490 - val_loss: 2.9308 - val_root_mean_squared_error: 1.7119\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.9608 - root_mean_squared_error: 1.7207 - val_loss: 2.8580 - val_root_mean_squared_error: 1.6906\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.8789 - root_mean_squared_error: 1.6967 - val_loss: 2.7644 - val_root_mean_squared_error: 1.6626\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.8017 - root_mean_squared_error: 1.6738 - val_loss: 2.6952 - val_root_mean_squared_error: 1.6417\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.7373 - root_mean_squared_error: 1.6545 - val_loss: 2.6374 - val_root_mean_squared_error: 1.6240\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.6712 - root_mean_squared_error: 1.6344 - val_loss: 2.5950 - val_root_mean_squared_error: 1.6109\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.6239 - root_mean_squared_error: 1.6198 - val_loss: 2.5214 - val_root_mean_squared_error: 1.5879\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5751 - root_mean_squared_error: 1.6047 - val_loss: 2.4729 - val_root_mean_squared_error: 1.5725\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5349 - root_mean_squared_error: 1.5921 - val_loss: 2.4492 - val_root_mean_squared_error: 1.5650\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4933 - root_mean_squared_error: 1.5790 - val_loss: 2.4190 - val_root_mean_squared_error: 1.5553\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4502 - root_mean_squared_error: 1.5653 - val_loss: 2.3667 - val_root_mean_squared_error: 1.5384\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4139 - root_mean_squared_error: 1.5537 - val_loss: 2.3311 - val_root_mean_squared_error: 1.5268\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3774 - root_mean_squared_error: 1.5419 - val_loss: 2.2751 - val_root_mean_squared_error: 1.5083\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3368 - root_mean_squared_error: 1.5287 - val_loss: 2.2484 - val_root_mean_squared_error: 1.4995\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2978 - root_mean_squared_error: 1.5158 - val_loss: 2.2250 - val_root_mean_squared_error: 1.4916\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2712 - root_mean_squared_error: 1.5071 - val_loss: 2.1946 - val_root_mean_squared_error: 1.4814\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2479 - root_mean_squared_error: 1.4993 - val_loss: 2.1817 - val_root_mean_squared_error: 1.4771\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2169 - root_mean_squared_error: 1.4889 - val_loss: 2.1349 - val_root_mean_squared_error: 1.4611\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1856 - root_mean_squared_error: 1.4784 - val_loss: 2.1202 - val_root_mean_squared_error: 1.4561\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1611 - root_mean_squared_error: 1.4701 - val_loss: 2.1083 - val_root_mean_squared_error: 1.4520\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.1316 - root_mean_squared_error: 1.4600 - val_loss: 2.0785 - val_root_mean_squared_error: 1.4417\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1145 - root_mean_squared_error: 1.4541 - val_loss: 2.0445 - val_root_mean_squared_error: 1.4299\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0980 - root_mean_squared_error: 1.4484 - val_loss: 2.0212 - val_root_mean_squared_error: 1.4217\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0748 - root_mean_squared_error: 1.4404 - val_loss: 2.0247 - val_root_mean_squared_error: 1.4229\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0693 - root_mean_squared_error: 1.4385 - val_loss: 1.9920 - val_root_mean_squared_error: 1.4114\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0449 - root_mean_squared_error: 1.4300 - val_loss: 1.9925 - val_root_mean_squared_error: 1.4116\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0306 - root_mean_squared_error: 1.4250 - val_loss: 1.9683 - val_root_mean_squared_error: 1.4030\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0198 - root_mean_squared_error: 1.4212 - val_loss: 1.9699 - val_root_mean_squared_error: 1.4035\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0063 - root_mean_squared_error: 1.4164 - val_loss: 1.9573 - val_root_mean_squared_error: 1.3990\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9988 - root_mean_squared_error: 1.4138 - val_loss: 1.9550 - val_root_mean_squared_error: 1.3982\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9834 - root_mean_squared_error: 1.4083 - val_loss: 1.9382 - val_root_mean_squared_error: 1.3922\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9854 - root_mean_squared_error: 1.4090 - val_loss: 1.9355 - val_root_mean_squared_error: 1.3912\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9668 - root_mean_squared_error: 1.4024 - val_loss: 1.9309 - val_root_mean_squared_error: 1.3896\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9661 - root_mean_squared_error: 1.4022 - val_loss: 1.9224 - val_root_mean_squared_error: 1.3865\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9581 - root_mean_squared_error: 1.3993 - val_loss: 1.9180 - val_root_mean_squared_error: 1.3849\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9578 - root_mean_squared_error: 1.3992 - val_loss: 1.9074 - val_root_mean_squared_error: 1.3811\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9458 - root_mean_squared_error: 1.3949 - val_loss: 1.9186 - val_root_mean_squared_error: 1.3851\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9457 - root_mean_squared_error: 1.3949 - val_loss: 1.8956 - val_root_mean_squared_error: 1.3768\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9451 - root_mean_squared_error: 1.3947 - val_loss: 1.9024 - val_root_mean_squared_error: 1.3793\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9356 - root_mean_squared_error: 1.3913 - val_loss: 1.8856 - val_root_mean_squared_error: 1.3732\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9313 - root_mean_squared_error: 1.3897 - val_loss: 1.8961 - val_root_mean_squared_error: 1.3770\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9308 - root_mean_squared_error: 1.3895 - val_loss: 1.8717 - val_root_mean_squared_error: 1.3681\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9333 - root_mean_squared_error: 1.3904 - val_loss: 1.8865 - val_root_mean_squared_error: 1.3735\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9361 - root_mean_squared_error: 1.3914 - val_loss: 1.8922 - val_root_mean_squared_error: 1.3756\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9240 - root_mean_squared_error: 1.3871 - val_loss: 1.8789 - val_root_mean_squared_error: 1.3707\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9228 - root_mean_squared_error: 1.3866 - val_loss: 1.8772 - val_root_mean_squared_error: 1.3701\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9318 - root_mean_squared_error: 1.3899 - val_loss: 1.8913 - val_root_mean_squared_error: 1.3753\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9176 - root_mean_squared_error: 1.3848 - val_loss: 1.8883 - val_root_mean_squared_error: 1.3741\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9284 - root_mean_squared_error: 1.3887 - val_loss: 1.8454 - val_root_mean_squared_error: 1.3585\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9206 - root_mean_squared_error: 1.3859 - val_loss: 1.8735 - val_root_mean_squared_error: 1.3688\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9161 - root_mean_squared_error: 1.3842 - val_loss: 1.8846 - val_root_mean_squared_error: 1.3728\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9224 - root_mean_squared_error: 1.3865 - val_loss: 1.8873 - val_root_mean_squared_error: 1.3738\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9184 - root_mean_squared_error: 1.3851 - val_loss: 1.8999 - val_root_mean_squared_error: 1.3784\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9161 - root_mean_squared_error: 1.3842 - val_loss: 1.8906 - val_root_mean_squared_error: 1.3750\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9150 - root_mean_squared_error: 1.3838 - val_loss: 1.8724 - val_root_mean_squared_error: 1.3684\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9083 - root_mean_squared_error: 1.3814 - val_loss: 1.9108 - val_root_mean_squared_error: 1.3823\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9167 - root_mean_squared_error: 1.3845 - val_loss: 1.8668 - val_root_mean_squared_error: 1.3663\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9220 - root_mean_squared_error: 1.3864 - val_loss: 1.8826 - val_root_mean_squared_error: 1.3721\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9138 - root_mean_squared_error: 1.3834 - val_loss: 1.8762 - val_root_mean_squared_error: 1.3697\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9198 - root_mean_squared_error: 1.3856 - val_loss: 1.8752 - val_root_mean_squared_error: 1.3694\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9136 - root_mean_squared_error: 1.3833 - val_loss: 1.8923 - val_root_mean_squared_error: 1.3756\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9107 - root_mean_squared_error: 1.3823 - val_loss: 1.8766 - val_root_mean_squared_error: 1.3699\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9101 - root_mean_squared_error: 1.3821 - val_loss: 1.8565 - val_root_mean_squared_error: 1.3625\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9199 - root_mean_squared_error: 1.3856 - val_loss: 1.8760 - val_root_mean_squared_error: 1.3697\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9147 - root_mean_squared_error: 1.3837 - val_loss: 1.8873 - val_root_mean_squared_error: 1.3738\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9021 - root_mean_squared_error: 1.3792 - val_loss: 1.8686 - val_root_mean_squared_error: 1.3670\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9045 - root_mean_squared_error: 1.3801 - val_loss: 1.8598 - val_root_mean_squared_error: 1.3637\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9156 - root_mean_squared_error: 1.3840 - val_loss: 1.8573 - val_root_mean_squared_error: 1.3628\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9082 - root_mean_squared_error: 1.3814 - val_loss: 1.8683 - val_root_mean_squared_error: 1.3668\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9104 - root_mean_squared_error: 1.3822 - val_loss: 1.8682 - val_root_mean_squared_error: 1.3668\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9132 - root_mean_squared_error: 1.3832 - val_loss: 1.8728 - val_root_mean_squared_error: 1.3685\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9036 - root_mean_squared_error: 1.3797 - val_loss: 1.8546 - val_root_mean_squared_error: 1.3618\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9061 - root_mean_squared_error: 1.3806 - val_loss: 1.8626 - val_root_mean_squared_error: 1.3648\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9024 - root_mean_squared_error: 1.3793 - val_loss: 1.8454 - val_root_mean_squared_error: 1.3585\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9095 - root_mean_squared_error: 1.3818 - val_loss: 1.8807 - val_root_mean_squared_error: 1.3714\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9052 - root_mean_squared_error: 1.3803 - val_loss: 1.8604 - val_root_mean_squared_error: 1.3640\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9137 - root_mean_squared_error: 1.3833 - val_loss: 1.8645 - val_root_mean_squared_error: 1.3655\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 1.9066 - root_mean_squared_error: 1.3808 - val_loss: 1.8826 - val_root_mean_squared_error: 1.3721\n",
      "01-12 17:35:52 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 17:35:52 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 17:35:52 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112173420_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 17:35:52 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:35:52 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 17:35:52 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:35:52 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "#########################\n",
      "### Fold 10\n",
      "#########################\n",
      "01-12 17:35:52 I deeptables.m.deeptable.py 338 - X.Shape=(25920, 57), y.Shape=(25920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7da2b82b14e0>, loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=15, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-12 17:35:52 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-12 17:35:52 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:35:52 I hypernets.t.toolbox.py 334 - Target column type is float64, so inferred as a [regression] task.\n",
      "01-12 17:35:52 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-12 17:35:52 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.0227358341217041s\n",
      "01-12 17:35:52 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-12 17:35:52 I deeptables.m.preprocessor.py 383 - Imputation taken 0.0814199447631836s\n",
      "01-12 17:35:52 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-12 17:35:52 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.05626034736633301s\n",
      "01-12 17:35:52 I deeptables.m.preprocessor.py 196 - fit_transform taken 0.2148122787475586s\n",
      "01-12 17:35:52 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-12 17:35:52 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:15, mode:min\n",
      "01-12 17:35:52 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:35:52 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "01-12 17:35:52 I deeptables.m.deepmodel.py 231 - Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-12 17:35:53 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (35)', 'input_continuous_all: (22)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [14, 6, 10, 6, 10, 6, 4, 5, 6, 6, 20, 7, 11, 5, 4, 8, 9, 6, 6, 5, 5, 7, 6, 6, 6, 20, 6, 7, 8, 6, 7, 6, 5, 6, 6]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 162)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 57), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: True\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-12 17:35:53 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 6s 7ms/step - loss: 25301.4375 - root_mean_squared_error: 159.0643 - val_loss: 303.1257 - val_root_mean_squared_error: 17.4105\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 147.2352 - root_mean_squared_error: 12.1341 - val_loss: 127.5409 - val_root_mean_squared_error: 11.2934\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 125.8934 - root_mean_squared_error: 11.2202 - val_loss: 121.2370 - val_root_mean_squared_error: 11.0108\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 120.1210 - root_mean_squared_error: 10.9600 - val_loss: 115.7920 - val_root_mean_squared_error: 10.7607\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 113.9249 - root_mean_squared_error: 10.6736 - val_loss: 108.8779 - val_root_mean_squared_error: 10.4345\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 107.1137 - root_mean_squared_error: 10.3496 - val_loss: 100.4351 - val_root_mean_squared_error: 10.0217\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 100.2118 - root_mean_squared_error: 10.0106 - val_loss: 94.5339 - val_root_mean_squared_error: 9.7229\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 93.2619 - root_mean_squared_error: 9.6572 - val_loss: 87.6685 - val_root_mean_squared_error: 9.3631\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 86.4822 - root_mean_squared_error: 9.2996 - val_loss: 81.3064 - val_root_mean_squared_error: 9.0170\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 79.9370 - root_mean_squared_error: 8.9407 - val_loss: 74.4838 - val_root_mean_squared_error: 8.6304\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 73.6670 - root_mean_squared_error: 8.5829 - val_loss: 69.3446 - val_root_mean_squared_error: 8.3273\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 67.7784 - root_mean_squared_error: 8.2328 - val_loss: 63.5327 - val_root_mean_squared_error: 7.9707\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 62.2805 - root_mean_squared_error: 7.8918 - val_loss: 58.4580 - val_root_mean_squared_error: 7.6458\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 56.8632 - root_mean_squared_error: 7.5408 - val_loss: 52.4917 - val_root_mean_squared_error: 7.2451\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 51.8279 - root_mean_squared_error: 7.1992 - val_loss: 48.3990 - val_root_mean_squared_error: 6.9569\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 46.9378 - root_mean_squared_error: 6.8511 - val_loss: 43.5173 - val_root_mean_squared_error: 6.5968\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 42.4361 - root_mean_squared_error: 6.5143 - val_loss: 38.9155 - val_root_mean_squared_error: 6.2382\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 37.9832 - root_mean_squared_error: 6.1631 - val_loss: 35.0977 - val_root_mean_squared_error: 5.9243\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 33.7736 - root_mean_squared_error: 5.8115 - val_loss: 30.9507 - val_root_mean_squared_error: 5.5633\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 29.9443 - root_mean_squared_error: 5.4721 - val_loss: 27.4524 - val_root_mean_squared_error: 5.2395\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 26.3652 - root_mean_squared_error: 5.1347 - val_loss: 23.9283 - val_root_mean_squared_error: 4.8917\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 22.9659 - root_mean_squared_error: 4.7923 - val_loss: 20.4566 - val_root_mean_squared_error: 4.5229\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 19.9202 - root_mean_squared_error: 4.4632 - val_loss: 17.6730 - val_root_mean_squared_error: 4.2039\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 17.1171 - root_mean_squared_error: 4.1373 - val_loss: 15.1742 - val_root_mean_squared_error: 3.8954\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 14.7182 - root_mean_squared_error: 3.8364 - val_loss: 12.8290 - val_root_mean_squared_error: 3.5818\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 12.4763 - root_mean_squared_error: 3.5322 - val_loss: 10.9668 - val_root_mean_squared_error: 3.3116\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 10.5497 - root_mean_squared_error: 3.2480 - val_loss: 9.2497 - val_root_mean_squared_error: 3.0413\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 8.9300 - root_mean_squared_error: 2.9883 - val_loss: 7.7514 - val_root_mean_squared_error: 2.7841\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 7.5655 - root_mean_squared_error: 2.7505 - val_loss: 6.5654 - val_root_mean_squared_error: 2.5623\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 6.4131 - root_mean_squared_error: 2.5324 - val_loss: 5.5277 - val_root_mean_squared_error: 2.3511\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 5.4437 - root_mean_squared_error: 2.3332 - val_loss: 4.7230 - val_root_mean_squared_error: 2.1733\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 4.7073 - root_mean_squared_error: 2.1696 - val_loss: 4.1162 - val_root_mean_squared_error: 2.0288\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 4.1012 - root_mean_squared_error: 2.0251 - val_loss: 3.5862 - val_root_mean_squared_error: 1.8937\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.6382 - root_mean_squared_error: 1.9074 - val_loss: 3.2598 - val_root_mean_squared_error: 1.8055\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.3034 - root_mean_squared_error: 1.8175 - val_loss: 2.9365 - val_root_mean_squared_error: 1.7136\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 3.0290 - root_mean_squared_error: 1.7404 - val_loss: 2.7335 - val_root_mean_squared_error: 1.6533\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.8407 - root_mean_squared_error: 1.6854 - val_loss: 2.5861 - val_root_mean_squared_error: 1.6081\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.6742 - root_mean_squared_error: 1.6353 - val_loss: 2.4629 - val_root_mean_squared_error: 1.5694\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5883 - root_mean_squared_error: 1.6088 - val_loss: 2.3784 - val_root_mean_squared_error: 1.5422\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.5109 - root_mean_squared_error: 1.5846 - val_loss: 2.3112 - val_root_mean_squared_error: 1.5203\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.4641 - root_mean_squared_error: 1.5697 - val_loss: 2.2832 - val_root_mean_squared_error: 1.5110\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3999 - root_mean_squared_error: 1.5491 - val_loss: 2.2463 - val_root_mean_squared_error: 1.4988\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3488 - root_mean_squared_error: 1.5326 - val_loss: 2.2531 - val_root_mean_squared_error: 1.5010\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.3071 - root_mean_squared_error: 1.5189 - val_loss: 2.2003 - val_root_mean_squared_error: 1.4833\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2992 - root_mean_squared_error: 1.5163 - val_loss: 2.1623 - val_root_mean_squared_error: 1.4705\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2530 - root_mean_squared_error: 1.5010 - val_loss: 2.1541 - val_root_mean_squared_error: 1.4677\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.2363 - root_mean_squared_error: 1.4954 - val_loss: 2.1376 - val_root_mean_squared_error: 1.4621\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2253 - root_mean_squared_error: 1.4917 - val_loss: 2.1510 - val_root_mean_squared_error: 1.4666\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.2137 - root_mean_squared_error: 1.4879 - val_loss: 2.1545 - val_root_mean_squared_error: 1.4678\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1976 - root_mean_squared_error: 1.4824 - val_loss: 2.0863 - val_root_mean_squared_error: 1.4444\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1540 - root_mean_squared_error: 1.4676 - val_loss: 2.0863 - val_root_mean_squared_error: 1.4444\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1417 - root_mean_squared_error: 1.4635 - val_loss: 2.0915 - val_root_mean_squared_error: 1.4462\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1235 - root_mean_squared_error: 1.4572 - val_loss: 2.1675 - val_root_mean_squared_error: 1.4723\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1224 - root_mean_squared_error: 1.4569 - val_loss: 2.0398 - val_root_mean_squared_error: 1.4282\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.1169 - root_mean_squared_error: 1.4550 - val_loss: 2.0698 - val_root_mean_squared_error: 1.4387\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0962 - root_mean_squared_error: 1.4478 - val_loss: 2.3032 - val_root_mean_squared_error: 1.5176\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0895 - root_mean_squared_error: 1.4455 - val_loss: 2.0242 - val_root_mean_squared_error: 1.4227\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0743 - root_mean_squared_error: 1.4403 - val_loss: 2.0510 - val_root_mean_squared_error: 1.4321\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0550 - root_mean_squared_error: 1.4335 - val_loss: 2.0024 - val_root_mean_squared_error: 1.4151\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0676 - root_mean_squared_error: 1.4379 - val_loss: 2.0014 - val_root_mean_squared_error: 1.4147\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0584 - root_mean_squared_error: 1.4347 - val_loss: 2.0850 - val_root_mean_squared_error: 1.4440\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0658 - root_mean_squared_error: 1.4373 - val_loss: 2.0135 - val_root_mean_squared_error: 1.4190\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0221 - root_mean_squared_error: 1.4220 - val_loss: 1.9643 - val_root_mean_squared_error: 1.4015\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0219 - root_mean_squared_error: 1.4219 - val_loss: 1.9503 - val_root_mean_squared_error: 1.3965\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0262 - root_mean_squared_error: 1.4234 - val_loss: 2.0654 - val_root_mean_squared_error: 1.4372\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0321 - root_mean_squared_error: 1.4255 - val_loss: 2.0271 - val_root_mean_squared_error: 1.4238\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9923 - root_mean_squared_error: 1.4115 - val_loss: 1.9887 - val_root_mean_squared_error: 1.4102\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0007 - root_mean_squared_error: 1.4145 - val_loss: 2.0211 - val_root_mean_squared_error: 1.4216\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0000 - root_mean_squared_error: 1.4142 - val_loss: 1.9467 - val_root_mean_squared_error: 1.3953\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0222 - root_mean_squared_error: 1.4220 - val_loss: 2.5222 - val_root_mean_squared_error: 1.5882\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0458 - root_mean_squared_error: 1.4303 - val_loss: 2.2306 - val_root_mean_squared_error: 1.4935\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0687 - root_mean_squared_error: 1.4383 - val_loss: 1.9850 - val_root_mean_squared_error: 1.4089\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9808 - root_mean_squared_error: 1.4074 - val_loss: 1.9343 - val_root_mean_squared_error: 1.3908\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9630 - root_mean_squared_error: 1.4011 - val_loss: 1.9472 - val_root_mean_squared_error: 1.3954\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0245 - root_mean_squared_error: 1.4229 - val_loss: 2.2091 - val_root_mean_squared_error: 1.4863\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0153 - root_mean_squared_error: 1.4196 - val_loss: 2.1438 - val_root_mean_squared_error: 1.4642\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9723 - root_mean_squared_error: 1.4044 - val_loss: 1.9484 - val_root_mean_squared_error: 1.3959\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0028 - root_mean_squared_error: 1.4152 - val_loss: 1.9380 - val_root_mean_squared_error: 1.3921\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0301 - root_mean_squared_error: 1.4248 - val_loss: 1.9278 - val_root_mean_squared_error: 1.3885\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0034 - root_mean_squared_error: 1.4154 - val_loss: 1.9833 - val_root_mean_squared_error: 1.4083\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0057 - root_mean_squared_error: 1.4162 - val_loss: 1.9251 - val_root_mean_squared_error: 1.3875\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9645 - root_mean_squared_error: 1.4016 - val_loss: 2.1552 - val_root_mean_squared_error: 1.4681\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0006 - root_mean_squared_error: 1.4144 - val_loss: 1.9324 - val_root_mean_squared_error: 1.3901\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 2.0088 - root_mean_squared_error: 1.4173 - val_loss: 1.9675 - val_root_mean_squared_error: 1.4027\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0107 - root_mean_squared_error: 1.4180 - val_loss: 2.0212 - val_root_mean_squared_error: 1.4217\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9945 - root_mean_squared_error: 1.4123 - val_loss: 1.9878 - val_root_mean_squared_error: 1.4099\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0390 - root_mean_squared_error: 1.4279 - val_loss: 1.9525 - val_root_mean_squared_error: 1.3973\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0083 - root_mean_squared_error: 1.4171 - val_loss: 2.0463 - val_root_mean_squared_error: 1.4305\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0168 - root_mean_squared_error: 1.4201 - val_loss: 1.9912 - val_root_mean_squared_error: 1.4111\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0405 - root_mean_squared_error: 1.4285 - val_loss: 1.9226 - val_root_mean_squared_error: 1.3866\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9839 - root_mean_squared_error: 1.4085 - val_loss: 2.0797 - val_root_mean_squared_error: 1.4421\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0252 - root_mean_squared_error: 1.4231 - val_loss: 2.1291 - val_root_mean_squared_error: 1.4591\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0182 - root_mean_squared_error: 1.4206 - val_loss: 2.0082 - val_root_mean_squared_error: 1.4171\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9837 - root_mean_squared_error: 1.4084 - val_loss: 1.9741 - val_root_mean_squared_error: 1.4050\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9905 - root_mean_squared_error: 1.4109 - val_loss: 1.9434 - val_root_mean_squared_error: 1.3941\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0550 - root_mean_squared_error: 1.4335 - val_loss: 2.3536 - val_root_mean_squared_error: 1.5341\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9799 - root_mean_squared_error: 1.4071 - val_loss: 1.9491 - val_root_mean_squared_error: 1.3961\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9726 - root_mean_squared_error: 1.4045 - val_loss: 1.9427 - val_root_mean_squared_error: 1.3938\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 1.9686 - root_mean_squared_error: 1.4031 - val_loss: 1.9336 - val_root_mean_squared_error: 1.3905\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 2.0634 - root_mean_squared_error: 1.4365 - val_loss: 2.0352 - val_root_mean_squared_error: 1.4266\n",
      "01-12 17:37:20 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-12 17:37:20 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-12 17:37:20 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20250112173552_linear/linear.h5\n",
      "predicting on valid data\n",
      "01-12 17:37:20 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:37:20 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/deeptables/models/deepmodel.py:188: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(self.model, h, save_format='h5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test data\n",
      "01-12 17:37:21 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-12 17:37:21 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "CPU times: user 26min 45s, sys: 1min 28s, total: 28min 13s\n",
      "Wall time: 15min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FOLDS = 10\n",
    "    \n",
    "oof_lgb  = np.zeros(len(train_df))\n",
    "pred_lgb = np.zeros(len(test_df))\n",
    "\n",
    "models = {}\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    \n",
    "    print(\"#\"*25)\n",
    "    print(f\"### Fold {fold+1}\")\n",
    "    print(\"#\"*25)\n",
    "\n",
    "    x_train = train_df[train_df['fold']!=fold][FEATURES].copy()\n",
    "    y_train = train_df[train_df['fold']!=fold][\"y\"].copy()\n",
    "    x_valid = train_df[train_df['fold']==fold][FEATURES].copy()\n",
    "    y_valid = train_df[train_df['fold']==fold][\"y\"].copy()\n",
    "    x_test  = test_df[FEATURES].copy()\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "    config      = deeptable.ModelConfig(\n",
    "                                            categorical_columns='auto',\n",
    "                                            nets =['linear'], \n",
    "                                            stacking_op = 'add', \n",
    "                                            earlystopping_patience=15, \n",
    "                                            metrics=[\"RootMeanSquaredError\"],\n",
    "                                            optimizer=optimizer\n",
    "                                        )\n",
    "\n",
    "    dt             = deeptable.DeepTable(config=config)\n",
    "    model, history = dt.fit(x_train, y_train, epochs=100)\n",
    "\n",
    "    # INFER OOF\n",
    "    print(f\"predicting on valid data\")\n",
    "    model_out              = model.predict(x_valid)\n",
    "    oof_lgb[x_valid.index] = model_out.flatten()\n",
    "    # INFER TEST\n",
    "    print(f\"predicting on test data\")\n",
    "    pred_lgb              += (model.predict(x_test)).flatten()\n",
    "\n",
    "    models[fold] = model\n",
    "\n",
    "# COMPUTE AVERAGE TEST PREDS\n",
    "pred_lgb /= FOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e9f59",
   "metadata": {
    "papermill": {
     "duration": 0.972738,
     "end_time": "2025-01-12T17:37:23.188202",
     "exception": false,
     "start_time": "2025-01-12T17:37:22.215464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 7: Model score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "438df466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:37:25.214031Z",
     "iopub.status.busy": "2025-01-12T17:37:25.213522Z",
     "iopub.status.idle": "2025-01-12T17:37:25.372969Z",
     "shell.execute_reply": "2025-01-12T17:37:25.371970Z"
    },
    "papermill": {
     "duration": 1.173357,
     "end_time": "2025-01-12T17:37:25.375013",
     "exception": false,
     "start_time": "2025-01-12T17:37:24.201656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To evaluate the equitable prediction of transplant survival outcomes,\n",
    "we use the concordance index (C-index) between a series of event\n",
    "times and a predicted score across each race group.\n",
    " \n",
    "It represents the global assessment of the model discrimination power:\n",
    "this is the model’s ability to correctly provide a reliable ranking\n",
    "of the survival times based on the individual risk scores.\n",
    " \n",
    "The concordance index is a value between 0 and 1 where:\n",
    " \n",
    "0.5 is the expected result from random predictions,\n",
    "1.0 is perfect concordance (with no censoring, otherwise <1.0),\n",
    "0.0 is perfect anti-concordance (with no censoring, otherwise >0.0)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import numpy as np\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = \"id\"\n",
    "    >>> y_pred = {'prediction': {0: 1.0, 1: 0.0, 2: 1.0}}\n",
    "    >>> y_pred = pd.DataFrame(y_pred)\n",
    "    >>> y_pred.insert(0, row_id_column_name, range(len(y_pred)))\n",
    "    >>> y_true = { 'efs': {0: 1.0, 1: 0.0, 2: 0.0}, 'efs_time': {0: 25.1234,1: 250.1234,2: 2500.1234}, 'race_group': {0: 'race_group_1', 1: 'race_group_1', 2: 'race_group_1'}}\n",
    "    >>> y_true = pd.DataFrame(y_true)\n",
    "    >>> y_true.insert(0, row_id_column_name, range(len(y_true)))\n",
    "    >>> score(y_true.copy(), y_pred.copy(), row_id_column_name)\n",
    "    0.75\n",
    "    \"\"\"\n",
    "    \n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "    \n",
    "    event_label = 'efs'\n",
    "    interval_label = 'efs_time'\n",
    "    prediction_label = 'prediction'\n",
    "    for col in submission.columns:\n",
    "        if not pandas.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "    # Merging solution and submission dfs on ID\n",
    "    merged_df = pd.concat([solution, submission], axis=1)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n",
    "    metric_list = []\n",
    "    for race in merged_df_race_dict.keys():\n",
    "        # Retrieving values from y_test based on index\n",
    "        indices = sorted(merged_df_race_dict[race])\n",
    "        merged_df_race = merged_df.iloc[indices]\n",
    "        # Calculate the concordance index\n",
    "        c_index_race = concordance_index(\n",
    "                        merged_df_race[interval_label],\n",
    "                        -merged_df_race[prediction_label],\n",
    "                        merged_df_race[event_label])\n",
    "        metric_list.append(c_index_race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797bd36",
   "metadata": {
    "papermill": {
     "duration": 0.933959,
     "end_time": "2025-01-12T17:37:27.316322",
     "exception": false,
     "start_time": "2025-01-12T17:37:26.382363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 8: Model CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "096b22b5",
   "metadata": {
    "_cell_guid": "a16eceda-8973-4c17-8f13-0423fe0cb225",
    "_uuid": "ee05ecfe-1315-47c7-aac9-090b3cd60b56",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-12T17:37:29.462911Z",
     "iopub.status.busy": "2025-01-12T17:37:29.462470Z",
     "iopub.status.idle": "2025-01-12T17:37:29.859233Z",
     "shell.execute_reply": "2025-01-12T17:37:29.858114Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.455029,
     "end_time": "2025-01-12T17:37:29.860961",
     "exception": false,
     "start_time": "2025-01-12T17:37:28.405932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-a1aa375cd29b>:53: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall CV for DeepTables = 0.6422624332777372\n"
     ]
    }
   ],
   "source": [
    "y_true = train_df[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "y_pred = train_df[[\"ID\"]].copy()\n",
    "y_pred[\"prediction\"] = oof_lgb\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(f\"\\nOverall CV for DeepTables =\",m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e55b84",
   "metadata": {
    "papermill": {
     "duration": 0.992485,
     "end_time": "2025-01-12T17:37:31.788753",
     "exception": false,
     "start_time": "2025-01-12T17:37:30.796268",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 9: Create submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25e589fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:37:33.768642Z",
     "iopub.status.busy": "2025-01-12T17:37:33.768269Z",
     "iopub.status.idle": "2025-01-12T17:37:33.789625Z",
     "shell.execute_reply": "2025-01-12T17:37:33.788648Z"
    },
    "papermill": {
     "duration": 0.993367,
     "end_time": "2025-01-12T17:37:33.791260",
     "exception": false,
     "start_time": "2025-01-12T17:37:32.797893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub shape: (3, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28800</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28801</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28802</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  prediction\n",
       "0  28800         2.0\n",
       "1  28801         3.0\n",
       "2  28802         1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n",
    "sub.prediction = rankdata(pred_lgb) \n",
    "sub.to_csv(\"submission.csv\",index=False)\n",
    "print(\"Sub shape:\",sub.shape)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8f28a",
   "metadata": {
    "papermill": {
     "duration": 1.000939,
     "end_time": "2025-01-12T17:37:35.790554",
     "exception": false,
     "start_time": "2025-01-12T17:37:34.789615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 10: Save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "520b521f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:37:37.736578Z",
     "iopub.status.busy": "2025-01-12T17:37:37.736161Z",
     "iopub.status.idle": "2025-01-12T17:37:38.511042Z",
     "shell.execute_reply": "2025-01-12T17:37:38.509874Z"
    },
    "papermill": {
     "duration": 1.784095,
     "end_time": "2025-01-12T17:37:38.513137",
     "exception": false,
     "start_time": "2025-01-12T17:37:36.729042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for fold, model in models.items():\n",
    "    model.model.save(f\"model_fold_{fold}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2378036d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:37:40.533239Z",
     "iopub.status.busy": "2025-01-12T17:37:40.532868Z",
     "iopub.status.idle": "2025-01-12T17:37:40.536897Z",
     "shell.execute_reply": "2025-01-12T17:37:40.535815Z"
    },
    "papermill": {
     "duration": 1.018178,
     "end_time": "2025-01-12T17:37:40.538757",
     "exception": false,
     "start_time": "2025-01-12T17:37:39.520579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('tabm_exp_01.pkl', 'wb') as f:\n",
    "#     pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b8325a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T17:37:42.476603Z",
     "iopub.status.busy": "2025-01-12T17:37:42.476210Z",
     "iopub.status.idle": "2025-01-12T17:38:09.933435Z",
     "shell.execute_reply": "2025-01-12T17:38:09.932234Z"
    },
    "papermill": {
     "duration": 28.47067,
     "end_time": "2025-01-12T17:38:09.935641",
     "exception": false,
     "start_time": "2025-01-12T17:37:41.464971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['predictions'] = oof_lgb\n",
    "train_df.to_excel(\"dt_exp_01_oof.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86600d6a",
   "metadata": {
    "papermill": {
     "duration": 1.007125,
     "end_time": "2025-01-12T17:38:11.879452",
     "exception": false,
     "start_time": "2025-01-12T17:38:10.872327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    },
    {
     "datasetId": 4074593,
     "sourceId": 7074842,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4021289,
     "sourceId": 7570020,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6415434,
     "sourceId": 10370860,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 211322530,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1121.118564,
   "end_time": "2025-01-12T17:38:16.178680",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-12T17:19:35.060116",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
