{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e9b067",
   "metadata": {
    "_cell_guid": "46a264b2-b97f-487b-9a3d-2597a0ecdc21",
    "_uuid": "9daabf89-8847-4537-a0b7-2a722b5c452d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009525,
     "end_time": "2025-03-02T11:20:24.500545",
     "exception": false,
     "start_time": "2025-03-02T11:20:24.491020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a847e46d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:20:24.518916Z",
     "iopub.status.busy": "2025-03-02T11:20:24.518573Z",
     "iopub.status.idle": "2025-03-02T11:20:44.143285Z",
     "shell.execute_reply": "2025-03-02T11:20:44.142236Z"
    },
    "papermill": {
     "duration": 19.636047,
     "end_time": "2025-03-02T11:20:44.145172",
     "exception": false,
     "start_time": "2025-03-02T11:20:24.509125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd==1.7.0) (1.26.4)\r\n",
      "autograd is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Processing /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: autograd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.7.0)\r\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.13.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd>=1.2.0->autograd-gamma==0.5.0) (1.26.4)\r\n",
      "Building wheels for collected packages: autograd-gamma\r\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=e57de1b7de0f71c89022a757b5ad5ceae1f0e2591cf7ad298a397f80609fd1fe\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/b5/e0/4c79e15c0b5f2c15ecf613c720bb20daab20a666eb67135155\r\n",
      "Successfully built autograd-gamma\r\n",
      "Installing collected packages: autograd-gamma\r\n",
      "Successfully installed autograd-gamma-0.5.0\r\n",
      "Processing /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\r\n",
      "Installing collected packages: interface-meta\r\n",
      "Successfully installed interface-meta-1.3.0\r\n",
      "Processing /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (2.1.4)\r\n",
      "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->formulaic==1.0.2) (1.16.0)\r\n",
      "Installing collected packages: formulaic\r\n",
      "Successfully installed formulaic-1.0.2\r\n",
      "Processing /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.13.1)\r\n",
      "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (2.1.4)\r\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (3.7.1)\r\n",
      "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.7.0)\r\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (0.5.0)\r\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.0.2)\r\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.16.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (4.53.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (24.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (3.1.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines==0.30.0) (1.16.0)\r\n",
      "Installing collected packages: lifelines\r\n",
      "Successfully installed lifelines-0.30.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
    "!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22fde96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:20:44.165038Z",
     "iopub.status.busy": "2025-03-02T11:20:44.164773Z",
     "iopub.status.idle": "2025-03-02T11:20:50.635290Z",
     "shell.execute_reply": "2025-03-02T11:20:50.634300Z"
    },
    "papermill": {
     "duration": 6.48205,
     "end_time": "2025-03-02T11:20:50.637004",
     "exception": false,
     "start_time": "2025-03-02T11:20:44.154954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/tabm-tabular-dl-library\r\n",
      "Processing /kaggle/input/tabm-tabular-dl-library/tabm-0.0.1.dev0-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch<3,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tabm==0.0.1.dev0) (2.4.1+cu121)\r\n",
      "Processing /kaggle/input/tabm-tabular-dl-library/rtdl_num_embeddings-0.0.11-py3-none-any.whl (from tabm==0.0.1.dev0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.12->tabm==0.0.1.dev0) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1.12->tabm==0.0.1.dev0) (1.3.0)\r\n",
      "Installing collected packages: rtdl_num_embeddings, tabm\r\n",
      "Successfully installed rtdl_num_embeddings-0.0.11 tabm-0.0.1.dev0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index -U --find-links=/kaggle/input/tabm-tabular-dl-library tabm==0.0.1.dev0\n",
    "!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc994bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:20:50.658186Z",
     "iopub.status.busy": "2025-03-02T11:20:50.657896Z",
     "iopub.status.idle": "2025-03-02T11:20:53.917104Z",
     "shell.execute_reply": "2025-03-02T11:20:53.916136Z"
    },
    "papermill": {
     "duration": 3.272049,
     "end_time": "2025-03-02T11:20:53.919152",
     "exception": false,
     "start_time": "2025-03-02T11:20:50.647103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install /kaggle/input/tabpfn-v2/tabpfn-2.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93905a",
   "metadata": {
    "papermill": {
     "duration": 0.009901,
     "end_time": "2025-03-02T11:20:53.938961",
     "exception": false,
     "start_time": "2025-03-02T11:20:53.929060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1 : Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6905535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:20:53.958864Z",
     "iopub.status.busy": "2025-03-02T11:20:53.958547Z",
     "iopub.status.idle": "2025-03-02T11:21:07.531913Z",
     "shell.execute_reply": "2025-03-02T11:21:07.531169Z"
    },
    "papermill": {
     "duration": 13.585243,
     "end_time": "2025-03-02T11:21:07.533478",
     "exception": false,
     "start_time": "2025-03-02T11:20:53.948235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata \n",
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/tabm-tabular-dl-library')\n",
    "\n",
    "import os\n",
    "import tabm\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rtdl_num_embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import rankdata \n",
    "from colorama import Fore, Style\n",
    "from typing import Optional, Tuple\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from tabm_reference import Model, make_parameter_groups\n",
    "from sklearn.preprocessing import OrdinalEncoder, QuantileTransformer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.layers import Concatenate, BatchNormalization\n",
    "import tensorflow.keras.backend as K\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from tabpfn import TabPFNRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c2edb",
   "metadata": {
    "papermill": {
     "duration": 0.009338,
     "end_time": "2025-03-02T11:21:07.553253",
     "exception": false,
     "start_time": "2025-03-02T11:21:07.543915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Experiments Paths  to add in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60cb7633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:21:07.574092Z",
     "iopub.status.busy": "2025-03-02T11:21:07.573427Z",
     "iopub.status.idle": "2025-03-02T11:21:07.578480Z",
     "shell.execute_reply": "2025-03-02T11:21:07.577584Z"
    },
    "papermill": {
     "duration": 0.016855,
     "end_time": "2025-03-02T11:21:07.579916",
     "exception": false,
     "start_time": "2025-03-02T11:21:07.563061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments = [\n",
    "\"/kaggle/input/xgboost-exp-01\",\n",
    "\"/kaggle/input/catboost-exp-01\", \n",
    "\"/kaggle/input/lgbm-exp-01\",\n",
    "\"/kaggle/input/xgboost-exp-02\",\n",
    "\"/kaggle/input/catboost-exp-02\",\n",
    "\"/kaggle/input/lgbm-exp-03\",\n",
    "\"/kaggle/input/catboost-exp-03\",\n",
    "\"/kaggle/input/tabm-exp-01\",\n",
    "\"/kaggle/input/nn-exp-01\",\n",
    "\"/kaggle/input/tn-exp-01\",\n",
    "\"/kaggle/input/tf-exp-01\",\n",
    "\"/kaggle/input/svr-exp-01\",\n",
    "\"/kaggle/input/abd-exp-01\",\n",
    "\"/kaggle/input/catboost-exp-04\",\n",
    "\"/kaggle/input/lgbm-exp-04\",\n",
    "\"/kaggle/input/tabm-exp-02\",\n",
    "\"/kaggle/input/ds-exp-01\",\n",
    "\"/kaggle/input/nn-exp-02\",\n",
    "\"/kaggle/input/nn-exp-04\",\n",
    "#\"/kaggle/input/tabm-exp-03\",\n",
    "\"/kaggle/input/catboost-exp-05\",\n",
    "\"/kaggle/input/xgboost-exp-05\",\n",
    "\"/kaggle/input/lgbm-exp-05\",\n",
    "\"/kaggle/input/tn-exp-02\",\n",
    "#\"/kaggle/input/ag-exp-01\",\n",
    "\"/kaggle/input/vr-exp-01\",\n",
    "\"/kaggle/input/tt-exp-01\",\n",
    "\"/kaggle/input/en-exp-01\",\n",
    "\"/kaggle/input/en-exp-02\",\n",
    "\"/kaggle/input/nn-exp-05\",\n",
    "\"/kaggle/input/rf-exp-05\",\n",
    "\"/kaggle/input/mcts-exp-02\",\n",
    "\"/kaggle/input/catboost-exp-06\",\n",
    "\"/kaggle/input/xgboost-exp-06\",\n",
    "\"/kaggle/input/lgbm-exp-06\",\n",
    "# \"/kaggle/input/nn-exp-06\",\n",
    "# \"/kaggle/input/xgboost-exp-07\",\n",
    "\"/kaggle/input/ri-exp-01\",\n",
    "# \"/kaggle/input/xgboost-exp-08\",\n",
    "# \"/kaggle/input/catboost-exp-08\",\n",
    "# \"/kaggle/input/lgbm-exp-08\",\n",
    "\"/kaggle/input/xgboost-exp-09\",\n",
    "\"/kaggle/input/prlnn-exp-01\",\n",
    "\"/kaggle/input/ri-exp-06\",\n",
    "\"/kaggle/input/xgboost-exp-10\"\n",
    "\n",
    "# \"/kaggle/input/suv-ran-exp-01\" -> File not found\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c71f6a",
   "metadata": {
    "papermill": {
     "duration": 0.009527,
     "end_time": "2025-03-02T11:21:07.599475",
     "exception": false,
     "start_time": "2025-03-02T11:21:07.589948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 3 : Competition metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9735ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:21:07.619451Z",
     "iopub.status.busy": "2025-03-02T11:21:07.619123Z",
     "iopub.status.idle": "2025-03-02T11:21:07.714209Z",
     "shell.execute_reply": "2025-03-02T11:21:07.713461Z"
    },
    "papermill": {
     "duration": 0.106785,
     "end_time": "2025-03-02T11:21:07.715805",
     "exception": false,
     "start_time": "2025-03-02T11:21:07.609020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import numpy as np\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "    \n",
    "    event_label = 'efs'\n",
    "    interval_label = 'efs_time'\n",
    "    prediction_label = 'predictions'\n",
    "    for col in submission.columns:\n",
    "        if not pandas.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "    # Merging solution and submission dfs on ID\n",
    "    merged_df = pd.concat([solution, submission], axis=1)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n",
    "    metric_list = []\n",
    "\n",
    "    for race in merged_df_race_dict.keys():\n",
    "        # Retrieving values from y_test based on index\n",
    "        indices = sorted(merged_df_race_dict[race])\n",
    "        merged_df_race = merged_df.iloc[indices]\n",
    "        # Calculate the concordance index\n",
    "        c_index_race = concordance_index(\n",
    "                        merged_df_race[interval_label],\n",
    "                        -merged_df_race[prediction_label],\n",
    "                        merged_df_race[event_label])\n",
    "        metric_list.append(c_index_race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f564e2c",
   "metadata": {
    "papermill": {
     "duration": 0.009162,
     "end_time": "2025-03-02T11:21:07.734948",
     "exception": false,
     "start_time": "2025-03-02T11:21:07.725786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4: Find best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8d06f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:21:07.754406Z",
     "iopub.status.busy": "2025-03-02T11:21:07.754128Z",
     "iopub.status.idle": "2025-03-02T11:21:07.758198Z",
     "shell.execute_reply": "2025-03-02T11:21:07.757535Z"
    },
    "papermill": {
     "duration": 0.015189,
     "end_time": "2025-03-02T11:21:07.759392",
     "exception": false,
     "start_time": "2025-03-02T11:21:07.744203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parquet_experiments = [\n",
    "    \"/kaggle/input/abd-exp-01\",\n",
    "    \"/kaggle/input/lgbm-exp-04\", \n",
    "    \"/kaggle/input/catboost-exp-01\", \n",
    "    \"/kaggle/input/lgbm-exp-01\", \n",
    "    \"/kaggle/input/lgbm-exp-03\", \n",
    "    \"/kaggle/input/ds-exp-01\",\n",
    "    \"/kaggle/input/nn-exp-02\",\n",
    "    \"/kaggle/input/nn-exp-04\",\n",
    "    \"/kaggle/input/tabm-exp-03\",\n",
    "    \"/kaggle/input/catboost-exp-05\",\n",
    "    \"/kaggle/input/xgboost-exp-05\",\n",
    "    \"/kaggle/input/lgbm-exp-05\",\n",
    "    \"/kaggle/input/tn-exp-02\",\n",
    "    #\"/kaggle/input/ag-exp-01\",\n",
    "    \"/kaggle/input/vr-exp-01\",\n",
    "    \"/kaggle/input/tt-exp-01\",\n",
    "    \"/kaggle/input/en-exp-01\",\n",
    "    \"/kaggle/input/svr-exp-01\",\n",
    "    \"/kaggle/input/en-exp-02\",\n",
    "    \"/kaggle/input/nn-exp-05\",\n",
    "    \"/kaggle/input/rf-exp-05\",\n",
    "    \"/kaggle/input/mcts-exp-02\",\n",
    "    \"/kaggle/input/catboost-exp-06\",\n",
    "    \"/kaggle/input/xgboost-exp-06\",\n",
    "    \"/kaggle/input/lgbm-exp-06\",\n",
    "    \"/kaggle/input/nn-exp-06\",\n",
    "    \"/kaggle/input/xgboost-exp-07\",\n",
    "    \"/kaggle/input/ri-exp-01\",\n",
    "    \"/kaggle/input/xgboost-exp-08\",\n",
    "    \"/kaggle/input/catboost-exp-08\",\n",
    "    \"/kaggle/input/lgbm-exp-08\",\n",
    "    \"/kaggle/input/xgboost-exp-09\",\n",
    "    \"/kaggle/input/prlnn-exp-01\",\n",
    "    \"/kaggle/input/ri-exp-06\",\n",
    "    \"/kaggle/input/xgboost-exp-10\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6683c3de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:21:07.778736Z",
     "iopub.status.busy": "2025-03-02T11:21:07.778469Z",
     "iopub.status.idle": "2025-03-02T11:21:07.782016Z",
     "shell.execute_reply": "2025-03-02T11:21:07.781388Z"
    },
    "papermill": {
     "duration": 0.0146,
     "end_time": "2025-03-02T11:21:07.783307",
     "exception": false,
     "start_time": "2025-03-02T11:21:07.768707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metric_cindex(oof, exp_name):    \n",
    "    y_true = oof[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "    y_pred = oof[[\"ID\",\"predictions\"]].copy()\n",
    "    return score(y_true.copy(), y_pred.copy(), \"ID\"), oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9de13f5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:21:07.802896Z",
     "iopub.status.busy": "2025-03-02T11:21:07.802652Z",
     "iopub.status.idle": "2025-03-02T11:24:41.391618Z",
     "shell.execute_reply": "2025-03-02T11:24:41.390721Z"
    },
    "papermill": {
     "duration": 213.600246,
     "end_time": "2025-03-02T11:24:41.392986",
     "exception": false,
     "start_time": "2025-03-02T11:21:07.792740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [03:15<00:00,  5.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize empty list to store individual prediction dataframes\n",
    "dfs_to_merge = []\n",
    "\n",
    "# Load first experiment to get the base structure\n",
    "base_exp = experiments[0]\n",
    "if base_exp in parquet_experiments:\n",
    "    base_df = pd.read_parquet(base_exp + '/' + (base_exp.split('/')[-1]).replace('-','_') + '_oof.parquet')\n",
    "else:\n",
    "    base_df = pd.read_excel(base_exp + '/' + (base_exp.split('/')[-1]).replace('-','_') + '_oof.xlsx')\n",
    "\n",
    "# Create base dataframe with ID and target variables\n",
    "preds_df = base_df[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "\n",
    "# Load and merge predictions from each experiment\n",
    "for exp_name in tqdm(experiments):\n",
    "    # Read the prediction file\n",
    "    if exp_name in parquet_experiments:\n",
    "        if \"mcts-exp-02\" in exp_name:\n",
    "            curr_df = pd.read_parquet(exp_name + '/mcts_exp_01' + '_oof.parquet')\n",
    "        else:\n",
    "            curr_df = pd.read_parquet(exp_name + '/' + (exp_name.split('/')[-1]).replace('-','_') + '_oof.parquet')\n",
    "    else:\n",
    "        curr_df = pd.read_excel(exp_name + '/' + (exp_name.split('/')[-1]).replace('-','_') + '_oof.xlsx')\n",
    "    \n",
    "    # Create a temporary dataframe with ID and predictions\n",
    "    temp_df = curr_df[[\"ID\", \"predictions\"]].copy()\n",
    "    temp_df = temp_df.rename(columns={\"predictions\": exp_name})\n",
    "    \n",
    "    # Merge with the main dataframe\n",
    "    preds_df = preds_df.merge(temp_df, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f0040d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:24:41.416897Z",
     "iopub.status.busy": "2025-03-02T11:24:41.416328Z",
     "iopub.status.idle": "2025-03-02T11:24:41.422449Z",
     "shell.execute_reply": "2025-03-02T11:24:41.421776Z"
    },
    "papermill": {
     "duration": 0.018644,
     "end_time": "2025-03-02T11:24:41.423773",
     "exception": false,
     "start_time": "2025-03-02T11:24:41.405129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28800, 42),\n",
       " Index(['ID', 'efs', 'efs_time', 'race_group', '/kaggle/input/xgboost-exp-01',\n",
       "        '/kaggle/input/catboost-exp-01', '/kaggle/input/lgbm-exp-01',\n",
       "        '/kaggle/input/xgboost-exp-02', '/kaggle/input/catboost-exp-02',\n",
       "        '/kaggle/input/lgbm-exp-03', '/kaggle/input/catboost-exp-03',\n",
       "        '/kaggle/input/tabm-exp-01', '/kaggle/input/nn-exp-01',\n",
       "        '/kaggle/input/tn-exp-01', '/kaggle/input/tf-exp-01',\n",
       "        '/kaggle/input/svr-exp-01', '/kaggle/input/abd-exp-01',\n",
       "        '/kaggle/input/catboost-exp-04', '/kaggle/input/lgbm-exp-04',\n",
       "        '/kaggle/input/tabm-exp-02', '/kaggle/input/ds-exp-01',\n",
       "        '/kaggle/input/nn-exp-02', '/kaggle/input/nn-exp-04',\n",
       "        '/kaggle/input/catboost-exp-05', '/kaggle/input/xgboost-exp-05',\n",
       "        '/kaggle/input/lgbm-exp-05', '/kaggle/input/tn-exp-02',\n",
       "        '/kaggle/input/vr-exp-01', '/kaggle/input/tt-exp-01',\n",
       "        '/kaggle/input/en-exp-01', '/kaggle/input/en-exp-02',\n",
       "        '/kaggle/input/nn-exp-05', '/kaggle/input/rf-exp-05',\n",
       "        '/kaggle/input/mcts-exp-02', '/kaggle/input/catboost-exp-06',\n",
       "        '/kaggle/input/xgboost-exp-06', '/kaggle/input/lgbm-exp-06',\n",
       "        '/kaggle/input/ri-exp-01', '/kaggle/input/xgboost-exp-09',\n",
       "        '/kaggle/input/prlnn-exp-01', '/kaggle/input/ri-exp-06',\n",
       "        '/kaggle/input/xgboost-exp-10'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df.shape, preds_df.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8a51ec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:24:41.445927Z",
     "iopub.status.busy": "2025-03-02T11:24:41.445697Z",
     "iopub.status.idle": "2025-03-02T11:24:41.450181Z",
     "shell.execute_reply": "2025-03-02T11:24:41.449410Z"
    },
    "papermill": {
     "duration": 0.017012,
     "end_time": "2025-03-02T11:24:41.451305",
     "exception": false,
     "start_time": "2025-03-02T11:24:41.434293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/input/xgboost-exp-01',\n",
       " '/kaggle/input/catboost-exp-01',\n",
       " '/kaggle/input/lgbm-exp-01',\n",
       " '/kaggle/input/xgboost-exp-02',\n",
       " '/kaggle/input/catboost-exp-02',\n",
       " '/kaggle/input/lgbm-exp-03',\n",
       " '/kaggle/input/catboost-exp-03',\n",
       " '/kaggle/input/tabm-exp-01',\n",
       " '/kaggle/input/nn-exp-01',\n",
       " '/kaggle/input/tn-exp-01',\n",
       " '/kaggle/input/tf-exp-01',\n",
       " '/kaggle/input/svr-exp-01',\n",
       " '/kaggle/input/abd-exp-01',\n",
       " '/kaggle/input/catboost-exp-04',\n",
       " '/kaggle/input/lgbm-exp-04',\n",
       " '/kaggle/input/tabm-exp-02',\n",
       " '/kaggle/input/ds-exp-01',\n",
       " '/kaggle/input/nn-exp-02',\n",
       " '/kaggle/input/nn-exp-04',\n",
       " '/kaggle/input/catboost-exp-05',\n",
       " '/kaggle/input/xgboost-exp-05',\n",
       " '/kaggle/input/lgbm-exp-05',\n",
       " '/kaggle/input/tn-exp-02',\n",
       " '/kaggle/input/vr-exp-01',\n",
       " '/kaggle/input/tt-exp-01',\n",
       " '/kaggle/input/en-exp-01',\n",
       " '/kaggle/input/en-exp-02',\n",
       " '/kaggle/input/nn-exp-05',\n",
       " '/kaggle/input/rf-exp-05',\n",
       " '/kaggle/input/mcts-exp-02',\n",
       " '/kaggle/input/catboost-exp-06',\n",
       " '/kaggle/input/xgboost-exp-06',\n",
       " '/kaggle/input/lgbm-exp-06',\n",
       " '/kaggle/input/ri-exp-01',\n",
       " '/kaggle/input/xgboost-exp-09',\n",
       " '/kaggle/input/prlnn-exp-01',\n",
       " '/kaggle/input/ri-exp-06',\n",
       " '/kaggle/input/xgboost-exp-10']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aafb837c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:24:41.473536Z",
     "iopub.status.busy": "2025-03-02T11:24:41.473315Z",
     "iopub.status.idle": "2025-03-02T11:24:53.358869Z",
     "shell.execute_reply": "2025-03-02T11:24:53.357964Z"
    },
    "papermill": {
     "duration": 11.898129,
     "end_time": "2025-03-02T11:24:53.360242",
     "exception": false,
     "start_time": "2025-03-02T11:24:41.462113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-index 0.6743662038463064 /kaggle/input/xgboost-exp-01\n",
      "C-index 0.673797469068503 /kaggle/input/catboost-exp-01\n",
      "C-index 0.6735659109490936 /kaggle/input/lgbm-exp-01\n",
      "C-index 0.6719739976829171 /kaggle/input/xgboost-exp-02\n",
      "C-index 0.6715438291634074 /kaggle/input/catboost-exp-02\n",
      "C-index 0.6744113273004797 /kaggle/input/lgbm-exp-03\n",
      "C-index 0.6750446351219317 /kaggle/input/catboost-exp-03\n",
      "C-index 0.6698445972872783 /kaggle/input/tabm-exp-01\n",
      "C-index 0.6672504731273489 /kaggle/input/nn-exp-01\n",
      "C-index 0.6082304286386713 /kaggle/input/tn-exp-01\n",
      "C-index 0.6609386133349129 /kaggle/input/tf-exp-01\n",
      "C-index 0.6204664346764159 /kaggle/input/svr-exp-01\n",
      "C-index 0.6775546112659996 /kaggle/input/abd-exp-01\n",
      "C-index 0.6228381497451487 /kaggle/input/catboost-exp-04\n",
      "C-index 0.6216961934710434 /kaggle/input/lgbm-exp-04\n",
      "C-index 0.6777472728532595 /kaggle/input/tabm-exp-02\n",
      "C-index 0.6427991982766068 /kaggle/input/ds-exp-01\n",
      "C-index 0.6541867987232884 /kaggle/input/nn-exp-02\n",
      "C-index 0.6794831551153862 /kaggle/input/nn-exp-04\n",
      "I am here\n",
      "C-index 0.680734863930053 /kaggle/input/catboost-exp-05\n",
      "C-index 0.6801801958617659 /kaggle/input/xgboost-exp-05\n",
      "C-index 0.6791851357350963 /kaggle/input/lgbm-exp-05\n",
      "C-index 0.659742829229569 /kaggle/input/tn-exp-02\n",
      "C-index 0.6754542289908226 /kaggle/input/vr-exp-01\n",
      "C-index 0.6051212359506114 /kaggle/input/tt-exp-01\n",
      "C-index 0.6244728416862964 /kaggle/input/en-exp-01\n",
      "C-index 0.6417663206377346 /kaggle/input/en-exp-02\n",
      "C-index 0.6794831551153862 /kaggle/input/nn-exp-05\n",
      "C-index 0.6511582598647447 /kaggle/input/rf-exp-05\n",
      "C-index 0.6113856489762708 /kaggle/input/mcts-exp-02\n",
      "C-index 0.6805225213899068 /kaggle/input/catboost-exp-06\n",
      "C-index 0.6795077628167032 /kaggle/input/xgboost-exp-06\n",
      "C-index 0.6791063706461627 /kaggle/input/lgbm-exp-06\n",
      "C-index 0.6241849482559193 /kaggle/input/ri-exp-01\n",
      "C-index 0.681724483438663 /kaggle/input/xgboost-exp-09\n",
      "C-index 0.6817974276309455 /kaggle/input/prlnn-exp-01\n",
      "C-index 0.6356258684894599 /kaggle/input/ri-exp-06\n",
      "C-index 0.6816452214688579 /kaggle/input/xgboost-exp-10\n",
      "\n",
      "Best single model is /kaggle/input/prlnn-exp-01 with C-Index = 0.6817974276309455\n"
     ]
    }
   ],
   "source": [
    "best_score    = 0\n",
    "best_index    = -1\n",
    "best_ensemble = 0\n",
    "\n",
    "for k,name in enumerate(experiments):\n",
    "    if \"catboost-exp-05\" in name:\n",
    "        print(\"I am here\")\n",
    "    oof_pre = preds_df[[\"ID\",\"efs\",\"efs_time\",\"race_group\",name]]\n",
    "    oof_pre = oof_pre.rename(columns={name: \"predictions\"})\n",
    "    s, oof = compute_metric_cindex(oof_pre, name)\n",
    "    if s > best_score:\n",
    "        best_score    = s\n",
    "        best_index    = name\n",
    "        best_ensemble = oof\n",
    "        \n",
    "    print(f'C-index {s} {name}') \n",
    "print()\n",
    "print(f'Best single model is {best_index} with C-Index = {best_score}')\n",
    "experiments.remove(best_index)\n",
    "first_best_index = best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52261e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:24:53.386364Z",
     "iopub.status.busy": "2025-03-02T11:24:53.386095Z",
     "iopub.status.idle": "2025-03-02T11:24:53.390953Z",
     "shell.execute_reply": "2025-03-02T11:24:53.390234Z"
    },
    "papermill": {
     "duration": 0.018974,
     "end_time": "2025-03-02T11:24:53.392186",
     "exception": false,
     "start_time": "2025-03-02T11:24:53.373212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/input/xgboost-exp-01',\n",
       " '/kaggle/input/catboost-exp-01',\n",
       " '/kaggle/input/lgbm-exp-01',\n",
       " '/kaggle/input/xgboost-exp-02',\n",
       " '/kaggle/input/catboost-exp-02',\n",
       " '/kaggle/input/lgbm-exp-03',\n",
       " '/kaggle/input/catboost-exp-03',\n",
       " '/kaggle/input/tabm-exp-01',\n",
       " '/kaggle/input/nn-exp-01',\n",
       " '/kaggle/input/tn-exp-01',\n",
       " '/kaggle/input/tf-exp-01',\n",
       " '/kaggle/input/svr-exp-01',\n",
       " '/kaggle/input/abd-exp-01',\n",
       " '/kaggle/input/catboost-exp-04',\n",
       " '/kaggle/input/lgbm-exp-04',\n",
       " '/kaggle/input/tabm-exp-02',\n",
       " '/kaggle/input/ds-exp-01',\n",
       " '/kaggle/input/nn-exp-02',\n",
       " '/kaggle/input/nn-exp-04',\n",
       " '/kaggle/input/catboost-exp-05',\n",
       " '/kaggle/input/xgboost-exp-05',\n",
       " '/kaggle/input/lgbm-exp-05',\n",
       " '/kaggle/input/tn-exp-02',\n",
       " '/kaggle/input/vr-exp-01',\n",
       " '/kaggle/input/tt-exp-01',\n",
       " '/kaggle/input/en-exp-01',\n",
       " '/kaggle/input/en-exp-02',\n",
       " '/kaggle/input/nn-exp-05',\n",
       " '/kaggle/input/rf-exp-05',\n",
       " '/kaggle/input/mcts-exp-02',\n",
       " '/kaggle/input/catboost-exp-06',\n",
       " '/kaggle/input/xgboost-exp-06',\n",
       " '/kaggle/input/lgbm-exp-06',\n",
       " '/kaggle/input/ri-exp-01',\n",
       " '/kaggle/input/xgboost-exp-09',\n",
       " '/kaggle/input/ri-exp-06',\n",
       " '/kaggle/input/xgboost-exp-10']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6009e",
   "metadata": {
    "papermill": {
     "duration": 0.012087,
     "end_time": "2025-03-02T11:24:53.416566",
     "exception": false,
     "start_time": "2025-03-02T11:24:53.404479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5: Iterations for hill climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "127efacb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:24:53.442201Z",
     "iopub.status.busy": "2025-03-02T11:24:53.441991Z",
     "iopub.status.idle": "2025-03-02T11:24:53.444808Z",
     "shell.execute_reply": "2025-03-02T11:24:53.444164Z"
    },
    "papermill": {
     "duration": 0.016842,
     "end_time": "2025-03-02T11:24:53.446070",
     "exception": false,
     "start_time": "2025-03-02T11:24:53.429228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_NEGATIVE_WGT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c84b82a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:24:53.471509Z",
     "iopub.status.busy": "2025-03-02T11:24:53.471300Z",
     "iopub.status.idle": "2025-03-02T11:24:53.474075Z",
     "shell.execute_reply": "2025-03-02T11:24:53.473461Z"
    },
    "papermill": {
     "duration": 0.01669,
     "end_time": "2025-03-02T11:24:53.475186",
     "exception": false,
     "start_time": "2025-03-02T11:24:53.458496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices        = [best_index]\n",
    "old_best_score = best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3561ea41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:24:53.500338Z",
     "iopub.status.busy": "2025-03-02T11:24:53.500140Z",
     "iopub.status.idle": "2025-03-02T11:24:53.503370Z",
     "shell.execute_reply": "2025-03-02T11:24:53.502765Z"
    },
    "papermill": {
     "duration": 0.017099,
     "end_time": "2025-03-02T11:24:53.504540",
     "exception": false,
     "start_time": "2025-03-02T11:24:53.487441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PREPARE/MOVE VARIABLES TO GPU FOR SPEED UP\n",
    "best_ensemble = best_ensemble\n",
    "start         = -0.50\n",
    "if not USE_NEGATIVE_WGT: start = 0.01\n",
    "ww            = np.arange(start,0.51,0.01) # GPU\n",
    "nn            = len(ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11e7e27c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:24:53.529964Z",
     "iopub.status.busy": "2025-03-02T11:24:53.529764Z",
     "iopub.status.idle": "2025-03-02T11:24:53.532735Z",
     "shell.execute_reply": "2025-03-02T11:24:53.532083Z"
    },
    "papermill": {
     "duration": 0.016921,
     "end_time": "2025-03-02T11:24:53.533858",
     "exception": false,
     "start_time": "2025-03-02T11:24:53.516937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BEGIN HILL CLIMBING\n",
    "models  = [best_index]\n",
    "weights = []\n",
    "metrics = [best_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c39be3f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:24:53.559080Z",
     "iopub.status.busy": "2025-03-02T11:24:53.558879Z",
     "iopub.status.idle": "2025-03-02T11:24:53.563922Z",
     "shell.execute_reply": "2025-03-02T11:24:53.563262Z"
    },
    "papermill": {
     "duration": 0.01902,
     "end_time": "2025-03-02T11:24:53.565123",
     "exception": false,
     "start_time": "2025-03-02T11:24:53.546103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/kaggle/input/prlnn-exp-01'],\n",
       " [0.6817974276309455],\n",
       " array([-5.0000000e-01, -4.9000000e-01, -4.8000000e-01, -4.7000000e-01,\n",
       "        -4.6000000e-01, -4.5000000e-01, -4.4000000e-01, -4.3000000e-01,\n",
       "        -4.2000000e-01, -4.1000000e-01, -4.0000000e-01, -3.9000000e-01,\n",
       "        -3.8000000e-01, -3.7000000e-01, -3.6000000e-01, -3.5000000e-01,\n",
       "        -3.4000000e-01, -3.3000000e-01, -3.2000000e-01, -3.1000000e-01,\n",
       "        -3.0000000e-01, -2.9000000e-01, -2.8000000e-01, -2.7000000e-01,\n",
       "        -2.6000000e-01, -2.5000000e-01, -2.4000000e-01, -2.3000000e-01,\n",
       "        -2.2000000e-01, -2.1000000e-01, -2.0000000e-01, -1.9000000e-01,\n",
       "        -1.8000000e-01, -1.7000000e-01, -1.6000000e-01, -1.5000000e-01,\n",
       "        -1.4000000e-01, -1.3000000e-01, -1.2000000e-01, -1.1000000e-01,\n",
       "        -1.0000000e-01, -9.0000000e-02, -8.0000000e-02, -7.0000000e-02,\n",
       "        -6.0000000e-02, -5.0000000e-02, -4.0000000e-02, -3.0000000e-02,\n",
       "        -2.0000000e-02, -1.0000000e-02,  4.4408921e-16,  1.0000000e-02,\n",
       "         2.0000000e-02,  3.0000000e-02,  4.0000000e-02,  5.0000000e-02,\n",
       "         6.0000000e-02,  7.0000000e-02,  8.0000000e-02,  9.0000000e-02,\n",
       "         1.0000000e-01,  1.1000000e-01,  1.2000000e-01,  1.3000000e-01,\n",
       "         1.4000000e-01,  1.5000000e-01,  1.6000000e-01,  1.7000000e-01,\n",
       "         1.8000000e-01,  1.9000000e-01,  2.0000000e-01,  2.1000000e-01,\n",
       "         2.2000000e-01,  2.3000000e-01,  2.4000000e-01,  2.5000000e-01,\n",
       "         2.6000000e-01,  2.7000000e-01,  2.8000000e-01,  2.9000000e-01,\n",
       "         3.0000000e-01,  3.1000000e-01,  3.2000000e-01,  3.3000000e-01,\n",
       "         3.4000000e-01,  3.5000000e-01,  3.6000000e-01,  3.7000000e-01,\n",
       "         3.8000000e-01,  3.9000000e-01,  4.0000000e-01,  4.1000000e-01,\n",
       "         4.2000000e-01,  4.3000000e-01,  4.4000000e-01,  4.5000000e-01,\n",
       "         4.6000000e-01,  4.7000000e-01,  4.8000000e-01,  4.9000000e-01,\n",
       "         5.0000000e-01]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models, metrics, ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c26df07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T11:24:53.590489Z",
     "iopub.status.busy": "2025-03-02T11:24:53.590285Z",
     "iopub.status.idle": "2025-03-02T17:35:44.857192Z",
     "shell.execute_reply": "2025-03-02T17:35:44.856146Z"
    },
    "papermill": {
     "duration": 22251.281216,
     "end_time": "2025-03-02T17:35:44.858616",
     "exception": false,
     "start_time": "2025-03-02T11:24:53.577400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.11it/s]\n",
      "Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n",
      "Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ds_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for catboost_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.10it/s]\n",
      "Testing weights for rf_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for xgboost_exp_09: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Added /kaggle/input/catboost-exp-05 with weight 0.4800, Score: 0.6862\n",
      "1th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for xgboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for ds_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for nn_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for rf_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_09: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: Added /kaggle/input/xgboost-exp-09 with weight 0.2700, Score: 0.6868\n",
      "2th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for xgboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for ds_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for rf_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3: Added /kaggle/input/nn-exp-04 with weight 0.1600, Score: 0.6870\n",
      "3th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for ds_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for rf_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4: Added /kaggle/input/catboost-exp-01 with weight 0.0700, Score: 0.6872\n",
      "4th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for xgboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for ds_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.11it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for rf_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5: Added /kaggle/input/svr-exp-01 with weight -0.0500, Score: 0.6873\n",
      "5th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for xgboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for ds_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for rf_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for catboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.16it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6: Added /kaggle/input/rf-exp-05 with weight -0.0600, Score: 0.6875\n",
      "6th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for xgboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n",
      "Testing weights for ds_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.11it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for catboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7: Added /kaggle/input/lgbm-exp-03 with weight -0.0900, Score: 0.6875\n",
      "7th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for ds_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.11it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.12it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.12it/s]\n",
      "Testing weights for catboost_exp_06: 100%|██████████| 101/101 [00:32<00:00,  3.11it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8: Added /kaggle/input/tf-exp-01 with weight 0.0500, Score: 0.6876\n",
      "8th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for xgboost_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for ds_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.16it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.16it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for catboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9: Added /kaggle/input/xgboost-exp-02 with weight -0.0900, Score: 0.6877\n",
      "9th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:32<00:00,  3.11it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for ds_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Added /kaggle/input/ds-exp-01 with weight 0.0400, Score: 0.6878\n",
      "10th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:32<00:00,  3.16it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:32<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.12it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for catboost_exp_06: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11: Added /kaggle/input/catboost-exp-06 with weight 0.0700, Score: 0.6879\n",
      "11th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for xgboost_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12: Added /kaggle/input/xgboost-exp-06 with weight -0.1200, Score: 0.6879\n",
      "12th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.16it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13: Added /kaggle/input/tn-exp-02 with weight 0.0300, Score: 0.6880\n",
      "13th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.11it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.11it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for xgboost_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14: Added /kaggle/input/xgboost-exp-05 with weight 0.0700, Score: 0.6880\n",
      "14th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for catboost_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15: Added /kaggle/input/catboost-exp-03 with weight -0.0500, Score: 0.6881\n",
      "15th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for ri_exp_06: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:32<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16: Added /kaggle/input/ri-exp-06 with weight -0.0200, Score: 0.6881\n",
      "16th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17: Added /kaggle/input/en-exp-02 with weight 0.0300, Score: 0.6881\n",
      "17th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.11it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.12it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18: Added /kaggle/input/nn-exp-01 with weight -0.0400, Score: 0.6882\n",
      "18th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:32<00:00,  3.11it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.10it/s]\n",
      "Testing weights for nn_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19: Added /kaggle/input/nn-exp-02 with weight -0.0300, Score: 0.6882\n",
      "19th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.12it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.10it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for xgboost_exp_10: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20: Added /kaggle/input/xgboost-exp-10 with weight 0.0500, Score: 0.6882\n",
      "20th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.16it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21: Added /kaggle/input/lgbm-exp-01 with weight -0.0400, Score: 0.6883\n",
      "21th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tabm_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22: Added /kaggle/input/tabm-exp-02 with weight 0.0200, Score: 0.6883\n",
      "22th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for nn_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23: Added /kaggle/input/nn-exp-05 with weight -0.0300, Score: 0.6883\n",
      "23th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.12it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_06: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24: Added /kaggle/input/lgbm-exp-06 with weight -0.0200, Score: 0.6883\n",
      "24th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.11it/s]\n",
      "Testing weights for catboost_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25: Added /kaggle/input/catboost-exp-04 with weight 0.0100, Score: 0.6883\n",
      "25th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:32<00:00,  3.10it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.12it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for ri_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26: Added /kaggle/input/ri-exp-01 with weight -0.0100, Score: 0.6883\n",
      "26th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for mcts_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27: Added /kaggle/input/mcts-exp-02 with weight 0.0100, Score: 0.6883\n",
      "27th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.15it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28: Added /kaggle/input/catboost-exp-02 with weight -0.0100, Score: 0.6883\n",
      "28th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for abd_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.09it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.09it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29: Added /kaggle/input/abd-exp-01 with weight 0.0100, Score: 0.6883\n",
      "29th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30: Added /kaggle/input/tabm-exp-01 with weight -0.0100, Score: 0.6883\n",
      "30th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for vr_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.08it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31: Added /kaggle/input/vr-exp-01 with weight 0.0100, Score: 0.6883\n",
      "31th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32: Added /kaggle/input/xgboost-exp-01 with weight 0.0000, Score: 0.6883\n",
      "32th iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for lgbm_exp_04: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for lgbm_exp_05: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n",
      "Testing weights for tt_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n",
      "Testing weights for en_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement found, stopping\n",
      "{'index': -1, 'weight': 0, 'score': 0.6883439098529451}\n",
      "CPU times: user 6h 11min 1s, sys: 24 s, total: 6h 11min 25s\n",
      "Wall time: 6h 10min 51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "def optimize_ensemble(experiments, initial_ensemble, weights_range, score_function, max_iterations = 100):\n",
    "    \n",
    "    # Initialize variables\n",
    "    iteration  = 0\n",
    "    best_score = score_function(\n",
    "        initial_ensemble[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy(),\n",
    "        initial_ensemble[[\"ID\", \"predictions\"]].copy(),\n",
    "        \"ID\"\n",
    "    )\n",
    "    \n",
    "    model_weights         = {}\n",
    "    remaining_experiments = experiments.copy()\n",
    "    best_ensemble         = initial_ensemble.copy()\n",
    "    \n",
    "    while remaining_experiments and iteration < max_iterations:\n",
    "        print(f\"{iteration}th iteration\")\n",
    "        iteration += 1\n",
    "        best_iteration = {\n",
    "            'index' : -1,\n",
    "            'weight': 0,\n",
    "            'score' : best_score\n",
    "        }\n",
    "        \n",
    "        # Try each remaining model\n",
    "        for model_path in remaining_experiments:\n",
    "            try:\n",
    "                #print(f\"Iteration {iteration}: Trying model {model_path}\")\n",
    "                \n",
    "                # Load model OOF predictions\n",
    "                model_name = model_path.split(\"/\")[-1].replace('-', '_')\n",
    "                model_oof  = preds_df[model_path]\n",
    "                \n",
    "                # Try different weights\n",
    "                for weight in tqdm(weights_range, desc=f\"Testing weights for {model_name}\"):\n",
    "                    # Create potential ensemble\n",
    "                    potential_ensemble = pd.DataFrame({\n",
    "                        \"ID\": best_ensemble[\"ID\"],\n",
    "                        \"predictions\": (1 - weight) * rankdata(best_ensemble[\"predictions\"]) + \n",
    "                                     weight * rankdata(model_oof)\n",
    "                    })\n",
    "                    \n",
    "                    # Evaluate new ensemble\n",
    "                    new_score = score_function(\n",
    "                        preds_df[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy(),\n",
    "                        potential_ensemble.copy(),\n",
    "                        \"ID\"\n",
    "                    )\n",
    "                    \n",
    "                    # Update best if improved\n",
    "                    if new_score > best_iteration['score']:\n",
    "                        best_iteration.update({\n",
    "                            'index' : model_path,\n",
    "                            'weight': weight,\n",
    "                            'score' : new_score\n",
    "                        })\n",
    "                \n",
    "                # Clean up\n",
    "                del model_oof\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {model_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Check if we found an improvement\n",
    "        if best_iteration['index'] == -1:\n",
    "            print(\"No improvement found, stopping\")\n",
    "            print(best_iteration)\n",
    "            break\n",
    "            \n",
    "        # Update ensemble with best model found\n",
    "        best_score                             = best_iteration['score']\n",
    "        model_weights[best_iteration['index']] = best_iteration['weight']\n",
    "        remaining_experiments.remove(best_iteration['index'])\n",
    "        \n",
    "        print(\n",
    "            f\"Iteration {iteration}: Added {best_iteration['index']} \"\n",
    "            f\"with weight {best_iteration['weight']:.4f}, \"\n",
    "            f\"Score: {best_iteration['score']:.4f}\"\n",
    "        )\n",
    "        \n",
    "        # Update best ensemble for next iteration\n",
    "        model_oof = preds_df[best_iteration['index']]\n",
    "        best_ensemble[\"predictions\"] = (\n",
    "            (1 - best_iteration['weight']) * rankdata(best_ensemble[\"predictions\"]) + \n",
    "            best_iteration['weight'] * rankdata(model_oof)\n",
    "        )\n",
    "        \n",
    "    return model_weights, best_score\n",
    "\n",
    "\n",
    "model_weights, best_score = optimize_ensemble(experiments, best_ensemble, ww, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2497f898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:35:51.977433Z",
     "iopub.status.busy": "2025-03-02T17:35:51.977100Z",
     "iopub.status.idle": "2025-03-02T17:35:51.983151Z",
     "shell.execute_reply": "2025-03-02T17:35:51.982358Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.512732,
     "end_time": "2025-03-02T17:35:51.984351",
     "exception": false,
     "start_time": "2025-03-02T17:35:48.471619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import cupy as cp\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import gc\n",
    "# from scipy.stats import rankdata\n",
    "# import time\n",
    "\n",
    "# def optimize_ensemble_gpu(experiments, initial_ensemble, weights_range, score_function, max_iterations=100):\n",
    "#     \"\"\"\n",
    "#     GPU-accelerated ensemble optimization using CuPy.\n",
    "    \n",
    "#     Args:\n",
    "#         experiments: List of experiment paths to try\n",
    "#         initial_ensemble: Initial ensemble predictions\n",
    "#         weights_range: Range of weights to try\n",
    "#         score_function: Function to evaluate ensemble quality\n",
    "#         max_iterations: Maximum number of iterations\n",
    "    \n",
    "#     Returns:\n",
    "#         model_weights: Dictionary of selected models and their weights\n",
    "#         best_score: Best score achieved\n",
    "#     \"\"\"\n",
    "#     # Initialize variables\n",
    "#     iteration = 0\n",
    "    \n",
    "#     # Convert initial data to numpy for faster processing\n",
    "#     ids = initial_ensemble[\"ID\"].values\n",
    "#     initial_preds = initial_ensemble[\"predictions\"].values\n",
    "    \n",
    "#     # Pre-compute initial ensemble rank once (using numpy for initial ranking)\n",
    "#     initial_ensemble_rank = rankdata(initial_preds)\n",
    "    \n",
    "#     # Move to GPU\n",
    "#     cp_initial_ensemble_rank = cp.asarray(initial_ensemble_rank)\n",
    "    \n",
    "#     # Evaluate initial score\n",
    "#     best_score = score_function(\n",
    "#         initial_ensemble[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy(),\n",
    "#         initial_ensemble[[\"ID\", \"predictions\"]].copy(),\n",
    "#         \"ID\"\n",
    "#     )\n",
    "    \n",
    "#     model_weights = {}\n",
    "#     remaining_experiments = experiments.copy()\n",
    "#     best_ensemble = initial_ensemble.copy()\n",
    "    \n",
    "#     # Pre-load all model predictions to avoid disk I/O in the loop\n",
    "#     print(\"Pre-computing model ranks and moving to GPU...\")\n",
    "#     model_ranks_gpu = {}\n",
    "#     for model_path in tqdm(experiments):\n",
    "#         # Get model predictions and compute ranks\n",
    "#         model_preds = preds_df[model_path].values\n",
    "#         model_ranks = rankdata(model_preds)\n",
    "        \n",
    "#         # Move ranks to GPU\n",
    "#         model_ranks_gpu[model_path] = cp.asarray(model_ranks)\n",
    "    \n",
    "#     # Clone initial ensemble rank for GPU operations\n",
    "#     best_ensemble_rank_gpu = cp.copy(cp_initial_ensemble_rank)\n",
    "    \n",
    "#     # Create a GPU array for all weight values to test at once\n",
    "#     cp_weights = cp.asarray(weights_range)\n",
    "    \n",
    "#     while remaining_experiments and iteration < max_iterations:\n",
    "#         print(f\"{iteration}th iteration, remaining models: {len(remaining_experiments)}\")\n",
    "#         iteration += 1\n",
    "        \n",
    "#         best_iteration = {\n",
    "#             'index': -1,\n",
    "#             'weight': 0,\n",
    "#             'score': best_score\n",
    "#         }\n",
    "        \n",
    "#         # Create batch scoring for all models and all weights\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         for model_path in tqdm(remaining_experiments, desc=\"Testing models\"):\n",
    "#             # Get the model ranks from GPU memory\n",
    "#             model_ranks_cp = model_ranks_gpu[model_path]\n",
    "            \n",
    "#             # Prepare GPU arrays for batch computation\n",
    "#             all_scores = []\n",
    "            \n",
    "#             # Process weights in batches to avoid GPU memory issues\n",
    "#             batch_size = min(len(weights_range), 25)  # Adjust based on GPU memory\n",
    "            \n",
    "#             for batch_start in range(0, len(weights_range), batch_size):\n",
    "#                 batch_end = min(batch_start + batch_size, len(weights_range))\n",
    "#                 batch_weights = cp_weights[batch_start:batch_end]\n",
    "                \n",
    "#                 # Reshape for broadcasting\n",
    "#                 weights_reshaped = batch_weights.reshape(-1, 1)\n",
    "                \n",
    "#                 # GPU batch computation for all weights at once (for this model)\n",
    "#                 # (1-w) * current_ensemble_rank + w * model_rank for each w in batch\n",
    "#                 batch_results = cp.zeros((len(batch_weights), len(best_ensemble_rank_gpu)))\n",
    "                \n",
    "#                 for i, w in enumerate(batch_weights):\n",
    "#                     batch_results[i] = (1 - w) * best_ensemble_rank_gpu + w * model_ranks_cp\n",
    "                \n",
    "#                 # Transfer back to CPU for scoring\n",
    "#                 batch_results_np = cp.asnumpy(batch_results)\n",
    "                \n",
    "#                 # Evaluate all weights in this batch\n",
    "#                 for i, w_idx in enumerate(range(batch_start, batch_end)):\n",
    "#                     weight = weights_range[w_idx]\n",
    "                    \n",
    "#                     # Create ensemble for scoring\n",
    "#                     potential_ensemble = pd.DataFrame({\n",
    "#                         \"ID\": ids,\n",
    "#                         \"predictions\": batch_results_np[i]\n",
    "#                     })\n",
    "                    \n",
    "#                     # Evaluate the ensemble\n",
    "#                     new_score = score_function(\n",
    "#                         preds_df[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy(),\n",
    "#                         potential_ensemble,\n",
    "#                         \"ID\"\n",
    "#                     )\n",
    "                    \n",
    "#                     # Check if this is better\n",
    "#                     if new_score > best_iteration['score']:\n",
    "#                         best_iteration.update({\n",
    "#                             'index': model_path,\n",
    "#                             'weight': weight,\n",
    "#                             'score': new_score\n",
    "#                         })\n",
    "            \n",
    "#             # Free GPU memory\n",
    "#             del batch_results\n",
    "#             cp.get_default_memory_pool().free_all_blocks()\n",
    "        \n",
    "#         print(f\"Iteration completed in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "#         # Check if we found an improvement\n",
    "#         if best_iteration['index'] == -1:\n",
    "#             print(\"No improvement found, stopping\")\n",
    "#             break\n",
    "            \n",
    "#         # Update ensemble with best model found\n",
    "#         best_score = best_iteration['score']\n",
    "#         model_weights[best_iteration['index']] = best_iteration['weight']\n",
    "#         remaining_experiments.remove(best_iteration['index'])\n",
    "        \n",
    "#         print(\n",
    "#             f\"Iteration {iteration}: Added {best_iteration['index']} \"\n",
    "#             f\"with weight {best_iteration['weight']:.4f}, \"\n",
    "#             f\"Score: {best_iteration['score']:.4f}\"\n",
    "#         )\n",
    "        \n",
    "#         # Update best ensemble ranks for next iteration using GPU computation\n",
    "#         best_ensemble_rank_gpu = (\n",
    "#             (1 - best_iteration['weight']) * best_ensemble_rank_gpu + \n",
    "#             best_iteration['weight'] * model_ranks_gpu[best_iteration['index']]\n",
    "#         )\n",
    "        \n",
    "#         # Update actual predictions for final evaluation\n",
    "#         best_ensemble[\"predictions\"] = cp.asnumpy(best_ensemble_rank_gpu)\n",
    "    \n",
    "#     return model_weights, best_score\n",
    "\n",
    "\n",
    "# def concordance_index_gpu(time_array, pred_array, event_array):\n",
    "#     \"\"\"\n",
    "#     GPU-accelerated implementation of concordance index.\n",
    "    \n",
    "#     If your original concordance_index function is very complex,\n",
    "#     you may need to adapt this further.\n",
    "#     \"\"\"\n",
    "#     # Note: This is a placeholder implementation\n",
    "#     # You'll need to adapt the specific concordance_index logic to GPU\n",
    "#     # This would depend on your existing implementation\n",
    "    \n",
    "#     # Move data to GPU\n",
    "#     time_gpu = cp.asarray(time_array)\n",
    "#     pred_gpu = cp.asarray(pred_array)\n",
    "#     event_gpu = cp.asarray(event_array)\n",
    "    \n",
    "#     # Compute concordance index on GPU\n",
    "#     # ...GPU computation logic here...\n",
    "    \n",
    "#     # For now, fall back to your existing implementation\n",
    "#     # Move data back to CPU\n",
    "#     time_cpu = cp.asnumpy(time_gpu)\n",
    "#     pred_cpu = cp.asnumpy(pred_gpu)\n",
    "#     event_cpu = cp.asnumpy(event_gpu)\n",
    "    \n",
    "#     return concordance_index(time_cpu, pred_cpu, event_cpu)\n",
    "\n",
    "\n",
    "# def score_gpu(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "#     \"\"\"\n",
    "#     GPU-accelerated scoring function.\n",
    "    \n",
    "#     This is a skeleton that you'd need to adapt based on your original score function.\n",
    "#     \"\"\"\n",
    "#     sol = solution.copy()\n",
    "#     sub = submission.copy()\n",
    "    \n",
    "#     del sol[row_id_column_name]\n",
    "#     del sub[row_id_column_name]\n",
    "    \n",
    "#     event_label = 'efs'\n",
    "#     interval_label = 'efs_time'\n",
    "#     prediction_label = 'predictions'\n",
    "    \n",
    "#     # Merge solution and submission\n",
    "#     merged_df = pd.concat([sol, sub], axis=1)\n",
    "#     merged_df.reset_index(inplace=True)\n",
    "    \n",
    "#     # Group by race_group\n",
    "#     race_groups = merged_df.groupby('race_group')\n",
    "    \n",
    "#     # Use GPU for faster calculations\n",
    "#     c_indices = []\n",
    "#     for race, group in race_groups:\n",
    "#         # If you have a GPU version of concordance_index\n",
    "#         c_index_race = concordance_index_gpu(\n",
    "#             group[interval_label].values,\n",
    "#             -group[prediction_label].values,\n",
    "#             group[event_label].values\n",
    "#         )\n",
    "#         c_indices.append(c_index_race)\n",
    "    \n",
    "#     c_indices = np.array(c_indices)\n",
    "#     return float(np.mean(c_indices) - np.sqrt(np.var(c_indices)))\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         import cupy as cp\n",
    "#         print(\"CuPy is available - Using GPU acceleration\")\n",
    "        \n",
    "#         # Run GPU optimized version\n",
    "#         model_weights, best_score = optimize_ensemble_gpu(\n",
    "#             experiments, \n",
    "#             best_ensemble, \n",
    "#             weights_range=ww,\n",
    "#             score_function=score_gpu,  # Or use score_gpu if you implement it\n",
    "#             max_iterations=100\n",
    "#         )\n",
    "        \n",
    "#         print(f\"Best score: {best_score:.6f}\")\n",
    "#         print(\"Model weights:\")\n",
    "#         for model, weight in model_weights.items():\n",
    "#             print(f\"  {model}: {weight:.6f}\")\n",
    "            \n",
    "#     except ImportError:\n",
    "#         print(\"CuPy not available - falling back to CPU version\")\n",
    "#         # Run CPU version as fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c52b275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:35:59.126401Z",
     "iopub.status.busy": "2025-03-02T17:35:59.126088Z",
     "iopub.status.idle": "2025-03-02T17:35:59.131620Z",
     "shell.execute_reply": "2025-03-02T17:35:59.130806Z"
    },
    "papermill": {
     "duration": 3.705105,
     "end_time": "2025-03-02T17:35:59.133022",
     "exception": false,
     "start_time": "2025-03-02T17:35:55.427917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'/kaggle/input/catboost-exp-05': 0.48000000000000087,\n",
       "  '/kaggle/input/xgboost-exp-09': 0.2700000000000007,\n",
       "  '/kaggle/input/nn-exp-04': 0.1600000000000006,\n",
       "  '/kaggle/input/catboost-exp-01': 0.0700000000000005,\n",
       "  '/kaggle/input/svr-exp-01': -0.0499999999999996,\n",
       "  '/kaggle/input/rf-exp-05': -0.05999999999999961,\n",
       "  '/kaggle/input/lgbm-exp-03': -0.08999999999999964,\n",
       "  '/kaggle/input/tf-exp-01': 0.05000000000000049,\n",
       "  '/kaggle/input/xgboost-exp-02': -0.08999999999999964,\n",
       "  '/kaggle/input/ds-exp-01': 0.04000000000000048,\n",
       "  '/kaggle/input/catboost-exp-06': 0.0700000000000005,\n",
       "  '/kaggle/input/xgboost-exp-06': -0.11999999999999966,\n",
       "  '/kaggle/input/tn-exp-02': 0.03000000000000047,\n",
       "  '/kaggle/input/xgboost-exp-05': 0.0700000000000005,\n",
       "  '/kaggle/input/catboost-exp-03': -0.0499999999999996,\n",
       "  '/kaggle/input/ri-exp-06': -0.019999999999999574,\n",
       "  '/kaggle/input/en-exp-02': 0.03000000000000047,\n",
       "  '/kaggle/input/nn-exp-01': -0.03999999999999959,\n",
       "  '/kaggle/input/nn-exp-02': -0.029999999999999583,\n",
       "  '/kaggle/input/xgboost-exp-10': 0.05000000000000049,\n",
       "  '/kaggle/input/lgbm-exp-01': -0.03999999999999959,\n",
       "  '/kaggle/input/tabm-exp-02': 0.020000000000000462,\n",
       "  '/kaggle/input/nn-exp-05': -0.029999999999999583,\n",
       "  '/kaggle/input/lgbm-exp-06': -0.019999999999999574,\n",
       "  '/kaggle/input/catboost-exp-04': 0.010000000000000453,\n",
       "  '/kaggle/input/ri-exp-01': -0.009999999999999565,\n",
       "  '/kaggle/input/mcts-exp-02': 0.010000000000000453,\n",
       "  '/kaggle/input/catboost-exp-02': -0.009999999999999565,\n",
       "  '/kaggle/input/abd-exp-01': 0.010000000000000453,\n",
       "  '/kaggle/input/tabm-exp-01': -0.009999999999999565,\n",
       "  '/kaggle/input/vr-exp-01': 0.010000000000000453,\n",
       "  '/kaggle/input/xgboost-exp-01': 4.440892098500626e-16},\n",
       " 0.6883439098529451)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d32ccbc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:36:06.092214Z",
     "iopub.status.busy": "2025-03-02T17:36:06.091922Z",
     "iopub.status.idle": "2025-03-02T17:36:06.096884Z",
     "shell.execute_reply": "2025-03-02T17:36:06.096054Z"
    },
    "papermill": {
     "duration": 3.499486,
     "end_time": "2025-03-02T17:36:06.098111",
     "exception": false,
     "start_time": "2025-03-02T17:36:02.598625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/kaggle/input/prlnn-exp-01',\n",
       " ['/kaggle/input/xgboost-exp-01',\n",
       "  '/kaggle/input/catboost-exp-01',\n",
       "  '/kaggle/input/lgbm-exp-01',\n",
       "  '/kaggle/input/xgboost-exp-02',\n",
       "  '/kaggle/input/catboost-exp-02',\n",
       "  '/kaggle/input/lgbm-exp-03',\n",
       "  '/kaggle/input/catboost-exp-03',\n",
       "  '/kaggle/input/tabm-exp-01',\n",
       "  '/kaggle/input/nn-exp-01',\n",
       "  '/kaggle/input/tn-exp-01',\n",
       "  '/kaggle/input/tf-exp-01',\n",
       "  '/kaggle/input/svr-exp-01',\n",
       "  '/kaggle/input/abd-exp-01',\n",
       "  '/kaggle/input/catboost-exp-04',\n",
       "  '/kaggle/input/lgbm-exp-04',\n",
       "  '/kaggle/input/tabm-exp-02',\n",
       "  '/kaggle/input/ds-exp-01',\n",
       "  '/kaggle/input/nn-exp-02',\n",
       "  '/kaggle/input/nn-exp-04',\n",
       "  '/kaggle/input/catboost-exp-05',\n",
       "  '/kaggle/input/xgboost-exp-05',\n",
       "  '/kaggle/input/lgbm-exp-05',\n",
       "  '/kaggle/input/tn-exp-02',\n",
       "  '/kaggle/input/vr-exp-01',\n",
       "  '/kaggle/input/tt-exp-01',\n",
       "  '/kaggle/input/en-exp-01',\n",
       "  '/kaggle/input/en-exp-02',\n",
       "  '/kaggle/input/nn-exp-05',\n",
       "  '/kaggle/input/rf-exp-05',\n",
       "  '/kaggle/input/mcts-exp-02',\n",
       "  '/kaggle/input/catboost-exp-06',\n",
       "  '/kaggle/input/xgboost-exp-06',\n",
       "  '/kaggle/input/lgbm-exp-06',\n",
       "  '/kaggle/input/ri-exp-01',\n",
       "  '/kaggle/input/xgboost-exp-09',\n",
       "  '/kaggle/input/ri-exp-06',\n",
       "  '/kaggle/input/xgboost-exp-10'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_best_index, experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525555d4",
   "metadata": {
    "papermill": {
     "duration": 3.591145,
     "end_time": "2025-03-02T17:36:13.199520",
     "exception": false,
     "start_time": "2025-03-02T17:36:09.608375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 6: Inference on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa0e583d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:36:20.198785Z",
     "iopub.status.busy": "2025-03-02T17:36:20.198400Z",
     "iopub.status.idle": "2025-03-02T17:36:20.204416Z",
     "shell.execute_reply": "2025-03-02T17:36:20.203530Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.493538,
     "end_time": "2025-03-02T17:36:20.205827",
     "exception": false,
     "start_time": "2025-03-02T17:36:16.712289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CFG:\n",
    "#     folds = 10\n",
    "\n",
    "# # https://www.kaggle.com/datasets/jsday96/mcts-tabm-models/data?select=TabMRegressor.py\n",
    "# class TabMRegressor:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         arch_type: str        = 'tabm-mini',\n",
    "#         backbone: dict        = {'type': 'MLP', 'n_blocks': 3, 'd_block': 512, 'dropout': 0.1},\n",
    "#         d_embedding: int      = 64,  # Only used for 'tabm-mini'\n",
    "#         bin_count: int        = 48,  # Only used for 'tabm-mini'\n",
    "#         k: int                = 32,\n",
    "#         learning_rate: float  = 1e-4,\n",
    "#         weight_decay: float   = 1e-3,\n",
    "#         clip_grad_norm: bool  = True,\n",
    "#         max_epochs: int       = 100,\n",
    "#         patience: int         = 15,\n",
    "#         batch_size: int       = 32,\n",
    "#         compile_model: bool   = False,\n",
    "#         device: Optional[str] = 'cuda:0',\n",
    "#         random_state: int     = 0,\n",
    "#         verbose: bool         = True\n",
    "#     ):\n",
    "#         self.arch_type = arch_type\n",
    "#         self.backbone = backbone\n",
    "#         self.d_embedding = d_embedding\n",
    "#         self.bin_count = bin_count\n",
    "#         self.k = k\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.weight_decay = weight_decay\n",
    "#         self.clip_grad_norm = clip_grad_norm\n",
    "#         self.max_epochs = max_epochs\n",
    "#         self.patience = patience\n",
    "#         self.batch_size = batch_size\n",
    "#         self.compile_model = compile_model\n",
    "#         self.device = torch.device(device if device else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "#         self.random_state = random_state\n",
    "#         self.verbose = verbose\n",
    "\n",
    "#     def fit(\n",
    "#         self,\n",
    "#         X: pd.DataFrame,\n",
    "#         y: np.array,\n",
    "#         eval_set: Tuple[pd.DataFrame, np.array]\n",
    "#     ):\n",
    "#         # PREPROCESS DATA.\n",
    "#         X_cat_train, X_cont_train, cat_cardinalities, y_train = self._preprocess_data(X, y, training=True)\n",
    "#         X_cat_val, X_cont_val, _, y_val = self._preprocess_data(eval_set[0], eval_set[1], training=False)\n",
    "\n",
    "#         # CREATE MODEL & TRAINING ALGO.\n",
    "#         bins = rtdl_num_embeddings.compute_bins(X_cont_train, n_bins=self.bin_count) if self.arch_type == 'tabm-mini' else None\n",
    "#         self.model = Model(\n",
    "#             n_num_features=X_cont_train.shape[1],\n",
    "#             cat_cardinalities=cat_cardinalities,\n",
    "#             n_classes=None,\n",
    "#             backbone=self.backbone,\n",
    "#             bins=bins,\n",
    "#             num_embeddings=(\n",
    "#                 None\n",
    "#                 if bins is None\n",
    "#                 else {\n",
    "#                     'type': 'PiecewiseLinearEmbeddings',\n",
    "#                     'd_embedding': self.d_embedding,\n",
    "#                     'activation': True,\n",
    "#                     'version': 'B',\n",
    "#                 }\n",
    "#             ),\n",
    "#             arch_type=self.arch_type,\n",
    "#             k=self.k,\n",
    "#         ).to(self.device)\n",
    "#         optimizer = torch.optim.AdamW(make_parameter_groups(self.model), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "#         if self.compile_model:\n",
    "#             self.model = torch.compile(self.model)\n",
    "\n",
    "#         loss_fn = torch.nn.MSELoss().to(self.device)\n",
    "#         # TRAIN & TEST MODEL.\n",
    "#         best = {\n",
    "#             'epoch': -1,\n",
    "#             'eval_loss': math.inf,\n",
    "#             'model_state_dict': None,\n",
    "#         }\n",
    "#         remaining_patience = self.patience\n",
    "#         epoch_size = math.ceil(len(X) / self.batch_size)\n",
    "\n",
    "\n",
    "#         for epoch in range(self.max_epochs):\n",
    "#             # TRAIN.\n",
    "#             optimizer.zero_grad()\n",
    "#             train_losses = []\n",
    "#             progress_bar = torch.randperm(len(y_train), device=self.device).split(self.batch_size)\n",
    "#             progress_bar = tqdm(progress_bar, desc=f'Epoch {epoch}', total=epoch_size) if self.verbose else progress_bar\n",
    "#             for batch_idx in progress_bar:\n",
    "#                 self.model.train()\n",
    "\n",
    "#                 with torch.amp.autocast(device_type='cuda', dtype = torch.bfloat16):\n",
    "#                     y_pred = self.model(\n",
    "#                         X_cont_train[batch_idx],\n",
    "#                         X_cat_train[batch_idx],\n",
    "#                     ).squeeze(-1).float()\n",
    "\n",
    "#                 loss = loss_fn(y_pred.flatten(0, 1), y_train[batch_idx].repeat_interleave(self.k))\n",
    "#                 loss.backward()\n",
    "#                 if self.clip_grad_norm:\n",
    "#                     torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "#              # EVALUATE.\n",
    "#             self.model.eval()\n",
    "#             val_losses = []\n",
    "#             with torch.no_grad():\n",
    "#                 for batch_idx in torch.arange(0, len(y_val), self.batch_size, device=self.device):\n",
    "#                     y_pred = self.model(\n",
    "#                         X_cont_val[batch_idx:batch_idx+self.batch_size],\n",
    "#                         X_cat_val[batch_idx:batch_idx+self.batch_size],\n",
    "#                     ).squeeze(-1).float()\n",
    "\n",
    "#                     loss = loss_fn(y_pred.flatten(0, 1), y_val[batch_idx:batch_idx+self.batch_size].repeat_interleave(self.k))\n",
    "#                     val_losses.append(loss.item())\n",
    "\n",
    "\n",
    "#             # PRINT INFO.\n",
    "#             mean_train_loss = np.mean(train_losses)\n",
    "#             mean_val_loss = np.mean(val_losses)\n",
    "#             if self.verbose:\n",
    "#                 print(f'Epoch {epoch} | Train Loss: {mean_train_loss} | Val Loss: {mean_val_loss}')\n",
    "\n",
    "\n",
    "#             # COMPARE TO BEST.\n",
    "#             if mean_val_loss < best['eval_loss']:\n",
    "#                 best['epoch'] = epoch\n",
    "#                 best['eval_loss'] = mean_val_loss\n",
    "#                 best['model_state_dict'] = self.model.state_dict()\n",
    "#                 remaining_patience = self.patience\n",
    "                \n",
    "#                 if self.verbose:\n",
    "#                     print('🌸 New best epoch! 🌸')\n",
    "#             else:\n",
    "#                 remaining_patience -= 1\n",
    "\n",
    "#             # EARLY STOPPING.\n",
    "#             if remaining_patience == 0:\n",
    "#                 break\n",
    "\n",
    "#             # RESTORE BEST MODEL.\n",
    "#             self.model.load_state_dict(best['model_state_dict'])\n",
    "\n",
    "\n",
    "#     def predict(\n",
    "#         self,\n",
    "#         X: pd.DataFrame,\n",
    "#         batch_size: Optional[int] = 8096\n",
    "#     ) -> np.ndarray:\n",
    "#         # PREPROCESS DATA.\n",
    "#         X_cat, X_cont, _, _ = self._preprocess_data(X, y=None, training=False)\n",
    "\n",
    "#         # PREDICT.\n",
    "#         self.model.eval()\n",
    "#         y_pred = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch_idx in torch.arange(0, len(X), batch_size, device=self.device):\n",
    "#                 y_pred.append(\n",
    "#                     self.model(\n",
    "#                         X_cont[batch_idx:batch_idx+batch_size],\n",
    "#                         X_cat[batch_idx:batch_idx+batch_size],\n",
    "#                     ).squeeze(-1).float().cpu().numpy()\n",
    "#                 )\n",
    "\n",
    "#         y_pred = np.concatenate(y_pred)\n",
    "\n",
    "\n",
    "#         # DENORMALIZE TARGETS.\n",
    "#         y_pred = y_pred * self._target_std + self._target_mean\n",
    "\n",
    "\n",
    "#         # COMPUTE ENSEMBLE MEAN.\n",
    "#         y_pred = np.mean(y_pred, axis=1)\n",
    "\n",
    "#         return y_pred\n",
    "\n",
    "\n",
    "#     def _preprocess_data(self, X: pd.DataFrame, y: pd.Series, training: bool):\n",
    "#         # PICK NON-CONSTANT COLUMNS.\n",
    "#         if training:\n",
    "#             self._non_constant_columns = X.columns[X.nunique() > 1]\n",
    "\n",
    "#         X = X[self._non_constant_columns]\n",
    "\n",
    "#         # SEPARATE CATEGORICAL & CONTINUOUS FEATURES.\n",
    "#         categorical_features = [col for col in X.columns if X[col].dtype.name == 'object']\n",
    "#         X_cat = X[categorical_features].to_numpy()\n",
    "#         X_cont = X.drop(columns=categorical_features).to_numpy()\n",
    "\n",
    "#         # ENCODE CATEGORICAL FEATURES.\n",
    "#         cat_cardinalities = [X[col].nunique() for col in categorical_features]\n",
    "\n",
    "#         if training:\n",
    "#             self._categorical_encoders = [\n",
    "#                 OrdinalEncoder()\n",
    "#                 for _ in range(X_cat.shape[1])\n",
    "#             ]\n",
    "#         X_cat = np.concatenate([\n",
    "#             encoder.fit_transform(X_cat[:, i:i+1])\n",
    "#             for i, encoder in enumerate(self._categorical_encoders)\n",
    "#         ], axis=1)\n",
    "\n",
    "#         # NORMALIZE TARGETS.\n",
    "#         if training:\n",
    "#             self._target_mean = y.mean()\n",
    "#             self._target_std = y.std()\n",
    "\n",
    "#             y = (y - self._target_mean) / self._target_std\n",
    "\n",
    "\n",
    "#         # SCALE CONTINUOUS FEATURES.\n",
    "#         if training:\n",
    "#             noise = (\n",
    "#                 np.random.default_rng(0)\n",
    "#                 .normal(0.0, 1e-5, X_cont.shape)\n",
    "#                 .astype(X_cont.dtype)\n",
    "#             )\n",
    "#             self._cont_feature_preprocessor = QuantileTransformer(\n",
    "#                 n_quantiles=max(min(len(X) // 30, 1000), 10),\n",
    "#                 output_distribution='normal',\n",
    "#                 subsample=10**9,\n",
    "#             ).fit(X_cont + noise)\n",
    "\n",
    "#         X_cont = self._cont_feature_preprocessor.transform(X_cont)\n",
    "\n",
    "\n",
    "#         # CONVERT TO TENSORS.\n",
    "#         X_cat = torch.tensor(X_cat, dtype=torch.long, device=self.device)\n",
    "#         X_cont = torch.tensor(X_cont, dtype=torch.float32, device=self.device)\n",
    "\n",
    "#         if y is not None:\n",
    "#             y = torch.tensor(y, dtype=torch.float32, device=self.device)\n",
    "\n",
    "#         return X_cat, X_cont, cat_cardinalities, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83b3ba3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:36:27.060529Z",
     "iopub.status.busy": "2025-03-02T17:36:27.060215Z",
     "iopub.status.idle": "2025-03-02T17:36:27.063545Z",
     "shell.execute_reply": "2025-03-02T17:36:27.062886Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.414309,
     "end_time": "2025-03-02T17:36:27.064748",
     "exception": false,
     "start_time": "2025-03-02T17:36:23.650439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_tabm_features(data):\n",
    "#     RMV = [\"ID\",\"efs\",\"efs_time\",\"y\",\"fold\"]\n",
    "#     FEATURES = [c for c in data.columns if not c in RMV]\n",
    "    \n",
    "#     RMV              = ['ID']\n",
    "#     X_test           = data.drop(RMV, axis=1)\n",
    "#     y_pred           = data[['ID']]\n",
    "    \n",
    "#     #print(\"X_test shape:\", X_test.shape, '\\n')\n",
    "    \n",
    "#     cat_cols         = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "#     num_cols         = X_test.select_dtypes(exclude=['object']).columns.tolist()\n",
    "    \n",
    "#     # Preprocessing categorical\n",
    "#     imputer          = SimpleImputer(strategy='constant', fill_value='NAN')\n",
    "#     X_test[cat_cols] = imputer.fit_transform(X_test[cat_cols])\n",
    "\n",
    "#     # Preprocessing numerical\n",
    "#     imputer          = SimpleImputer(strategy=\"median\")\n",
    "#     X_test[num_cols] = imputer.fit_transform(X_test[num_cols])\n",
    "\n",
    "#     return X_test,FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98e683da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:36:34.157343Z",
     "iopub.status.busy": "2025-03-02T17:36:34.157047Z",
     "iopub.status.idle": "2025-03-02T17:36:34.160643Z",
     "shell.execute_reply": "2025-03-02T17:36:34.159961Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.46005,
     "end_time": "2025-03-02T17:36:34.161903",
     "exception": false,
     "start_time": "2025-03-02T17:36:30.701853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def prepare_features(model_path, train, test):\n",
    "\n",
    "#     RMV = [\"ID\",\"efs\",\"efs_time\",\"y\"]\n",
    "#     FEATURES = [c for c in train.columns if not c in RMV]\n",
    "#     #print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n",
    "    \n",
    "\n",
    "#     CATS = []\n",
    "#     for c in FEATURES:\n",
    "#         if train[c].dtype==\"object\":\n",
    "#             CATS.append(c)\n",
    "#             train[c] = train[c].fillna(\"NAN\")\n",
    "#             test[c]  = test[c].fillna(\"NAN\")\n",
    "#         elif \"DeepTabels\" in model_path or \"tn\" in model_path or \"svr\" in model_path:\n",
    "#             train[c] = train[c].fillna(-1)\n",
    "#             test[c]  = test[c].fillna(-1)\n",
    "            \n",
    "        \n",
    "#     #print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")\n",
    "    \n",
    "#     combined = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "#     #print(\"Combined data shape:\", combined.shape )\n",
    "    \n",
    "#     # LABEL ENCODE CATEGORICAL FEATURES\n",
    "#     #print(\"We LABEL ENCODE the CATEGORICAL FEATURES: \",end=\"\")\n",
    "#     for c in FEATURES:\n",
    "    \n",
    "#         # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "#         if c in CATS:\n",
    "#             #print(f\"{c}, \",end=\"\")\n",
    "#             combined[c],_ = combined[c].factorize()\n",
    "#             combined[c]  -= combined[c].min()\n",
    "#             combined[c]   = combined[c].astype(\"int32\")\n",
    "#             combined[c]   = combined[c].astype(\"category\")\n",
    "            \n",
    "#         # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n",
    "#         else:\n",
    "#             if combined[c].dtype ==\"float64\":\n",
    "#                 combined[c]      = combined[c].astype(\"float32\")\n",
    "#             if combined[c].dtype ==\"int64\":\n",
    "#                 combined[c]      = combined[c].astype(\"int32\")\n",
    "        \n",
    "#     train = combined.iloc[:len(train)].copy()\n",
    "#     test  = combined.iloc[len(train):].reset_index(drop=True).copy()\n",
    "                \n",
    "#     return train, test, FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac94cc08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:36:41.218935Z",
     "iopub.status.busy": "2025-03-02T17:36:41.218550Z",
     "iopub.status.idle": "2025-03-02T17:36:41.222188Z",
     "shell.execute_reply": "2025-03-02T17:36:41.221537Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.609566,
     "end_time": "2025-03-02T17:36:41.223477",
     "exception": false,
     "start_time": "2025-03-02T17:36:37.613911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_nn_features(train, test):\n",
    "    \n",
    "#     CAT_SIZE = []\n",
    "#     CAT_EMB  = []\n",
    "#     NUMS     = []\n",
    "#     CATS     = []\n",
    "\n",
    "#     RMV = [\"ID\",\"efs\",\"efs_time\",\"y\",\"fold\"]\n",
    "#     FEATURES = [c for c in train.columns if not c in RMV]\n",
    "    \n",
    "#     for c in FEATURES:\n",
    "#         if train[c].dtype==\"object\":\n",
    "#             train[c] = train[c].fillna(\"NAN\")\n",
    "#             test[c]  = test[c].fillna(\"NAN\")\n",
    "#             CATS.append(c)\n",
    "#         elif not \"age\" in c:\n",
    "#             train[c] = train[c].astype(\"str\")\n",
    "#             test[c]  = test[c].astype(\"str\")\n",
    "#             CATS.append(c)\n",
    "\n",
    "\n",
    "#     combined = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "#     for c in FEATURES:\n",
    "#         if c in CATS:\n",
    "#             # LABEL ENCODE\n",
    "#             combined[c],_ = combined[c].factorize()\n",
    "#             combined[c] -= combined[c].min()\n",
    "#             combined[c] = combined[c].astype(\"int32\")\n",
    "#             #combined[c] = combined[c].astype(\"category\")\n",
    "\n",
    "#             n = combined[c].nunique()\n",
    "#             mn = combined[c].min()\n",
    "#             mx = combined[c].max()\n",
    "#             #print(f'{c} has ({n}) unique values')\n",
    "    \n",
    "#             CAT_SIZE.append(mx+1) \n",
    "#             CAT_EMB.append( int(np.ceil( np.sqrt(mx+1))) ) \n",
    "#         else:\n",
    "#             if combined[c].dtype==\"float64\":\n",
    "#                 combined[c] = combined[c].astype(\"float32\")\n",
    "#             if combined[c].dtype==\"int64\":\n",
    "#                 combined[c] = combined[c].astype(\"int32\")\n",
    "                \n",
    "#             m = combined[c].mean()\n",
    "#             s = combined[c].std()\n",
    "#             combined[c] = (combined[c]-m)/s\n",
    "#             combined[c] = combined[c].fillna(0)\n",
    "            \n",
    "#             NUMS.append(c)\n",
    "\n",
    "#     train = combined.iloc[:len(train)].copy()\n",
    "#     test = combined.iloc[len(train):].reset_index(drop=True).copy()\n",
    "\n",
    "#     return test[CATS], test[NUMS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af87a404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:36:48.333786Z",
     "iopub.status.busy": "2025-03-02T17:36:48.333466Z",
     "iopub.status.idle": "2025-03-02T17:36:48.337230Z",
     "shell.execute_reply": "2025-03-02T17:36:48.336361Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.672832,
     "end_time": "2025-03-02T17:36:48.338649",
     "exception": false,
     "start_time": "2025-03-02T17:36:44.665817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_tf_features(train, test):\n",
    "#     RMV = [\"ID\",\"efs\",\"efs_time\",\"y\",\"y_na\",\"fold\"]\n",
    "#     FEATURES = [c for c in train.columns if not c in RMV]\n",
    "\n",
    "\n",
    "#     test                             = test.replace('Not done', 'missing')\n",
    "#     test                             = test.replace('Not tested', 'missing')\n",
    "    \n",
    "#     test['na_count']                 = test.isna().sum(axis=1)\n",
    "#     test['age_karnofsky']            = test['age_at_hct'] * test['karnofsky_score']\n",
    "#     test['age_comorbidity']          = test['age_at_hct'] * test['comorbidity_score']\n",
    "#     test['donor_recipient_age_diff'] = abs(test['donor_age'] - test['age_at_hct'])\n",
    "#     test['hla_match_ratio']          = (test['hla_high_res_8'] + test['hla_low_res_8']) / 16\n",
    "#     test['age_squared']              = test['age_at_hct'] ** 2\n",
    "#     test['karnofsky_squared']        = test['karnofsky_score'] ** 2\n",
    "#     test['16?']                      = np.where(test['age_at_hct']<=16,1,0)\n",
    "    \n",
    "#     FEATURES.extend([\"na_count\", \"age_karnofsky\", \"age_comorbidity\", \"donor_recipient_age_diff\", \"hla_match_ratio\", \"age_squared\", \"karnofsky_squared\", \"16?\"])\n",
    "\n",
    "#     CATS = []\n",
    "#     for c in FEATURES:\n",
    "#         if test[c].dtype==\"object\":\n",
    "#             CATS.append(c)\n",
    "#             test[c] = test[c].fillna(\"missing\")\n",
    "\n",
    "#     for c in FEATURES:\n",
    "#         # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "#         if c in CATS:\n",
    "#             #print(f\"{c}, \",end=\"\")\n",
    "#             test[c],_ = test[c].factorize()\n",
    "#             test[c]  -= test[c].min()\n",
    "#             test[c]   = test[c].astype(\"int32\")\n",
    "#             test[c]   = test[c].astype(\"category\")\n",
    "#         else:\n",
    "#             if test[c].dtype == \"float64\":\n",
    "#                 test[c]      = test[c].astype(\"float32\")\n",
    "#             if test[c].dtype ==\"int64\":\n",
    "#                 test[c]      = test[c].astype(\"int32\")\n",
    "    \n",
    "#     return test, FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d739cc12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:36:55.286056Z",
     "iopub.status.busy": "2025-03-02T17:36:55.285759Z",
     "iopub.status.idle": "2025-03-02T17:36:55.289511Z",
     "shell.execute_reply": "2025-03-02T17:36:55.288871Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.463008,
     "end_time": "2025-03-02T17:36:55.290794",
     "exception": false,
     "start_time": "2025-03-02T17:36:51.827786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.impute import KNNImputer\n",
    "\n",
    "# imputer               = KNNImputer(n_neighbors=5, weights='uniform')\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# FOLDS = 10\n",
    "\n",
    "# def inference(model_path, train, test_df):\n",
    "    \n",
    "#     path = model_path.split('/')[-1]\n",
    "#     file = path.replace('-','_')\n",
    "\n",
    "#     if \"dt\" not in model_path:\n",
    "#         with open(f\"/kaggle/input/{path}/{file}.pkl\", 'rb') as f:\n",
    "#             models = pickle.load(f)\n",
    "            \n",
    "#     print(\"All models are loaded successfully....!\")\n",
    "\n",
    "#     test_predictions = np.zeros(len(test_df))\n",
    "\n",
    "#     for fold in tqdm(range(FOLDS)):\n",
    "#         if \"dt\" in model_path:\n",
    "#             # model = keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "#             # train, test, FEATURES   = prepare_features(model_path, train, test_df)\n",
    "#             # model                   = tf.keras.models.load_model(model_path+f\"/model_fold_{fold}.keras\", custom_objects=dt.__dict__, compile=)\n",
    "#             # fold_preds              = model.predict(test.copy())\n",
    "#             # fold_preds              = fold_preds.flatten()\n",
    "#             pass\n",
    "#         else:\n",
    "#             model  = models[fold]\n",
    "            \n",
    "            \n",
    "#         if \"svr\" in model_path:\n",
    "#             if fold==0:\n",
    "#                 train, test, FEATURES = prepare_features(model_path, train.copy(), test_df.copy())\n",
    "\n",
    "#             # Handle missing values\n",
    "#             train_imputed         = imputer.fit_transform(train[FEATURES].copy())\n",
    "#             test_imputed          = imputer.transform(test[FEATURES])\n",
    "\n",
    "#             # Convert back to DataFrame to maintain feature names\n",
    "#             train_imputed         = pd.DataFrame(train_imputed, columns=FEATURES, index=train.index)\n",
    "#             test_imputed          = pd.DataFrame(test_imputed, columns=FEATURES, index=test.index)\n",
    "            \n",
    "#             # Scale features\n",
    "#             scaler.fit(train_imputed)\n",
    "#             test_scaled           = scaler.transform(test_imputed)\n",
    "            \n",
    "#             fold_preds            = model.predict(test_scaled)\n",
    "\n",
    "#         elif \"tn\" in model_path:\n",
    "#             if fold == 0:\n",
    "#                 train, test, FEATURES   = prepare_features(model_path, train, test_df)\n",
    "                \n",
    "#             fold_preds              = model.predict(test[FEATURES].values).flatten()\n",
    "            \n",
    "#         elif \"nn\" in model_path:\n",
    "#             if fold == 0:\n",
    "#                 X_cat, X_num      = get_nn_features(train, test_df.copy())\n",
    "#             fold_preds        = model.predict([X_cat.values, X_num.values])\n",
    "#             fold_preds        = fold_preds.flatten()\n",
    "\n",
    "#         elif \"tf\" in model_path:\n",
    "#             if fold == 0:\n",
    "#                 test, FEATURES = get_tf_features(train.copy(),test_df.copy())\n",
    "#                 fold_preds     = model.predict(test[FEATURES].copy())\n",
    "\n",
    "#         elif \"tabm\" in model_path:\n",
    "#             if fold == 0:\n",
    "#                 test, FEATURES = get_tabm_features(test_df.copy())\n",
    "#             fold_preds              = model.predict(test[FEATURES].copy())\n",
    "            \n",
    "#         else: \n",
    "#             if fold == 0:\n",
    "#                 train, test, FEATURES   = prepare_features(model_path, train, test_df)\n",
    "#             fold_preds              = model.predict(test[FEATURES].copy())\n",
    "            \n",
    "#         test_predictions += fold_preds\n",
    "    \n",
    "#     # Get the average predictionr\n",
    "#     test_predictions /= FOLDS\n",
    "        \n",
    "#     return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffabd797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:37:02.370219Z",
     "iopub.status.busy": "2025-03-02T17:37:02.369892Z",
     "iopub.status.idle": "2025-03-02T17:37:02.373459Z",
     "shell.execute_reply": "2025-03-02T17:37:02.372614Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.623944,
     "end_time": "2025-03-02T17:37:02.374939",
     "exception": false,
     "start_time": "2025-03-02T17:36:58.750995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n",
    "# test_df  = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e667faec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:37:09.348562Z",
     "iopub.status.busy": "2025-03-02T17:37:09.348266Z",
     "iopub.status.idle": "2025-03-02T17:37:09.351615Z",
     "shell.execute_reply": "2025-03-02T17:37:09.350788Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.481608,
     "end_time": "2025-03-02T17:37:09.353051",
     "exception": false,
     "start_time": "2025-03-02T17:37:05.871443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first_best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8d59eae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:37:16.421104Z",
     "iopub.status.busy": "2025-03-02T17:37:16.420791Z",
     "iopub.status.idle": "2025-03-02T17:37:16.424038Z",
     "shell.execute_reply": "2025-03-02T17:37:16.423337Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.617438,
     "end_time": "2025-03-02T17:37:16.425254",
     "exception": false,
     "start_time": "2025-03-02T17:37:12.807816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initial_test_preds = inference(first_best_index, train_df, test_df)\n",
    "# initial_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05bef6dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:37:23.475532Z",
     "iopub.status.busy": "2025-03-02T17:37:23.475191Z",
     "iopub.status.idle": "2025-03-02T17:37:23.478405Z",
     "shell.execute_reply": "2025-03-02T17:37:23.477725Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.434146,
     "end_time": "2025-03-02T17:37:23.479555",
     "exception": false,
     "start_time": "2025-03-02T17:37:20.045409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2830ef5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:37:30.380278Z",
     "iopub.status.busy": "2025-03-02T17:37:30.379952Z",
     "iopub.status.idle": "2025-03-02T17:37:30.383368Z",
     "shell.execute_reply": "2025-03-02T17:37:30.382523Z"
    },
    "papermill": {
     "duration": 3.451747,
     "end_time": "2025-03-02T17:37:30.384758",
     "exception": false,
     "start_time": "2025-03-02T17:37:26.933011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_preds = []\n",
    "# for model, weight in model_weights.items():\n",
    "#     print(f\"Using model : {model}\")\n",
    "#     test_df            = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "    \n",
    "#     test_preds         = inference(model, train_df.copy(), test_df)\n",
    "#     test_preds         = (1-weight) * rankdata(initial_test_preds) + weight * rankdata(test_preds)\n",
    "#     initial_test_preds = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48fb5fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:37:37.465185Z",
     "iopub.status.busy": "2025-03-02T17:37:37.464864Z",
     "iopub.status.idle": "2025-03-02T17:37:37.468296Z",
     "shell.execute_reply": "2025-03-02T17:37:37.467560Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.457151,
     "end_time": "2025-03-02T17:37:37.469472",
     "exception": false,
     "start_time": "2025-03-02T17:37:34.012321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df               = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n",
    "# test_df                = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "# test_preds             = np.zeros(len(test_df))\n",
    "\n",
    "\n",
    "# preds_dict = {}\n",
    "# for model, weight in model_weights.items():\n",
    "#     print(f\"exp name : {model}\\n\")\n",
    "#     test_df                = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "#     test_preds             = inference(model, train_df.copy(), test_df.copy())\n",
    "#     test_preds             = (1-weight) * rankdata(initial_test_preds) + weight * rankdata(test_preds)\n",
    "#     initial_test_preds     = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84184019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:37:44.518788Z",
     "iopub.status.busy": "2025-03-02T17:37:44.518425Z",
     "iopub.status.idle": "2025-03-02T17:37:44.521571Z",
     "shell.execute_reply": "2025-03-02T17:37:44.520994Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.563817,
     "end_time": "2025-03-02T17:37:44.522840",
     "exception": false,
     "start_time": "2025-03-02T17:37:40.959023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1971f2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:37:51.683890Z",
     "iopub.status.busy": "2025-03-02T17:37:51.683540Z",
     "iopub.status.idle": "2025-03-02T17:37:51.686578Z",
     "shell.execute_reply": "2025-03-02T17:37:51.685899Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.722058,
     "end_time": "2025-03-02T17:37:51.687850",
     "exception": false,
     "start_time": "2025-03-02T17:37:47.965792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# potential_ensemble                = pd.DataFrame()\n",
    "# potential_ensemble[\"predictions\"] = test_preds\n",
    "# potential_ensemble[\"ID\"]          = test_df[\"ID\"]\n",
    "# new_score                         = score(test_df[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy(), potential_ensemble.copy(), \"ID\")\n",
    "# new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ac82376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:37:58.596609Z",
     "iopub.status.busy": "2025-03-02T17:37:58.596293Z",
     "iopub.status.idle": "2025-03-02T17:37:58.599510Z",
     "shell.execute_reply": "2025-03-02T17:37:58.598846Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.455744,
     "end_time": "2025-03-02T17:37:58.600821",
     "exception": false,
     "start_time": "2025-03-02T17:37:55.145077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca86df",
   "metadata": {
    "papermill": {
     "duration": 3.46274,
     "end_time": "2025-03-02T17:38:05.533531",
     "exception": false,
     "start_time": "2025-03-02T17:38:02.070791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 7: Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c768d7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:38:12.599994Z",
     "iopub.status.busy": "2025-03-02T17:38:12.599668Z",
     "iopub.status.idle": "2025-03-02T17:38:12.602777Z",
     "shell.execute_reply": "2025-03-02T17:38:12.602038Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.434702,
     "end_time": "2025-03-02T17:38:12.604082",
     "exception": false,
     "start_time": "2025-03-02T17:38:09.169380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub            = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n",
    "# sub.prediction = test_preds\n",
    "# sub.to_csv(\"submission.csv\",index=False)\n",
    "# print(\"Sub shape:\",sub.shape)\n",
    "# sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6acff",
   "metadata": {
    "papermill": {
     "duration": 3.609341,
     "end_time": "2025-03-02T17:38:19.665576",
     "exception": false,
     "start_time": "2025-03-02T17:38:16.056235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6226248,
     "sourceId": 10097128,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6415434,
     "sourceId": 10370860,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6450507,
     "sourceId": 10409094,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 211322530,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 216172723,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 217481514,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 217813503,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 217931837,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 218131499,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 218144682,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 218801338,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 218802756,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219153168,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219154661,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219195490,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219311236,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219519933,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219788453,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 219953795,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 220208646,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 220212754,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 220243528,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 220393279,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 221224474,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 221474387,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 221524326,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 221636013,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 221640769,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 221830134,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 222021427,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 222265918,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 222267362,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 222268629,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 222637056,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 222638468,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 222639128,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 222646580,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 222647954,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 222688114,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 222831405,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 223111714,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 223730449,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 223733528,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 223734261,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 223847496,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 224446847,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 224920880,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 225083324,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 225297866,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22683.117295,
   "end_time": "2025-03-02T17:38:25.349427",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-02T11:20:22.232132",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
