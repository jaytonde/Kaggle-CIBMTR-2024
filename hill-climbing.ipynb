{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":70942,"databundleVersionId":10381525,"sourceType":"competition"},{"sourceId":7453542,"sourceType":"datasetVersion","datasetId":921302},{"sourceId":10097128,"sourceType":"datasetVersion","datasetId":6226248},{"sourceId":10370860,"sourceType":"datasetVersion","datasetId":6415434},{"sourceId":10409094,"sourceType":"datasetVersion","datasetId":6450507},{"sourceId":211322530,"sourceType":"kernelVersion"},{"sourceId":216080876,"sourceType":"kernelVersion"},{"sourceId":216172723,"sourceType":"kernelVersion"},{"sourceId":216576876,"sourceType":"kernelVersion"},{"sourceId":216836411,"sourceType":"kernelVersion"},{"sourceId":217481514,"sourceType":"kernelVersion"},{"sourceId":217558741,"sourceType":"kernelVersion"},{"sourceId":217813503,"sourceType":"kernelVersion"},{"sourceId":217931837,"sourceType":"kernelVersion"},{"sourceId":217941306,"sourceType":"kernelVersion"},{"sourceId":218131499,"sourceType":"kernelVersion"},{"sourceId":218144682,"sourceType":"kernelVersion"},{"sourceId":218434656,"sourceType":"kernelVersion"}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installations","metadata":{"_uuid":"9daabf89-8847-4537-a0b7-2a722b5c452d","_cell_guid":"46a264b2-b97f-487b-9a3d-2597a0ecdc21","collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:55:26.315401Z","iopub.execute_input":"2025-01-22T13:55:26.315695Z","iopub.status.idle":"2025-01-22T13:55:45.722159Z","shell.execute_reply.started":"2025-01-22T13:55:26.315664Z","shell.execute_reply":"2025-01-22T13:55:45.721057Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd==1.7.0) (1.26.4)\nautograd is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nProcessing /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: autograd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.7.0)\nRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.13.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd>=1.2.0->autograd-gamma==0.5.0) (1.26.4)\nBuilding wheels for collected packages: autograd-gamma\n  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=8ad715d820b47a9d27511384f345c4b32445cc9271ef8860ad0f0de7dcedad39\n  Stored in directory: /root/.cache/pip/wheels/6b/b5/e0/4c79e15c0b5f2c15ecf613c720bb20daab20a666eb67135155\nSuccessfully built autograd-gamma\nInstalling collected packages: autograd-gamma\nSuccessfully installed autograd-gamma-0.5.0\nProcessing /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\nInstalling collected packages: interface-meta\nSuccessfully installed interface-meta-1.3.0\nProcessing /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\nRequirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.3.0)\nRequirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.26.4)\nRequirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (2.1.4)\nRequirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.13.1)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (4.12.2)\nRequirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->formulaic==1.0.2) (1.16.0)\nInstalling collected packages: formulaic\nSuccessfully installed formulaic-1.0.2\nProcessing /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.26.4)\nRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.13.1)\nRequirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (2.1.4)\nRequirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (3.7.1)\nRequirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.7.0)\nRequirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (0.5.0)\nRequirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.0.2)\nRequirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.3.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (4.12.2)\nRequirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.16.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (24.1)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (10.4.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines==0.30.0) (1.16.0)\nInstalling collected packages: lifelines\nSuccessfully installed lifelines-0.30.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --no-index -U --find-links=/kaggle/input/tabm-tabular-dl-library tabm==0.0.1.dev0\n!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:55:45.723196Z","iopub.execute_input":"2025-01-22T13:55:45.723454Z","iopub.status.idle":"2025-01-22T13:55:52.279816Z","shell.execute_reply.started":"2025-01-22T13:55:45.723433Z","shell.execute_reply":"2025-01-22T13:55:52.278641Z"}},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/tabm-tabular-dl-library\nProcessing /kaggle/input/tabm-tabular-dl-library/tabm-0.0.1.dev0-py3-none-any.whl\nRequirement already satisfied: torch<3,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tabm==0.0.1.dev0) (2.4.1+cu121)\nProcessing /kaggle/input/tabm-tabular-dl-library/rtdl_num_embeddings-0.0.11-py3-none-any.whl (from tabm==0.0.1.dev0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.12->tabm==0.0.1.dev0) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.12->tabm==0.0.1.dev0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1.12->tabm==0.0.1.dev0) (1.3.0)\nInstalling collected packages: rtdl_num_embeddings, tabm\nSuccessfully installed rtdl_num_embeddings-0.0.11 tabm-0.0.1.dev0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip -q install /kaggle/input/tabpfn-v2/tabpfn-2.0.0-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:55:52.280940Z","iopub.execute_input":"2025-01-22T13:55:52.281258Z","iopub.status.idle":"2025-01-22T13:55:55.549369Z","shell.execute_reply.started":"2025-01-22T13:55:52.281225Z","shell.execute_reply":"2025-01-22T13:55:55.548430Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Step 1 : Installations","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import rankdata \n\nimport sys\nsys.path.append('/kaggle/input/tabm-tabular-dl-library')\n\nimport os\nimport tabm\nimport math\nimport torch\nimport random\nimport warnings\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport rtdl_num_embeddings\nimport matplotlib.pyplot as plt\nfrom typing import Optional, Tuple\nfrom sklearn.model_selection import KFold\nfrom scipy.stats import rankdata \nfrom colorama import Fore, Style\nfrom typing import Optional, Tuple\nfrom numpy.typing import ArrayLike\nfrom sklearn.base import BaseEstimator\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import KFold\nfrom tabm_reference import Model, make_parameter_groups\nfrom sklearn.preprocessing import OrdinalEncoder, QuantileTransformer\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Input, Embedding\nfrom tensorflow.keras.layers import Concatenate, BatchNormalization\nimport tensorflow.keras.backend as K\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom tabpfn import TabPFNRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:56:21.656711Z","iopub.execute_input":"2025-01-22T13:56:21.657030Z","iopub.status.idle":"2025-01-22T13:56:33.921704Z","shell.execute_reply.started":"2025-01-22T13:56:21.657005Z","shell.execute_reply":"2025-01-22T13:56:33.920829Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Step 2: Experiments Paths  to add in ensemble","metadata":{}},{"cell_type":"code","source":"experiments = [\n\"/kaggle/input/xgboost-exp-01\",\n\"/kaggle/input/catboost-exp-01\",\n\"/kaggle/input/lgbm-exp-01\",\n\"/kaggle/input/xgboost-exp-02\",\n\"/kaggle/input/catboost-exp-02\",\n\"/kaggle/input/lgbm-exp-03\",\n\"/kaggle/input/catboost-exp-03\",\n\"/kaggle/input/tabm-exp-01\",\n\"/kaggle/input/nn-exp-01\",\n\"/kaggle/input/tn-exp-01\",\n\"/kaggle/input/tf-exp-01\",\n\"/kaggle/input/svr-exp-01\",\n# \"/kaggle/input/suv-ran-exp-01\" -> File not found\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:56:37.433077Z","iopub.execute_input":"2025-01-22T13:56:37.433745Z","iopub.status.idle":"2025-01-22T13:56:37.437760Z","shell.execute_reply.started":"2025-01-22T13:56:37.433710Z","shell.execute_reply":"2025-01-22T13:56:37.436794Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Step 3 : Competition metric","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport pandas.api.types\nimport numpy as np\nfrom lifelines.utils import concordance_index\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    >>> import pandas as pd\n    >>> row_id_column_name = \"id\"\n    >>> y_pred = {'prediction': {0: 1.0, 1: 0.0, 2: 1.0}}\n    >>> y_pred = pd.DataFrame(y_pred)\n    >>> y_pred.insert(0, row_id_column_name, range(len(y_pred)))\n    >>> y_true = { 'efs': {0: 1.0, 1: 0.0, 2: 0.0}, 'efs_time': {0: 25.1234,1: 250.1234,2: 2500.1234}, 'race_group': {0: 'race_group_1', 1: 'race_group_1', 2: 'race_group_1'}}\n    >>> y_true = pd.DataFrame(y_true)\n    >>> y_true.insert(0, row_id_column_name, range(len(y_true)))\n    >>> score(y_true.copy(), y_pred.copy(), row_id_column_name)\n    0.75\n    \"\"\"\n\n    del solution[row_id_column_name]\n    del submission[row_id_column_name]\n    \n    event_label = 'efs'\n    interval_label = 'efs_time'\n    prediction_label = 'predictions'\n    for col in submission.columns:\n        if not pandas.api.types.is_numeric_dtype(submission[col]):\n            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n    # Merging solution and submission dfs on ID\n    merged_df = pd.concat([solution, submission], axis=1)\n    merged_df.reset_index(inplace=True)\n    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n    metric_list = []\n\n    for race in merged_df_race_dict.keys():\n        # Retrieving values from y_test based on index\n        indices = sorted(merged_df_race_dict[race])\n        merged_df_race = merged_df.iloc[indices]\n        # Calculate the concordance index\n        c_index_race = concordance_index(\n                        merged_df_race[interval_label],\n                        -merged_df_race[prediction_label],\n                        merged_df_race[event_label])\n        metric_list.append(c_index_race)\n    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:56:39.673208Z","iopub.execute_input":"2025-01-22T13:56:39.673559Z","iopub.status.idle":"2025-01-22T13:56:39.765528Z","shell.execute_reply.started":"2025-01-22T13:56:39.673493Z","shell.execute_reply":"2025-01-22T13:56:39.764900Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Step 4: Find best model ","metadata":{}},{"cell_type":"code","source":"def compute_metric_cindex(exp_name):\n    oof    = pd.read_excel(exp_name+'/'+(exp_name.split('/')[-1]).replace('-','_')+'_oof.xlsx')\n    y_true = oof[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n    y_pred = oof[[\"ID\",\"predictions\"]].copy()\n    return score(y_true.copy(), y_pred.copy(), \"ID\"), oof","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:56:42.721093Z","iopub.execute_input":"2025-01-22T13:56:42.721400Z","iopub.status.idle":"2025-01-22T13:56:42.725907Z","shell.execute_reply.started":"2025-01-22T13:56:42.721377Z","shell.execute_reply":"2025-01-22T13:56:42.724947Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"best_score    = 0\nbest_index    = -1\nbest_ensemble = 0\n\nfor k,name in enumerate(experiments):\n    s, oof = compute_metric_cindex(name)\n    if s > best_score:\n        best_score    = s\n        best_index    = name\n        best_ensemble = oof\n        \n    print(f'C-index {s} {name}') \nprint()\nprint(f'Best single model is {best_index} with C-Index = {best_score}')\nexperiments.remove(best_index)\nfirst_best_index = best_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:56:43.592255Z","iopub.execute_input":"2025-01-22T13:56:43.592608Z","iopub.status.idle":"2025-01-22T14:00:36.102447Z","shell.execute_reply.started":"2025-01-22T13:56:43.592577Z","shell.execute_reply":"2025-01-22T14:00:36.101511Z"}},"outputs":[{"name":"stdout","text":"C-index 0.6743662038463064 /kaggle/input/xgboost-exp-01\nC-index 0.6737253105870038 /kaggle/input/catboost-exp-01\nC-index 0.673512045762966 /kaggle/input/lgbm-exp-01\nC-index 0.6719739976829171 /kaggle/input/xgboost-exp-02\nC-index 0.6715438291634074 /kaggle/input/catboost-exp-02\nC-index 0.6730595395519774 /kaggle/input/lgbm-exp-03\nC-index 0.6750446351219317 /kaggle/input/catboost-exp-03\nC-index 0.6692934020445873 /kaggle/input/tabm-exp-01\nC-index 0.6672504731273489 /kaggle/input/nn-exp-01\nC-index 0.6082304286386713 /kaggle/input/tn-exp-01\nC-index 0.6609106027219831 /kaggle/input/tf-exp-01\nC-index 0.6061054384127524 /kaggle/input/svr-exp-01\n\nBest single model is /kaggle/input/catboost-exp-03 with C-Index = 0.6750446351219317\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"experiments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:00:48.964912Z","iopub.execute_input":"2025-01-22T14:00:48.965495Z","iopub.status.idle":"2025-01-22T14:00:48.971344Z","shell.execute_reply.started":"2025-01-22T14:00:48.965467Z","shell.execute_reply":"2025-01-22T14:00:48.970564Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['/kaggle/input/xgboost-exp-01',\n '/kaggle/input/catboost-exp-01',\n '/kaggle/input/lgbm-exp-01',\n '/kaggle/input/xgboost-exp-02',\n '/kaggle/input/catboost-exp-02',\n '/kaggle/input/lgbm-exp-03',\n '/kaggle/input/tabm-exp-01',\n '/kaggle/input/nn-exp-01',\n '/kaggle/input/tn-exp-01',\n '/kaggle/input/tf-exp-01',\n '/kaggle/input/svr-exp-01']"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Step 5: Iterations for hill climbing","metadata":{}},{"cell_type":"code","source":"USE_NEGATIVE_WGT = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:00:50.098179Z","iopub.execute_input":"2025-01-22T14:00:50.098495Z","iopub.status.idle":"2025-01-22T14:00:50.102256Z","shell.execute_reply.started":"2025-01-22T14:00:50.098471Z","shell.execute_reply":"2025-01-22T14:00:50.101360Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"indices        = [best_index]\nold_best_score = best_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:00:50.345680Z","iopub.execute_input":"2025-01-22T14:00:50.345974Z","iopub.status.idle":"2025-01-22T14:00:50.349601Z","shell.execute_reply.started":"2025-01-22T14:00:50.345953Z","shell.execute_reply":"2025-01-22T14:00:50.348898Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# PREPARE/MOVE VARIABLES TO GPU FOR SPEED UP\nbest_ensemble = best_ensemble\nstart         = -0.50\nif not USE_NEGATIVE_WGT: start = 0.01\nww            = np.arange(start,0.51,0.01) # GPU\nnn            = len(ww)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:00:52.571897Z","iopub.execute_input":"2025-01-22T14:00:52.572194Z","iopub.status.idle":"2025-01-22T14:00:52.576573Z","shell.execute_reply.started":"2025-01-22T14:00:52.572169Z","shell.execute_reply":"2025-01-22T14:00:52.575595Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# BEGIN HILL CLIMBING\nmodels  = [best_index]\nweights = []\nmetrics = [best_score]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:00:52.805117Z","iopub.execute_input":"2025-01-22T14:00:52.805428Z","iopub.status.idle":"2025-01-22T14:00:52.809325Z","shell.execute_reply.started":"2025-01-22T14:00:52.805404Z","shell.execute_reply":"2025-01-22T14:00:52.808557Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"models, metrics, ww","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:00:53.288825Z","iopub.execute_input":"2025-01-22T14:00:53.289143Z","iopub.status.idle":"2025-01-22T14:00:53.295211Z","shell.execute_reply.started":"2025-01-22T14:00:53.289116Z","shell.execute_reply":"2025-01-22T14:00:53.294348Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(['/kaggle/input/catboost-exp-03'],\n [0.6750446351219317],\n array([-5.0000000e-01, -4.9000000e-01, -4.8000000e-01, -4.7000000e-01,\n        -4.6000000e-01, -4.5000000e-01, -4.4000000e-01, -4.3000000e-01,\n        -4.2000000e-01, -4.1000000e-01, -4.0000000e-01, -3.9000000e-01,\n        -3.8000000e-01, -3.7000000e-01, -3.6000000e-01, -3.5000000e-01,\n        -3.4000000e-01, -3.3000000e-01, -3.2000000e-01, -3.1000000e-01,\n        -3.0000000e-01, -2.9000000e-01, -2.8000000e-01, -2.7000000e-01,\n        -2.6000000e-01, -2.5000000e-01, -2.4000000e-01, -2.3000000e-01,\n        -2.2000000e-01, -2.1000000e-01, -2.0000000e-01, -1.9000000e-01,\n        -1.8000000e-01, -1.7000000e-01, -1.6000000e-01, -1.5000000e-01,\n        -1.4000000e-01, -1.3000000e-01, -1.2000000e-01, -1.1000000e-01,\n        -1.0000000e-01, -9.0000000e-02, -8.0000000e-02, -7.0000000e-02,\n        -6.0000000e-02, -5.0000000e-02, -4.0000000e-02, -3.0000000e-02,\n        -2.0000000e-02, -1.0000000e-02,  4.4408921e-16,  1.0000000e-02,\n         2.0000000e-02,  3.0000000e-02,  4.0000000e-02,  5.0000000e-02,\n         6.0000000e-02,  7.0000000e-02,  8.0000000e-02,  9.0000000e-02,\n         1.0000000e-01,  1.1000000e-01,  1.2000000e-01,  1.3000000e-01,\n         1.4000000e-01,  1.5000000e-01,  1.6000000e-01,  1.7000000e-01,\n         1.8000000e-01,  1.9000000e-01,  2.0000000e-01,  2.1000000e-01,\n         2.2000000e-01,  2.3000000e-01,  2.4000000e-01,  2.5000000e-01,\n         2.6000000e-01,  2.7000000e-01,  2.8000000e-01,  2.9000000e-01,\n         3.0000000e-01,  3.1000000e-01,  3.2000000e-01,  3.3000000e-01,\n         3.4000000e-01,  3.5000000e-01,  3.6000000e-01,  3.7000000e-01,\n         3.8000000e-01,  3.9000000e-01,  4.0000000e-01,  4.1000000e-01,\n         4.2000000e-01,  4.3000000e-01,  4.4000000e-01,  4.5000000e-01,\n         4.6000000e-01,  4.7000000e-01,  4.8000000e-01,  4.9000000e-01,\n         5.0000000e-01]))"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"%%time\n\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Tuple\nimport logging\nimport gc\n\ndef optimize_ensemble(experiments, initial_ensemble, weights_range, score_function, max_iterations = 100):\n    \n    # Initialize variables\n    iteration  = 0\n    best_score = score_function(\n        initial_ensemble[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy(),\n        initial_ensemble[[\"ID\", \"predictions\"]].copy(),\n        \"ID\"\n    )\n    \n    model_weights         = {}\n    remaining_experiments = experiments.copy()\n    best_ensemble         = initial_ensemble.copy()\n    \n    while remaining_experiments and iteration < max_iterations:\n        print(f\"{iteration}th iteration\")\n        iteration += 1\n        best_iteration = {\n            'index' : -1,\n            'weight': 0,\n            'score' : best_score\n        }\n        \n        # Try each remaining model\n        for model_path in remaining_experiments:\n            try:\n                print(f\"Iteration {iteration}: Trying model {model_path}\")\n                \n                # Load model OOF predictions\n                model_name = model_path.split(\"/\")[-1].replace('-', '_')\n                oof_path   = f\"{model_path}/{model_name}_oof.xlsx\"\n                model_oof  = pd.read_excel(oof_path)\n                \n                # Try different weights\n                for weight in tqdm(weights_range, desc=f\"Testing weights for {model_name}\"):\n                    # Create potential ensemble\n                    potential_ensemble = pd.DataFrame({\n                        \"ID\": best_ensemble[\"ID\"],\n                        \"predictions\": (1 - weight) * rankdata(best_ensemble[\"predictions\"]) + \n                                     weight * rankdata(model_oof[\"predictions\"])\n                    })\n                    \n                    # Evaluate new ensemble\n                    new_score = score_function(\n                        model_oof[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy(),\n                        potential_ensemble.copy(),\n                        \"ID\"\n                    )\n                    \n                    # Update best if improved\n                    if new_score > best_iteration['score']:\n                        best_iteration.update({\n                            'index' : model_path,\n                            'weight': weight,\n                            'score' : new_score\n                        })\n                \n                # Clean up\n                del model_oof\n                gc.collect()\n                \n            except Exception as e:\n                print(f\"Error processing {model_path}: {str(e)}\")\n                continue\n        \n        # Check if we found an improvement\n        if best_iteration['index'] == -1:\n            print(\"No improvement found, stopping\")\n            print(best_iteration)\n            break\n            \n        # Update ensemble with best model found\n        best_score                             = best_iteration['score']\n        model_weights[best_iteration['index']] = best_iteration['weight']\n        remaining_experiments.remove(best_iteration['index'])\n        \n        print(\n            f\"Iteration {iteration}: Added {best_iteration['index']} \"\n            f\"with weight {best_iteration['weight']:.4f}, \"\n            f\"Score: {best_iteration['score']:.4f}\"\n        )\n        \n        # Update best ensemble for next iteration\n        model_oof = pd.read_excel(\n            f\"{best_iteration['index']}/{best_iteration['index'].split('/')[-1].replace('-', '_')}_oof.xlsx\"\n        )\n        best_ensemble[\"predictions\"] = (\n            (1 - best_iteration['weight']) * rankdata(best_ensemble[\"predictions\"]) + \n            best_iteration['weight'] * rankdata(model_oof[\"predictions\"])\n        )\n        \n    return model_weights, best_score\n\n\nmodel_weights, best_score = optimize_ensemble(experiments, best_ensemble, ww, score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:00:57.304188Z","iopub.execute_input":"2025-01-22T14:00:57.304545Z","iopub.status.idle":"2025-01-22T15:00:13.992687Z","shell.execute_reply.started":"2025-01-22T14:00:57.304484Z","shell.execute_reply":"2025-01-22T15:00:13.991891Z"}},"outputs":[{"name":"stdout","text":"0th iteration\nIteration 1: Trying model /kaggle/input/xgboost-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1: Trying model /kaggle/input/catboost-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for catboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1: Trying model /kaggle/input/lgbm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1: Trying model /kaggle/input/xgboost-exp-02\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for xgboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1: Trying model /kaggle/input/catboost-exp-02\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1: Trying model /kaggle/input/lgbm-exp-03\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1: Trying model /kaggle/input/tabm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1: Trying model /kaggle/input/nn-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1: Trying model /kaggle/input/tn-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1: Trying model /kaggle/input/tf-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1: Trying model /kaggle/input/svr-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1: Added /kaggle/input/xgboost-exp-02 with weight 0.4300, Score: 0.6787\n1th iteration\nIteration 2: Trying model /kaggle/input/xgboost-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2: Trying model /kaggle/input/catboost-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for catboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2: Trying model /kaggle/input/lgbm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2: Trying model /kaggle/input/catboost-exp-02\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2: Trying model /kaggle/input/lgbm-exp-03\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2: Trying model /kaggle/input/tabm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2: Trying model /kaggle/input/nn-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2: Trying model /kaggle/input/tn-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2: Trying model /kaggle/input/tf-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2: Trying model /kaggle/input/svr-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 2: Added /kaggle/input/catboost-exp-01 with weight 0.3400, Score: 0.6805\n2th iteration\nIteration 3: Trying model /kaggle/input/xgboost-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 3: Trying model /kaggle/input/lgbm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 3: Trying model /kaggle/input/catboost-exp-02\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 3: Trying model /kaggle/input/lgbm-exp-03\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 3: Trying model /kaggle/input/tabm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 3: Trying model /kaggle/input/nn-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for nn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 3: Trying model /kaggle/input/tn-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 3: Trying model /kaggle/input/tf-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 3: Trying model /kaggle/input/svr-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 3: Added /kaggle/input/nn-exp-01 with weight 0.2000, Score: 0.6813\n3th iteration\nIteration 4: Trying model /kaggle/input/xgboost-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for xgboost_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 4: Trying model /kaggle/input/lgbm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 4: Trying model /kaggle/input/catboost-exp-02\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 4: Trying model /kaggle/input/lgbm-exp-03\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 4: Trying model /kaggle/input/tabm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 4: Trying model /kaggle/input/tn-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 4: Trying model /kaggle/input/tf-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 4: Trying model /kaggle/input/svr-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 4: Added /kaggle/input/xgboost-exp-01 with weight 0.1300, Score: 0.6815\n4th iteration\nIteration 5: Trying model /kaggle/input/lgbm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 5: Trying model /kaggle/input/catboost-exp-02\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 5: Trying model /kaggle/input/lgbm-exp-03\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 5: Trying model /kaggle/input/tabm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tabm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 5: Trying model /kaggle/input/tn-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 5: Trying model /kaggle/input/tf-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 5: Trying model /kaggle/input/svr-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 5: Added /kaggle/input/tabm-exp-01 with weight 0.1300, Score: 0.6817\n5th iteration\nIteration 6: Trying model /kaggle/input/lgbm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 6: Trying model /kaggle/input/catboost-exp-02\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for catboost_exp_02: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 6: Trying model /kaggle/input/lgbm-exp-03\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 6: Trying model /kaggle/input/tn-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:32<00:00,  3.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 6: Trying model /kaggle/input/tf-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 6: Trying model /kaggle/input/svr-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 6: Added /kaggle/input/catboost-exp-02 with weight 0.0600, Score: 0.6817\n6th iteration\nIteration 7: Trying model /kaggle/input/lgbm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 7: Trying model /kaggle/input/lgbm-exp-03\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_03: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 7: Trying model /kaggle/input/tn-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 7: Trying model /kaggle/input/tf-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 7: Trying model /kaggle/input/svr-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 7: Added /kaggle/input/lgbm-exp-03 with weight 0.0700, Score: 0.6818\n7th iteration\nIteration 8: Trying model /kaggle/input/lgbm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 8: Trying model /kaggle/input/tn-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tn_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 8: Trying model /kaggle/input/tf-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 8: Trying model /kaggle/input/svr-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 8: Added /kaggle/input/tn-exp-01 with weight -0.0300, Score: 0.6819\n8th iteration\nIteration 9: Trying model /kaggle/input/lgbm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 9: Trying model /kaggle/input/tf-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 9: Trying model /kaggle/input/svr-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for svr_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 9: Added /kaggle/input/svr-exp-01 with weight -0.0100, Score: 0.6819\n9th iteration\nIteration 10: Trying model /kaggle/input/lgbm-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for lgbm_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 10: Trying model /kaggle/input/tf-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 10: Added /kaggle/input/lgbm-exp-01 with weight 0.0300, Score: 0.6819\n10th iteration\nIteration 11: Trying model /kaggle/input/tf-exp-01\n","output_type":"stream"},{"name":"stderr","text":"Testing weights for tf_exp_01: 100%|██████████| 101/101 [00:31<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Iteration 11: Added /kaggle/input/tf-exp-01 with weight 0.0000, Score: 0.6819\nCPU times: user 59min 16s, sys: 3.44 s, total: 59min 19s\nWall time: 59min 16s\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"model_weights, best_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:20:53.032153Z","iopub.execute_input":"2025-01-22T15:20:53.032521Z","iopub.status.idle":"2025-01-22T15:20:53.037883Z","shell.execute_reply.started":"2025-01-22T15:20:53.032475Z","shell.execute_reply":"2025-01-22T15:20:53.037055Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"({'/kaggle/input/xgboost-exp-02': 0.4300000000000008,\n  '/kaggle/input/catboost-exp-01': 0.34000000000000075,\n  '/kaggle/input/nn-exp-01': 0.20000000000000062,\n  '/kaggle/input/xgboost-exp-01': 0.13000000000000056,\n  '/kaggle/input/tabm-exp-01': 0.13000000000000056,\n  '/kaggle/input/catboost-exp-02': 0.0600000000000005,\n  '/kaggle/input/lgbm-exp-03': 0.0700000000000005,\n  '/kaggle/input/tn-exp-01': -0.029999999999999583,\n  '/kaggle/input/svr-exp-01': -0.009999999999999565,\n  '/kaggle/input/lgbm-exp-01': 0.03000000000000047,\n  '/kaggle/input/tf-exp-01': 4.440892098500626e-16},\n 0.6818919354327349)"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"first_best_index, experiments","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Inference on test","metadata":{}},{"cell_type":"code","source":"# class CFG:\n#     folds = 10\n\n# # https://www.kaggle.com/datasets/jsday96/mcts-tabm-models/data?select=TabMRegressor.py\n# class TabMRegressor:\n#     def __init__(\n#         self,\n#         arch_type: str        = 'tabm-mini',\n#         backbone: dict        = {'type': 'MLP', 'n_blocks': 3, 'd_block': 512, 'dropout': 0.1},\n#         d_embedding: int      = 64,  # Only used for 'tabm-mini'\n#         bin_count: int        = 48,  # Only used for 'tabm-mini'\n#         k: int                = 32,\n#         learning_rate: float  = 1e-4,\n#         weight_decay: float   = 1e-3,\n#         clip_grad_norm: bool  = True,\n#         max_epochs: int       = 100,\n#         patience: int         = 15,\n#         batch_size: int       = 32,\n#         compile_model: bool   = False,\n#         device: Optional[str] = 'cuda:0',\n#         random_state: int     = 0,\n#         verbose: bool         = True\n#     ):\n#         self.arch_type = arch_type\n#         self.backbone = backbone\n#         self.d_embedding = d_embedding\n#         self.bin_count = bin_count\n#         self.k = k\n#         self.learning_rate = learning_rate\n#         self.weight_decay = weight_decay\n#         self.clip_grad_norm = clip_grad_norm\n#         self.max_epochs = max_epochs\n#         self.patience = patience\n#         self.batch_size = batch_size\n#         self.compile_model = compile_model\n#         self.device = torch.device(device if device else ('cuda' if torch.cuda.is_available() else 'cpu'))\n#         self.random_state = random_state\n#         self.verbose = verbose\n\n#     def fit(\n#         self,\n#         X: pd.DataFrame,\n#         y: np.array,\n#         eval_set: Tuple[pd.DataFrame, np.array]\n#     ):\n#         # PREPROCESS DATA.\n#         X_cat_train, X_cont_train, cat_cardinalities, y_train = self._preprocess_data(X, y, training=True)\n#         X_cat_val, X_cont_val, _, y_val = self._preprocess_data(eval_set[0], eval_set[1], training=False)\n\n#         # CREATE MODEL & TRAINING ALGO.\n#         bins = rtdl_num_embeddings.compute_bins(X_cont_train, n_bins=self.bin_count) if self.arch_type == 'tabm-mini' else None\n#         self.model = Model(\n#             n_num_features=X_cont_train.shape[1],\n#             cat_cardinalities=cat_cardinalities,\n#             n_classes=None,\n#             backbone=self.backbone,\n#             bins=bins,\n#             num_embeddings=(\n#                 None\n#                 if bins is None\n#                 else {\n#                     'type': 'PiecewiseLinearEmbeddings',\n#                     'd_embedding': self.d_embedding,\n#                     'activation': True,\n#                     'version': 'B',\n#                 }\n#             ),\n#             arch_type=self.arch_type,\n#             k=self.k,\n#         ).to(self.device)\n#         optimizer = torch.optim.AdamW(make_parameter_groups(self.model), lr=self.learning_rate, weight_decay=self.weight_decay)\n#         if self.compile_model:\n#             self.model = torch.compile(self.model)\n\n#         loss_fn = torch.nn.MSELoss().to(self.device)\n#         # TRAIN & TEST MODEL.\n#         best = {\n#             'epoch': -1,\n#             'eval_loss': math.inf,\n#             'model_state_dict': None,\n#         }\n#         remaining_patience = self.patience\n#         epoch_size = math.ceil(len(X) / self.batch_size)\n\n\n#         for epoch in range(self.max_epochs):\n#             # TRAIN.\n#             optimizer.zero_grad()\n#             train_losses = []\n#             progress_bar = torch.randperm(len(y_train), device=self.device).split(self.batch_size)\n#             progress_bar = tqdm(progress_bar, desc=f'Epoch {epoch}', total=epoch_size) if self.verbose else progress_bar\n#             for batch_idx in progress_bar:\n#                 self.model.train()\n\n#                 with torch.amp.autocast(device_type='cuda', dtype = torch.bfloat16):\n#                     y_pred = self.model(\n#                         X_cont_train[batch_idx],\n#                         X_cat_train[batch_idx],\n#                     ).squeeze(-1).float()\n\n#                 loss = loss_fn(y_pred.flatten(0, 1), y_train[batch_idx].repeat_interleave(self.k))\n#                 loss.backward()\n#                 if self.clip_grad_norm:\n#                     torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n#                 optimizer.step()\n\n#                 train_losses.append(loss.item())\n\n\n#              # EVALUATE.\n#             self.model.eval()\n#             val_losses = []\n#             with torch.no_grad():\n#                 for batch_idx in torch.arange(0, len(y_val), self.batch_size, device=self.device):\n#                     y_pred = self.model(\n#                         X_cont_val[batch_idx:batch_idx+self.batch_size],\n#                         X_cat_val[batch_idx:batch_idx+self.batch_size],\n#                     ).squeeze(-1).float()\n\n#                     loss = loss_fn(y_pred.flatten(0, 1), y_val[batch_idx:batch_idx+self.batch_size].repeat_interleave(self.k))\n#                     val_losses.append(loss.item())\n\n\n#             # PRINT INFO.\n#             mean_train_loss = np.mean(train_losses)\n#             mean_val_loss = np.mean(val_losses)\n#             if self.verbose:\n#                 print(f'Epoch {epoch} | Train Loss: {mean_train_loss} | Val Loss: {mean_val_loss}')\n\n\n#             # COMPARE TO BEST.\n#             if mean_val_loss < best['eval_loss']:\n#                 best['epoch'] = epoch\n#                 best['eval_loss'] = mean_val_loss\n#                 best['model_state_dict'] = self.model.state_dict()\n#                 remaining_patience = self.patience\n                \n#                 if self.verbose:\n#                     print('🌸 New best epoch! 🌸')\n#             else:\n#                 remaining_patience -= 1\n\n#             # EARLY STOPPING.\n#             if remaining_patience == 0:\n#                 break\n\n#             # RESTORE BEST MODEL.\n#             self.model.load_state_dict(best['model_state_dict'])\n\n\n#     def predict(\n#         self,\n#         X: pd.DataFrame,\n#         batch_size: Optional[int] = 8096\n#     ) -> np.ndarray:\n#         # PREPROCESS DATA.\n#         X_cat, X_cont, _, _ = self._preprocess_data(X, y=None, training=False)\n\n#         # PREDICT.\n#         self.model.eval()\n#         y_pred = []\n#         with torch.no_grad():\n#             for batch_idx in torch.arange(0, len(X), batch_size, device=self.device):\n#                 y_pred.append(\n#                     self.model(\n#                         X_cont[batch_idx:batch_idx+batch_size],\n#                         X_cat[batch_idx:batch_idx+batch_size],\n#                     ).squeeze(-1).float().cpu().numpy()\n#                 )\n\n#         y_pred = np.concatenate(y_pred)\n\n\n#         # DENORMALIZE TARGETS.\n#         y_pred = y_pred * self._target_std + self._target_mean\n\n\n#         # COMPUTE ENSEMBLE MEAN.\n#         y_pred = np.mean(y_pred, axis=1)\n\n#         return y_pred\n\n\n#     def _preprocess_data(self, X: pd.DataFrame, y: pd.Series, training: bool):\n#         # PICK NON-CONSTANT COLUMNS.\n#         if training:\n#             self._non_constant_columns = X.columns[X.nunique() > 1]\n\n#         X = X[self._non_constant_columns]\n\n#         # SEPARATE CATEGORICAL & CONTINUOUS FEATURES.\n#         categorical_features = [col for col in X.columns if X[col].dtype.name == 'object']\n#         X_cat = X[categorical_features].to_numpy()\n#         X_cont = X.drop(columns=categorical_features).to_numpy()\n\n#         # ENCODE CATEGORICAL FEATURES.\n#         cat_cardinalities = [X[col].nunique() for col in categorical_features]\n\n#         if training:\n#             self._categorical_encoders = [\n#                 OrdinalEncoder()\n#                 for _ in range(X_cat.shape[1])\n#             ]\n#         X_cat = np.concatenate([\n#             encoder.fit_transform(X_cat[:, i:i+1])\n#             for i, encoder in enumerate(self._categorical_encoders)\n#         ], axis=1)\n\n#         # NORMALIZE TARGETS.\n#         if training:\n#             self._target_mean = y.mean()\n#             self._target_std = y.std()\n\n#             y = (y - self._target_mean) / self._target_std\n\n\n#         # SCALE CONTINUOUS FEATURES.\n#         if training:\n#             noise = (\n#                 np.random.default_rng(0)\n#                 .normal(0.0, 1e-5, X_cont.shape)\n#                 .astype(X_cont.dtype)\n#             )\n#             self._cont_feature_preprocessor = QuantileTransformer(\n#                 n_quantiles=max(min(len(X) // 30, 1000), 10),\n#                 output_distribution='normal',\n#                 subsample=10**9,\n#             ).fit(X_cont + noise)\n\n#         X_cont = self._cont_feature_preprocessor.transform(X_cont)\n\n\n#         # CONVERT TO TENSORS.\n#         X_cat = torch.tensor(X_cat, dtype=torch.long, device=self.device)\n#         X_cont = torch.tensor(X_cont, dtype=torch.float32, device=self.device)\n\n#         if y is not None:\n#             y = torch.tensor(y, dtype=torch.float32, device=self.device)\n\n#         return X_cat, X_cont, cat_cardinalities, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:52:38.013044Z","iopub.execute_input":"2025-01-22T15:52:38.013333Z","iopub.status.idle":"2025-01-22T15:52:38.021341Z","shell.execute_reply.started":"2025-01-22T15:52:38.013310Z","shell.execute_reply":"2025-01-22T15:52:38.020210Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"# def get_tabm_features(data):\n#     RMV = [\"ID\",\"efs\",\"efs_time\",\"y\",\"fold\"]\n#     FEATURES = [c for c in data.columns if not c in RMV]\n    \n#     RMV              = ['ID']\n#     X_test           = data.drop(RMV, axis=1)\n#     y_pred           = data[['ID']]\n    \n#     #print(\"X_test shape:\", X_test.shape, '\\n')\n    \n#     cat_cols         = X_test.select_dtypes(include=['object']).columns.tolist()\n#     num_cols         = X_test.select_dtypes(exclude=['object']).columns.tolist()\n    \n#     # Preprocessing categorical\n#     imputer          = SimpleImputer(strategy='constant', fill_value='NAN')\n#     X_test[cat_cols] = imputer.fit_transform(X_test[cat_cols])\n\n#     # Preprocessing numerical\n#     imputer          = SimpleImputer(strategy=\"median\")\n#     X_test[num_cols] = imputer.fit_transform(X_test[num_cols])\n\n#     return X_test,FEATURES","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:52:27.416912Z","iopub.execute_input":"2025-01-22T15:52:27.417188Z","iopub.status.idle":"2025-01-22T15:52:27.420357Z","shell.execute_reply.started":"2025-01-22T15:52:27.417167Z","shell.execute_reply":"2025-01-22T15:52:27.419366Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# def prepare_features(model_path, train, test):\n\n#     RMV = [\"ID\",\"efs\",\"efs_time\",\"y\"]\n#     FEATURES = [c for c in train.columns if not c in RMV]\n#     #print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n    \n\n#     CATS = []\n#     for c in FEATURES:\n#         if train[c].dtype==\"object\":\n#             CATS.append(c)\n#             train[c] = train[c].fillna(\"NAN\")\n#             test[c]  = test[c].fillna(\"NAN\")\n#         elif \"DeepTabels\" in model_path or \"tn\" in model_path or \"svr\" in model_path:\n#             train[c] = train[c].fillna(-1)\n#             test[c]  = test[c].fillna(-1)\n            \n        \n#     #print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")\n    \n#     combined = pd.concat([train,test],axis=0,ignore_index=True)\n#     #print(\"Combined data shape:\", combined.shape )\n    \n#     # LABEL ENCODE CATEGORICAL FEATURES\n#     #print(\"We LABEL ENCODE the CATEGORICAL FEATURES: \",end=\"\")\n#     for c in FEATURES:\n    \n#         # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n#         if c in CATS:\n#             #print(f\"{c}, \",end=\"\")\n#             combined[c],_ = combined[c].factorize()\n#             combined[c]  -= combined[c].min()\n#             combined[c]   = combined[c].astype(\"int32\")\n#             combined[c]   = combined[c].astype(\"category\")\n            \n#         # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n#         else:\n#             if combined[c].dtype ==\"float64\":\n#                 combined[c]      = combined[c].astype(\"float32\")\n#             if combined[c].dtype ==\"int64\":\n#                 combined[c]      = combined[c].astype(\"int32\")\n        \n#     train = combined.iloc[:len(train)].copy()\n#     test  = combined.iloc[len(train):].reset_index(drop=True).copy()\n                \n#     return train, test, FEATURES","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:52:22.261913Z","iopub.execute_input":"2025-01-22T15:52:22.262226Z","iopub.status.idle":"2025-01-22T15:52:22.265689Z","shell.execute_reply.started":"2025-01-22T15:52:22.262197Z","shell.execute_reply":"2025-01-22T15:52:22.264976Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# def get_nn_features(train, test):\n    \n#     CAT_SIZE = []\n#     CAT_EMB  = []\n#     NUMS     = []\n#     CATS     = []\n\n#     RMV = [\"ID\",\"efs\",\"efs_time\",\"y\",\"fold\"]\n#     FEATURES = [c for c in train.columns if not c in RMV]\n    \n#     for c in FEATURES:\n#         if train[c].dtype==\"object\":\n#             train[c] = train[c].fillna(\"NAN\")\n#             test[c]  = test[c].fillna(\"NAN\")\n#             CATS.append(c)\n#         elif not \"age\" in c:\n#             train[c] = train[c].astype(\"str\")\n#             test[c]  = test[c].astype(\"str\")\n#             CATS.append(c)\n\n\n#     combined = pd.concat([train,test],axis=0,ignore_index=True)\n#     for c in FEATURES:\n#         if c in CATS:\n#             # LABEL ENCODE\n#             combined[c],_ = combined[c].factorize()\n#             combined[c] -= combined[c].min()\n#             combined[c] = combined[c].astype(\"int32\")\n#             #combined[c] = combined[c].astype(\"category\")\n\n#             n = combined[c].nunique()\n#             mn = combined[c].min()\n#             mx = combined[c].max()\n#             #print(f'{c} has ({n}) unique values')\n    \n#             CAT_SIZE.append(mx+1) \n#             CAT_EMB.append( int(np.ceil( np.sqrt(mx+1))) ) \n#         else:\n#             if combined[c].dtype==\"float64\":\n#                 combined[c] = combined[c].astype(\"float32\")\n#             if combined[c].dtype==\"int64\":\n#                 combined[c] = combined[c].astype(\"int32\")\n                \n#             m = combined[c].mean()\n#             s = combined[c].std()\n#             combined[c] = (combined[c]-m)/s\n#             combined[c] = combined[c].fillna(0)\n            \n#             NUMS.append(c)\n\n#     train = combined.iloc[:len(train)].copy()\n#     test = combined.iloc[len(train):].reset_index(drop=True).copy()\n\n#     return test[CATS], test[NUMS]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:52:18.343995Z","iopub.execute_input":"2025-01-22T15:52:18.344275Z","iopub.status.idle":"2025-01-22T15:52:18.347993Z","shell.execute_reply.started":"2025-01-22T15:52:18.344254Z","shell.execute_reply":"2025-01-22T15:52:18.347104Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# def get_tf_features(train, test):\n#     RMV = [\"ID\",\"efs\",\"efs_time\",\"y\",\"y_na\",\"fold\"]\n#     FEATURES = [c for c in train.columns if not c in RMV]\n\n\n#     test                             = test.replace('Not done', 'missing')\n#     test                             = test.replace('Not tested', 'missing')\n    \n#     test['na_count']                 = test.isna().sum(axis=1)\n#     test['age_karnofsky']            = test['age_at_hct'] * test['karnofsky_score']\n#     test['age_comorbidity']          = test['age_at_hct'] * test['comorbidity_score']\n#     test['donor_recipient_age_diff'] = abs(test['donor_age'] - test['age_at_hct'])\n#     test['hla_match_ratio']          = (test['hla_high_res_8'] + test['hla_low_res_8']) / 16\n#     test['age_squared']              = test['age_at_hct'] ** 2\n#     test['karnofsky_squared']        = test['karnofsky_score'] ** 2\n#     test['16?']                      = np.where(test['age_at_hct']<=16,1,0)\n    \n#     FEATURES.extend([\"na_count\", \"age_karnofsky\", \"age_comorbidity\", \"donor_recipient_age_diff\", \"hla_match_ratio\", \"age_squared\", \"karnofsky_squared\", \"16?\"])\n\n#     CATS = []\n#     for c in FEATURES:\n#         if test[c].dtype==\"object\":\n#             CATS.append(c)\n#             test[c] = test[c].fillna(\"missing\")\n\n#     for c in FEATURES:\n#         # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n#         if c in CATS:\n#             #print(f\"{c}, \",end=\"\")\n#             test[c],_ = test[c].factorize()\n#             test[c]  -= test[c].min()\n#             test[c]   = test[c].astype(\"int32\")\n#             test[c]   = test[c].astype(\"category\")\n#         else:\n#             if test[c].dtype == \"float64\":\n#                 test[c]      = test[c].astype(\"float32\")\n#             if test[c].dtype ==\"int64\":\n#                 test[c]      = test[c].astype(\"int32\")\n    \n#     return test, FEATURES","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:52:14.653655Z","iopub.execute_input":"2025-01-22T15:52:14.653930Z","iopub.status.idle":"2025-01-22T15:52:14.657340Z","shell.execute_reply.started":"2025-01-22T15:52:14.653909Z","shell.execute_reply":"2025-01-22T15:52:14.656475Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# import pickle\n# from tqdm import tqdm\n# from sklearn.svm import SVR\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.impute import KNNImputer\n\n# imputer               = KNNImputer(n_neighbors=5, weights='uniform')\n\n# scaler = StandardScaler()\n\n# FOLDS = 10\n\n# def inference(model_path, train, test_df):\n    \n#     path = model_path.split('/')[-1]\n#     file = path.replace('-','_')\n\n#     if \"dt\" not in model_path:\n#         with open(f\"/kaggle/input/{path}/{file}.pkl\", 'rb') as f:\n#             models = pickle.load(f)\n            \n#     print(\"All models are loaded successfully....!\")\n\n#     test_predictions = np.zeros(len(test_df))\n\n#     for fold in tqdm(range(FOLDS)):\n#         if \"dt\" in model_path:\n#             # model = keras.models.load_model(model_path, custom_objects=custom_objects)\n#             # train, test, FEATURES   = prepare_features(model_path, train, test_df)\n#             # model                   = tf.keras.models.load_model(model_path+f\"/model_fold_{fold}.keras\", custom_objects=dt.__dict__, compile=)\n#             # fold_preds              = model.predict(test.copy())\n#             # fold_preds              = fold_preds.flatten()\n#             pass\n#         else:\n#             model  = models[fold]\n            \n            \n#         if \"svr\" in model_path:\n#             if fold==0:\n#                 train, test, FEATURES = prepare_features(model_path, train.copy(), test_df.copy())\n\n#             # Handle missing values\n#             train_imputed         = imputer.fit_transform(train[FEATURES].copy())\n#             test_imputed          = imputer.transform(test[FEATURES])\n\n#             # Convert back to DataFrame to maintain feature names\n#             train_imputed         = pd.DataFrame(train_imputed, columns=FEATURES, index=train.index)\n#             test_imputed          = pd.DataFrame(test_imputed, columns=FEATURES, index=test.index)\n            \n#             # Scale features\n#             scaler.fit(train_imputed)\n#             test_scaled           = scaler.transform(test_imputed)\n            \n#             fold_preds            = model.predict(test_scaled)\n\n#         elif \"tn\" in model_path:\n#             if fold == 0:\n#                 train, test, FEATURES   = prepare_features(model_path, train, test_df)\n                \n#             fold_preds              = model.predict(test[FEATURES].values).flatten()\n            \n#         elif \"nn\" in model_path:\n#             if fold == 0:\n#                 X_cat, X_num      = get_nn_features(train, test_df.copy())\n#             fold_preds        = model.predict([X_cat.values, X_num.values])\n#             fold_preds        = fold_preds.flatten()\n\n#         elif \"tf\" in model_path:\n#             if fold == 0:\n#                 test, FEATURES = get_tf_features(train.copy(),test_df.copy())\n#                 fold_preds     = model.predict(test[FEATURES].copy())\n\n#         elif \"tabm\" in model_path:\n#             if fold == 0:\n#                 test, FEATURES = get_tabm_features(test_df.copy())\n#             fold_preds              = model.predict(test[FEATURES].copy())\n            \n#         else: \n#             if fold == 0:\n#                 train, test, FEATURES   = prepare_features(model_path, train, test_df)\n#             fold_preds              = model.predict(test[FEATURES].copy())\n            \n#         test_predictions += fold_preds\n    \n#     # Get the average predictionr\n#     test_predictions /= FOLDS\n        \n#     return test_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:52:11.189637Z","iopub.execute_input":"2025-01-22T15:52:11.189923Z","iopub.status.idle":"2025-01-22T15:52:11.193648Z","shell.execute_reply.started":"2025-01-22T15:52:11.189903Z","shell.execute_reply":"2025-01-22T15:52:11.192819Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# train_df = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n# test_df  = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-01-22T15:51:51.162801Z","iopub.execute_input":"2025-01-22T15:51:51.163120Z","iopub.status.idle":"2025-01-22T15:51:51.166096Z","shell.execute_reply.started":"2025-01-22T15:51:51.163097Z","shell.execute_reply":"2025-01-22T15:51:51.165367Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# first_best_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:51:54.701659Z","iopub.execute_input":"2025-01-22T15:51:54.701943Z","iopub.status.idle":"2025-01-22T15:51:54.704832Z","shell.execute_reply.started":"2025-01-22T15:51:54.701923Z","shell.execute_reply":"2025-01-22T15:51:54.704065Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# initial_test_preds = inference(first_best_index, train_df, test_df)\n# initial_test_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:51:57.348357Z","iopub.execute_input":"2025-01-22T15:51:57.348690Z","iopub.status.idle":"2025-01-22T15:51:57.351830Z","shell.execute_reply.started":"2025-01-22T15:51:57.348663Z","shell.execute_reply":"2025-01-22T15:51:57.351159Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# model_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:52:01.335658Z","iopub.execute_input":"2025-01-22T15:52:01.335938Z","iopub.status.idle":"2025-01-22T15:52:01.338972Z","shell.execute_reply.started":"2025-01-22T15:52:01.335916Z","shell.execute_reply":"2025-01-22T15:52:01.338017Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# test_preds = []\n# for model, weight in model_weights.items():\n#     print(f\"Using model : {model}\")\n#     test_df            = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n    \n#     test_preds         = inference(model, train_df.copy(), test_df)\n#     test_preds         = (1-weight) * rankdata(initial_test_preds) + weight * rankdata(test_preds)\n#     initial_test_preds = test_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:25:28.022763Z","iopub.execute_input":"2025-01-22T15:25:28.023081Z","iopub.status.idle":"2025-01-22T15:25:28.027486Z","shell.execute_reply.started":"2025-01-22T15:25:28.023052Z","shell.execute_reply":"2025-01-22T15:25:28.026444Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# train_df               = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n# test_df                = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n# test_preds             = np.zeros(len(test_df))\n\n\n# preds_dict = {}\n# for model, weight in model_weights.items():\n#     print(f\"exp name : {model}\\n\")\n#     test_df                = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n#     test_preds             = inference(model, train_df.copy(), test_df.copy())\n#     test_preds             = (1-weight) * rankdata(initial_test_preds) + weight * rankdata(test_preds)\n#     initial_test_preds     = test_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:51:34.790171Z","iopub.execute_input":"2025-01-22T15:51:34.790539Z","iopub.status.idle":"2025-01-22T15:51:34.795219Z","shell.execute_reply.started":"2025-01-22T15:51:34.790484Z","shell.execute_reply":"2025-01-22T15:51:34.793900Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# test_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:51:38.332302Z","iopub.execute_input":"2025-01-22T15:51:38.332656Z","iopub.status.idle":"2025-01-22T15:51:38.335669Z","shell.execute_reply.started":"2025-01-22T15:51:38.332625Z","shell.execute_reply":"2025-01-22T15:51:38.334872Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# potential_ensemble                = pd.DataFrame()\n# potential_ensemble[\"predictions\"] = test_preds\n# potential_ensemble[\"ID\"]          = test_df[\"ID\"]\n# new_score                         = score(test_df[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy(), potential_ensemble.copy(), \"ID\")\n# new_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:42:13.951997Z","iopub.execute_input":"2025-01-22T15:42:13.952261Z","iopub.status.idle":"2025-01-22T15:42:13.955904Z","shell.execute_reply.started":"2025-01-22T15:42:13.952239Z","shell.execute_reply":"2025-01-22T15:42:13.955011Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# best_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:51:41.520933Z","iopub.execute_input":"2025-01-22T15:51:41.521214Z","iopub.status.idle":"2025-01-22T15:51:41.524083Z","shell.execute_reply.started":"2025-01-22T15:51:41.521193Z","shell.execute_reply":"2025-01-22T15:51:41.523309Z"}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"## Step 7: Create submission file","metadata":{}},{"cell_type":"code","source":"# sub            = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n# sub.prediction = test_preds\n# sub.to_csv(\"submission.csv\",index=False)\n# print(\"Sub shape:\",sub.shape)\n# sub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:51:44.807144Z","iopub.execute_input":"2025-01-22T15:51:44.807435Z","iopub.status.idle":"2025-01-22T15:51:44.810222Z","shell.execute_reply.started":"2025-01-22T15:51:44.807409Z","shell.execute_reply":"2025-01-22T15:51:44.809395Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}